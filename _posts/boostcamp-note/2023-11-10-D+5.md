---
layout: single
title:  "Day 5 학습일지"
categories: boostcamp-note-week1
sidebar:
  nav: "docs"
---

# 23/11/10 (금) 학습 내용

<h2>AI Math</h2>

- <b>(9강) CNN 첫걸음</b><br><br>
: : 지금까지 배운 MLP의 경우 각 뉴런들이 선형모델과 활성함수로 모두 연결된(Fully connected) 구조였다.
: : 가중치 행렬의 크기와 학습시켜야 하는 parameter가 굉장히 커졌다.
: : Convolution에서는 공통된 커널을 사용해 연산하는 것이 특징.
: : i의 개수와 상관 없이, 커널 사이즈는 고정된 형태로 적용 (parameter 사이즈를 굉장히 많이 줄일 수 있다.)
: : 연속확률변수의 경우 적분, 이산확률변수일 경우 급수로 식이 정의된다.
: : 커널은 정의역 내에서 움직여도 변하지 않고(<u>translation invariant</u>), 주어진 신호에 <u>국소적(local)</u>으로 적용한다.
: : 커널 위치에서 행렬곱이 아닌, 성분곱을 하여 더해주는 연산을 하게 된다. (커널을 한 칸씩 움직여가면서 계산하는 것)
: : 입력 및 커널의 크기를 통해 출력의 크기를 계산할 수 있다.
: : 입력 크기를 (H, W), 커널 크기를(K<sub>H</sub>, K<sub>W</sub>), 출력 크기를 (OH, OW)라 하면 출력 크기는,
: : O<sub>H</sub> = H - K<sub>H</sub> + 1
: : O<sub>W</sub> = W - K<sub>W</sub> + 1
: : ex) 28x28 입력을 3x3 커널로 2D-Conv 연산을 하면 26x26이 된다.
: : 채널이 여러개인 2차원 입력의 경우 2차원 Conv.을 채널 개수만큼 적용한다.
: : 3차원부터는 행렬이 아닌 tensor라 부른다.
: : 채널이 여러개인 경우 <u>커널의 채널 수</u>와 <u>입력의 채널 수</u>가 같아야 한다.
: : 커널(K<sub>H</sub>, K<sub>W</sub>, C) * 3차원 입력(H, W, C) -> 출력(O<sub>H</sub>, O<sub>W</sub>, 1)
: : 커널을 O<sub>C</sub>개 사용하면 출력도 텐서가 된다.
: : 커널 (K<sub>H</sub>, K<sub>W</sub>, C)을 O<sub>C</sub>개 사용
: <br><b>Convolution 연산의 역전파(BPP) 이해하기</b><br><br>
: : Convolution 연산에 미분을 해도 g의 도함수인 g'에 대해서 convolution을 하는 똑같은 연산이 나온다. (이를 도식을 통해서 이해해보자!)
: : o<sub>i</sub> = <sub>j</sub>Σ w<sub>j</sub>·x<sub>i+j-1</sub>
: : 역전파 단계에서 다시 커널을 통해 그레디언트가 전달된다.
: : 각 커널에 들어오는 모든 그레디언트를 더하면 결국 convolution 연산과 같다.

- <b>(10강) RNN 첫걸음</b><br><br>
: : RNN은 기존까지와 다르게 시계열 데이터, 시퀀스 데이터에 적용되는 네트워크
: : i,i,d(독립동등분포)와 달리, 시계열 데이터는 독립적으로 들어오지 않는 경우가 많다.
: : 어떤 모델을 설계하고, 어떻게 학습시킬지에 대해 배워보는 시간
: <br><b>시퀀스 데이터 이해하기</b><br><br>
: : 소리, 문자열, 주가 등의 데이터를 시퀀스 데이터로 분류한다.
: : 순서를 바꾸거나 과거 정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 된다.
: : 이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률을 이용할 수 있다.
: : P(X<sub>1</sub>, ... ,X<sub>t</sub>) = P(X<sub>t</sub> ⎸ X1, ... ,X<sub>t-1</sub>) · P(X1, ..., X<sub>t-1</sub>)
: : 위의 식 설명 - 베이즈 법칙을 사용하여, X<sub>1</sub>에서 X<sub>t</sub>까지의 결합 확률분포를, X<sub>t</sub>에 대한, X<sub>1</sub>에서 X<sub>t-1</sub>까지의 정보가 주어졌을 때, X<sub>t</sub>에 대한 조건부 확률분포와, X<sub>1</sub>에서 X<sub>t-1</sub>까지의 결합확률을 곱해주면 원래 모델링하고자 했던 X<sub>1</sub> ~ X<sub>t</sub>까지의 결합확률분포를 모델링할 수 있다.
: : = <sub>s=1</sub>∏<sub>t</sub> P(X<sub>s</sub> ⎸ X<sub>s-1</sub>, ..., X<sub>1</sub>)
: : 시퀀스 데이터를 분석할 때, 모든 과거의 정보들이 필요한 것은 아니다. 가변적인 데이터를 다룰 수 있는 모델이 필요하다.
: : X<sub>t</sub> ~ P(X<sub>t</sub> ⎸ X<sub>t-1</sub>, ... ,X<sub>1</sub>)
: : X<sub>t+1</sub> ~ P(X<sub>t+1</sub> ⎸ X<sub>t</sub>, X<sub>t-1</sub>, ... ,X<sub>1</sub>)
: : 고정된 길이 𝜏(tau) 만큼의 시퀀스만 사용하는 경우 AR(𝜏) (Autoregressive Model) 자기회귀모델이라고 부른다
: : 𝜏를 결정하는데에도 사전 정보가 필요한 경우들이 있기 때문에, 혹은 먼 과거의 정보가 필요한 경우도 있기 때문에, 
: : 잠재 자기회귀모델 이라고 부르는 것을 활용하게 됨 (직전 정보와, 직전 정보가 아닌 정보들을 따로 모아서 H<sub>t</sub>라는 잠재 변수로 인코딩해서 활용하게 된다.)
: : X<sub>t</sub>에서는 X<sub>t-1</sub>이라는 직전 정보와, X<sub>t-2</sub> , ... , X<sub>1</sub> 이라는 데이터들로 만든 H<sub>t</sub> 라는 잠재변수를 활용하게 되는 것이다.
: : 이런 모델의 장점은 과거의 모든 데이터를 활용해 예측할 수 있고, 가변적인 데이터 문제를 고정된 길이의 문제로 잘 바꿀 수 있다는 장점이 있다.
: <br><b>Recurrent Neural Network을 이해하기</b><br><br>
: : 가장 기본적인 RNN 모형은 MLP와 유사하다
: : O = HW<sup>(2)</sup> + b<sup>(2)</sup>
: : H = 𝝈(XW<sup>(1)</sup> + b<sup>(1)</sup>)<br><br>
: : O<sub>t</sub> = H<sub>t</sub>W<sup>(2)</sup> + b<sup>(2)</sup>
: : H<sub>t</sub> = 𝜎(X<sub>t</sub>W<sup>(1)</sup><sub>X</sub> + H<sub>t-1</sub>W<sup>(1)</sup><sub>H</sub> + b<sup>(1)</sup>)
: : 가중치 행렬이 3개로 늘어나는데, W<sup>(1)</sup><sub>X</sub>, W<sup>(1)</sup><sub>H</sub>, W<sup>(2)</sup>이고, 이들은 t에 따라 변하지 않는 가중치 행렬이라는 사실을 기억하라.
: : t에 따라 변하는 것은 잠재 변수와 입력 벡터임.
: <br><b>RNN의 역전파(Backpropagation Through Time, BPTT)</b><br><br>
: : RNN의 역전파는 잠재변수의 연결그래프에 따라 순차적으로 계산합니다.
: : 잠재변수에는 다음 시점에서 들어오는 잠재변수 그레디언트와, 출력에서 들어오는 그레디언트 이렇게 두개가 들어오게 되고, 이 과정을 반복하여 학습이 이루어지게 된다.
: <br><b>기울기 소실의 해결책?</b><br><br>
: : truncated BPTT, 즉, 그레디언트를 모든 t 시점에서 전달하지 않고 특정 블록에서 끊고 나눠서 전달하는 방식이다. 이 방법을 통해 기울기 소실을 해결할 수 있지만...
: : 이런 문제들 때문에 Vanilla RNN (기본적인 RNN 모델)은 길이가 긴 시퀀스를 처리하는데 문제가 있다. 그래서 LSTM, GRU 등을 통해 긴 시퀀스를 처리한다.


<h2>과제 수행 내용</h2>
- AI Math 완성하기
: : 연쇄법칙(Chain Rule) 이해하기
- <b>심화 과제3</b><br><br>


<h2>Peer Session</h2>

