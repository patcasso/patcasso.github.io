---
layout: single
title:  "D-1 학습일지"
categories: boostcamp-note
sidebar:
  nav: "docs"
---

# 23/11/05 학습 내용

<h2>AI Math</h2>

- (9강) CNN 첫걸음<br><br>
: <b>1) Convolution 연산 이해하기</b><br><br>
: : 지금까지 배운 다층신경망(MLP)은 각 뉴런들이 선형모델과 활성함수로 모두 연결된(fully connected) 구조였음.
: : Convolution 은 커널이라는 고정된 가중치 행렬을 사용하고, 이 고정된 커널을 입력 벡터상에서 움직여가며 선형모델과 활성함수가 적용되는 구조
: : i의 개수와 상관없이 커널사이즈는 고정된 상태로 공통적으로 적용되기 때문에, parameter 사이즈를 많이 줄일 수 있는 것이 특징이다.
: : 연속공간에서는 적분으로 정의, 이산공간에서는 급수로 정의. But 적용되는 방식은 같다.
: : 커널은 정의역 내에서 움직여도 변하지 않고(translation invariant) 주어진 신호에 국소적(local)으로 적용된다.
: : Convolution은 1차원뿐만 아니라 다양한 차원에서 계산 가능하다 (PDF 수식 참조)
: <br><b>2) 2차원 Convolution 연산 이해하기</b><br><br>
: : 영상에서 많이 사용되는 2차원 Convolution
: : 커널의 위치에서 행렬 곱셈이 아닌, 커널과 입력에서 같은 위치에 있는 성분끼리 성분곱을 한 후 더해서 결과를 내게 된다.
: : 입력 크기를 (H, W), 커널 크기를 (K<sub>H</sub>, K<sub>W</sub>), 출력 크기를 (O<sub>H</sub>, O<sub>W</sub>)라 하면 출력 크기는 다음과 같다.
: : O<sub>H</sub> = H - K<sub>H</sub> + 1
: : O<sub>W</sub> = W - K<sub>W</sub> + 1
: : 예컨대, 28*28 입력을 3*3 커널로 2D-Conv 연산을 하면 26*26이 된다.
: : 채널이 여러개인 2차원 입력의 경우 (ex. RGB 데이터는 2차원이지만 3개의 채널 존재) 2차원 Convolution을 채널 개수만큼 적용한다고 생각하면 된다.
: : 채널이 여러개인 경우 커널의 채널 수와 입력의 채널 수가 같아야 한다.
: : (K<sub>H</sub>, K<sub>W</sub>, C)인 커널과 (H, W, C)인 3차원 입력을 Convolution 연산 하면 (O<sub>H</sub>, O<sub>W</sub>, 1) 짜리 1차원 출력이 나온다.
: : 만약 커널을 O<sub>C</sub>개 사용하면 출력도 텐서가 된다.
: <br><b>3) Convolution 연산의 역전파 이해하기</b><br><br>
: : Convolution 연산은 커널이 모든 입력데이터에 공통으로 적용되기 때문에 역전파를 계산할 때도 convolution 연산이 나오게 된다.
: : 도식에서 각 δ(delta)는 미분값을 나타낸다.
: : 각 커널에 들어오는 모든 그레디언트를 더하면 결국 convolution 연산과 같다.

<br><br>

- (10강) RNN 첫걸음<br><br>
: : 모델 자체 설계는 어렵지 않지만, 왜 이렇게 설계하는지 이해해야 한다.
: <br><b>1) 시퀀스 데이터 이해하기</b><br><br>
: : 소리, 문자열, 주가 등의 데이터를 sequence 데이터로 분류한다.
: : 시계열(time-series) 데이터는 시간 순서에 따라 나열된 데이터로 시퀀스 데이터에 속한다.
: : 시퀀스 데이터는 독립동등분포(independent and identical distribution, i.i.d.) 가정을 잘 위배하기 때문에 순서를 바꾸거나 과거 정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 된다.
: : 이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률을 이용할 수 있다.
: : P(X1, ... ,Xt) = P(X<sub>t</sub>\|X1, ... ,X<sub>t-1</sub>)P(X1, ... ,X<sub>t-1</sub>)
: : <a href="https://ko.wikipedia.org/wiki/%EB%B2%A0%EC%9D%B4%EC%A6%88_%EC%A0%95%EB%A6%AC">베이즈 정리</a>
: : t번째 X의 확률 = X의 t-1 번째까지의 확률 * t-1번째에 대한 X의 조건부 확률
: : 이를 반복적으로 적용하면, X1에서 X<sub>s-1</sub>까지의 조건부를 가지고 X<u>s</u>를 추론하는 조건부 확률을, S = 1 에서 t까지 쭉 곱해주는 형태로 표현할 수 있다.
: : 위 조건부확률은 과거의 모든 정보를 사용하지만, 시퀀스 데이터를 분석할 때 모든 과거 정보들이 필요한 것은 아니다.
: : 과거 어떤 시점의 정보는 필요하지 않을 때도 있다.
: : 시퀀스 데이터를 다루기 위해선 길이가 가변적인 데이터를 다룰 수 있는 모델이 필요하다.
: : 고정된 길이 𝝉 만큼의 시퀀스만 사용하는 경우 AR(𝝉)(Autoregressive Model) 자기회귀모델이라고 부른다.
: : 𝝉는 하이퍼파라미터이므로, 사전지식이 필요한 경우가 많다. 그래서 RNN을 배우는 것.
: : 이 방법은 바로 직전 정보를 제외한 나머지 정보들을 H<sub>t</sub>라는 잠재변수로 인코딩해서 활용하는 잠재 AR 모델이다.
: : 이 모델의 장점은 과거의 모든 정보를 활용해서 예측을 할 수 있으며, 또한 가변적인 데이터 문제를 고정된 길이의 문제로 바꿀 수 있다는 것이다.
: : 과거의 변수들을 어떻게 잠재변수로 인코딩할것인가를 해결하기 위해 등장한 모델이 바로 RNN(Recurrent Neural Network)이다.
: <br><b>2) Recurrent Neural Network 이해하기</b><br><br>
: : MLP모델의 경우 아래와 같다.
: : O = HW<sup>(2)</sup> + <sup>b</sup>
: : H = 𝜎(XW<sup>(1)</sup>+b<sup>(1)</sup>)
: : W<sup>(1)</sup>, W<sup>(2)</sup>는 시퀀스와 상관없이 불변인 행렬
: : 이 모델은 과거의 정보를 잠재변수에 다룰 수 없다.
: : RNN은 이전 순서의 잠재변수와 현재의 입력을 활용하여 모델링한다.
: : O<sub>t</sub> = H<sub>t</sub>W<sup>(2)</sup> + b<sup>(2)</sup>
: : H<sub>t</sub> = 𝜎(X<sub>t</sub>W<sup>(1)</sup><sub>X</sub> + H<sub>t-1</sub>W<sup>(1)</sup><sub>H</sub> + b<sup>(1)</sup>)
: : 입력으로부터 전달되는 W<sub>X</sub> 가중치 행렬과, 이전 잠재 변수로부터 정보를 전달받게 되는 W<sub>H</sub> 라는 새로운 가중치 행렬을 만들게 된다.
: : t번째 잠재변수는 현재 들어온 입력 벡터 X<sub>t</sub>와, 이전 시점의 잠재변수인 H<sub>t-1</sub>을 받아서, 현재 시점의 H<sub>t</sub>를 만들어 내는 것이고, 
: : 이 H<sub>t</sub>를 이용해 현재 시점의 출력인 O<sub>t</sub>를 만들어내는 것.
: : 이때 만들어진 H<sub>t</sub>를 복제해서 다음 순서의 잠재변수를 인코딩하는데 사용한다.
: : 여기서 가중치 행렬이 3개가 나오게 된다. 먼저 첫 번째 레이어에서는 W<sub>X</sub><sup>(1)</sup>, 즉 입력 데이터에서부터 선형모델을 통해 잠재변수로 인코딩하게 되는 W<sub>X</sub><sup>(1)</sup>과,
: : 이전 시점의 잠재변수로부터 정보를 받아서 현재 시점의 잠재 변수로 인코딩해주는 W<sub>H</sub><sup>(1)</sup>, 즉, 첫 번째 레이어의 가중치 행렬 W<sub>H</sub><sup>(1)</sup>를 사용하게 되는 것이고,
: : 이렇게 만든 잠재 변수를 통해 다시 출력으로 만들어주는 W<sup>(2)</sup> 가중치 행렬이 존재하게 되어, 총 3 개의 가중치 행렬이 있게 된다.
: : 명심할 점은, <u>W<sup>(2)</sup>, W<sub>(X)</sub><sup>(1)</sup>, W<sub>H</sub><sup>(1)</sup> 이렇게 세 개의 가중치 행렬은 t에 따라 변하지 않는 가중치 행렬이라는 사실을 기억하라.</u> 
: : 즉, t에 따라 변하는 것은 오로지 잠재변수와 입력 데이터에 해당하는 것이고, RNN에서 사용되는 가중치 행렬 세 개는 t에 따라 변하지 않는다는 것!
: : 이 가중치 행렬들은 동일하게 각각의 t 시점에 활용되어 모델링을 하게 되는 것이다.
: <br><b>* RNN의 역전파</b><br><br>
: : RNN의 역전파는 잠재변수의 연결그래프에 따라 <u>순차적으로</u> 계산한다. 이를 Backpropagation Through Time(BPTT)라 하며, RNN의 역전파 방법이다.
: : 잠재변수에는 두 개의 gradient가 들어오게 되는데, 다음 시점에서의 잠재변수에서 들어오게 되는 gradient vector와, 출력에서 들어오게 되는 gradient vector이다.
: : 이 잠재변수에 들어오는 gradient vector를 입력과 그 이전시점의 잠재변수로 전달하게 되고, 이를 반복해서 RNN의 학습이 이루어진다.
: <br><b>BPTT를 좀 더 살펴보자</b><br><br>
: : BPTT를 통해 RNN의 가중치행렬의 미분을 계산해보면, 아래와 같이 미분의 곱으로 이루어진 항이 계산된다. (PDF 수식 참조)
: : 시퀀스 길이가 곱해지는 term들이 불안정해지기 쉽다. (굉장히 커지거나 작아질 수 있음)
: : 기울기 소실(Vanishing gradient)의 해결책 - 시퀀스 길이가 길어지는 경우 BPTT를 통한 역전파 알고리즘의 계산이 불안정해지므로, 길이를 끊는 것이 필요하다.
: : 이를 truncated BPTT라고 부른다.
: : 이런 문제들 때문에 Vanilla RNN(기본적인 RNN 모형)은 길이가 긴 시퀀스를 처리하는데 문제가 있다.
: : 이를 해결하기 위해 등장한 RNN 네트워크가 LSTM과 GRU이다.



<h2>Git 기초</h2>

- (7강) git fetch & merge<br><br>
: : 그럼 만약 두 사람이 동시에 commit 해서 push 하면 어떻게 되나?
: : Sync 버튼을 누르면 pull & push동시에 된다.
: : fetch는 원격저장소에 있는 내용을 로컬 저장소로 가져온다. 이를 통해 로컬 저장소와 원격 저장소와의 차이를 비교할 수 있다. 이를 통해 충돌되는 상황은 발생하지 않는지, 충돌한다면 이를 어떻게 해결하면 좋을지 확인 한 후 merge를 통해 두 브랜치를 병합한다. branch와 merge는 이후 또 다루기로
: : pull은 fetch와 merge를 한 번에 하는 것
: : git graph 상에서 나눠진 branch를 우클릭하고 merge into current branch로 merge 한다
<br><br>

- (8강) git init & add <br><br>
: : 로컬 환경에서 먼저 repo를 생성하고 network로 push 하기 위한 강좌
: : commit 시 여러 단계로 분할해서 version을 control 하고 싶을 때는, source control에서 파일명 옆의 + 를 누르면 staged changes로 변경사항을 분리해서 버젼을 만들 수 있다.
<br><br>

- (9강) git checkout <br><br>
: : 디버깅을 할 때, 특정 스테이지로 돌아가 버그가 있는지 체크해볼 수 있다.
: : git graph에서 오른쪽 클릭해서 돌아가고 싶은 버젼으로 checkout하고,
: : 다시 최신으로 돌아오고 싶을 때는 'main' 글씨 위에서 오른쪽 클릭해서 checkout branch 해야 main의 테두리가 하이라이트 된다.

- (10강) git remote <br><br>
: : 하나의 저장소 안에서 여러 개의 작업을 해야 하거나,
: : 프로젝트 내에서 실험적인 작업을 해야 할 때가 있다. (막상 해봤는데 안 돼서 버려야 할 때가 있음)
: : 저장소 전체를 카피해서 거기서 작업을 하거나 할텐데, 그럴 필요가 없다.
: : ex. 여러 개의 작업을 동시헤 하고싶으면 어떻게 하나?
: : 실험적인 작업을 하고 싶을 때는 main 글씨 오른쪽 공터에서 오른쪽 클릭하고 Create Branch
: : 현재 main의 테두리가 진하면 head가 main을 가리키는거고, exp 오른쪽 클릭해서 checkout branch해야 exp로 head가 간다.
: : 다시 main 작업을 하고싶으면 main 마지막 작업 오른쪽 클릭해서 checkout
: : 다시 exp 가고싶으면 exp로 checkout
: : 실험이 실패한 작업을 버리고 싶으면 delete branch 해도 되고, 아니면 그냥 둬도 된다.


<h2>Python</h2>

- (6강) Numpy - part 1<br><br>
: : Numerical Python : 파이썬의 고성능 과학 계산용 패키지
: : Matrix 와 Vector 같은 Array 연산의 사실상의 표준
: : Numpy는 행렬을 어떻게 코드로 나타낼 것인가에 대한 것.
: : 선형대수의 연산은 list로만 하기에는 너무 복잡한 것들이 많다.
: : list는 메모리 효율적이지 않다.
: <br><b>* Numpy의 특징</b><br><br>
: : 반복문 없이 데이터 배열에 대한 처리를 지원함
<br><br>
: <br><b>1) ndarray</b><br><br>
: : numpy는 <u>하나의 데이터 type</u>만 배열에 넣을 수 있음
: : List와 가장 큰 차이점 -> dynamic typing 미지원
: : List에는 메모리 주소가 들어가고, Numpy Array에는 값 자체가 들어가는 차이점이 있다. (연산이 좋아짐)
: : shape : numpy array의 dimension 구성을 반환함
: : dtype : numpy array의 데이터 type을 반환
: : scalar[0차원] - vector[1차원] - matrix[2차원] - 3-tensor[3차원, third-order tensor] - n-tensor[n차원, n-order tensor]
: : ndim - number of dimensions
: : 64bit면 2<sup>63</sup>까지 표현 가능 (+, - 때문에)
: : float32는 32비트 값으로 2진수를 표현한다는 것
: : nbytes - ndarray object의 메모리 크기를 반환함
<br><br>

- (6강) Numpy - part 2<br><br>
: : **reshape** : Array의 크기를 변경함, element의 갯수는 동일
: : -1: size를 기반으로 row / col 개수 선정
: : **flatten**: 다차원 array를 1차원 array로 변환
: <br><b>1) indexing & slicing</b><br><br>
: : list와 달리 2차원 배열에서 [0,0] 표기법을 제공함 (list는 [i][j])
: : **slicing** : list와 달리 행과 열 부분을 나눠서 slicing이 가능함
: : matrix의 부분 집합을 추출할 때 유용함
: <br><b>2) creation function</b><br><br>
: : **arange** : array의 범위를 지정하여, 값의 list를 생성하는 명령어
: : step을 0.5로도 지정 가능
: <br><b>3) ones, zeros and empty</b><br><br>
: : np.zeros(shape, dtype, order)
: : **empty** - shape만 주어지고 비어있는 ndarray 생성 (memory initialization이 되지 않음)
: : **something_like** : 기존 ndarray의 shape 크기 만큼 1, 0 또는 empty array를 반환
: <br><b>4) eye, identity & diagonal</b><br><br>
: : **identity** : 단위 행렬(i 행렬)을 생성함
: : **diag** : 대각 행렬의 값을 추출함 (k로 start index 조절)
: : **random sampling** : 데이터 분포에 따른 sampling으로  array를 생성
: <br><b>5) operation functions</b><br><br>
: : **sum** : ndarray의 element들 간의 합을 구함, list의 sum 기능과 동일
: : **axis** : 모든 operation function을 실행할 때 기준이 되는 dimension 축
: : 새로운 rank가 생성될 때마다, 새로운 rank 가 axis = 0이 되고, 기존의 axis들은 하나씩 올라간다
: : **mean & std** : ndarray의 element들 간의 평균 또는 표준 편차를 반환
: : **mathematical functions** : 그 외에도 다양한 수학 연산자를 제공함 (np.something 호출)
: : **concatenate** : numpy array를 합치는(붙이는) 함수
: : vstack은 세로로 이어붙이기, hstack은 가로로 이어붙이기
: <br><b>6) array operations</b><br><br>
: : numpy는 array간의 기본적인 사칙 연산을 지원함
: : **Element-wise operations** : array간 shape이 같을 때 일어나는 연산
: : **Dot product** : Matrix의 기본 연산,  dot 함수 사용
: : **transpose** 또는 **T attribute** 사용
: : **broadcasting** : Shape이 다른 배열 간 연산을 지원하는 기능
: : Scalar - vector 외에도 vector - matrix 간의 연산도 지원
: : 굉장히 주의해야 하는 연산임
: : **Numpy performance**
: : 일반적으로 속도는 for loop < list comprehension < numpy 순서임
: : 대용량 계산에서는 가장 흔히 사용됨 
: : Numpy는 C로 구현되어 있어, 성능을 확보하는 대신 Dynamic typing을 포기함

- (6강) Numpy - part 3<br><br>
: <br><b>1) comparisons</b><br><br>
: : **All & Any**
: : Array의 데이터 전부(and) 또는 일부(or)가 조건에 만족 여부 반환
: : numpy는 배열의 크기가 동일할 때 element간 비교의 결과를 Boolean type으로 반환
: : logical_and, logical_not, logical_or로 조건의 condition도 가능
: : **np.where**
: : 특정 조건을 만족하는 원소가 어디에 있는지에 관한 함수
: : **argmax & argmin**
: : array내 최대값 또는 최소값의 index를 반환함
: : axis 기반의 반환도 가능
: <br><b>2) boolean & fancy index</b><br><br>
: : 특정 조건에 따른 값을 배열 형태로 추출
: : Comparison operation 함수들도 모두 사용 가능
: : **boolean index**
: : **fancy index**
: : numpy는 array를 index value로 사용해서 값 추출
: : matrix 형태의 데이터도 fancy index 사용 가능
: <br><b>3) numpy data i/o</b><br><br>
: : text type의 데이터는 읽고, 저장하는 기능
: : **loadtxt & savetxt**