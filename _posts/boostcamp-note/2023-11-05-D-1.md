---
layout: single
title:  "D-1 학습일지"
categories: boostcamp-note
sidebar:
  nav: "docs"
---

# 23/11/05 학습 내용

<h2>AI Math</h2>

- (9강) CNN 첫걸음<br><br>
: <b>1) Convolution 연산 이해하기</b><br><br>
: : 지금까지 배운 다층신경망(MLP)은 각 뉴런들이 선형모델과 활성함수로 모두 연결된(fully connected) 구조였음.
: : Convolution 은 커널이라는 고정된 가중치 행렬을 사용하고, 이 고정된 커널을 입력 벡터상에서 움직여가며 선형모델과 활성함수가 적용되는 구조
: : i의 개수와 상관없이 커널사이즈는 고정된 상태로 공통적으로 적용되기 때문에, parameter 사이즈를 많이 줄일 수 있는 것이 특징이다.
: : 연속공간에서는 적분으로 정의, 이산공간에서는 급수로 정의. But 적용되는 방식은 같다.
: : 커널은 정의역 내에서 움직여도 변하지 않고(translation invariant) 주어진 신호에 국소적(local)으로 적용된다.
: : Convolution은 1차원뿐만 아니라 다양한 차원에서 계산 가능하다 (PDF 수식 참조)
: <br><b>2) 2차원 Convolution 연산 이해하기</b><br><br>
: : 영상에서 많이 사용되는 2차원 Convolution
: : 커널의 위치에서 행렬 곱셈이 아닌, 커널과 입력에서 같은 위치에 있는 성분끼리 성분곱을 한 후 더해서 결과를 내게 된다.
: : 입력 크기를 (H, W), 커널 크기를 (K<sub>H</sub>, K<sub>W</sub>), 출력 크기를 (O<sub>H</sub>, O<sub>W</sub>)라 하면 출력 크기는 다음과 같다.
: : O<sub>H</sub> = H - K<sub>H</sub> + 1
: : O<sub>W</sub> = W - K<sub>W</sub> + 1
: : 예컨대, 28*28 입력을 3*3 커널로 2D-Conv 연산을 하면 26*26이 된다.
: : 채널이 여러개인 2차원 입력의 경우 (ex. RGB 데이터는 2차원이지만 3개의 채널 존재) 2차원 Convolution을 채널 개수만큼 적용한다고 생각하면 된다.
: : 채널이 여러개인 경우 커널의 채널 수와 입력의 채널 수가 같아야 한다.
: : (K<sub>H</sub>, K<sub>W</sub>, C)인 커널과 (H, W, C)인 3차원 입력을 Convolution 연산 하면 (O<sub>H</sub>, O<sub>W</sub>, 1) 짜리 1차원 출력이 나온다.
: : 만약 커널을 O<sub>C</sub>개 사용하면 출력도 텐서가 된다.
: <br><b>3) Convolution 연산의 역전파 이해하기</b><br><br>
: : Convolution 연산은 커널이 모든 입력데이터에 공통으로 적용되기 때문에 역전파를 계산할 때도 convolution 연산이 나오게 된다.
: : 도식에서 각 δ(delta)는 미분값을 나타낸다.
: : 각 커널에 들어오는 모든 그레디언트를 더하면 결국 convolution 연산과 같다.

<br><br>

- (10강) RNN 첫걸음<br><br>
: : 모델 자체 설계는 어렵지 않지만, 왜 이렇게 설계하는지 이해해야 한다.
: <br><b>1) 시퀀스 데이터 이해하기</b><br><br>
: : 소리, 문자열, 주가 등의 데이터를 sequence 데이터로 분류한다.
: : 시계열(time-series) 데이터는 시간 순서에 따라 나열된 데이터로 시퀀스 데이터에 속한다.
: : 시퀀스 데이터는 독립동등분포(independent and identical distribution, i.i.d.) 가정을 잘 위배하기 때문에 순서를 바꾸거나 과거 정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 된다.
: : 이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률을 이용할 수 있다.
: : P(X1, ... ,Xt) = P(X<sub>t</sub>\|X1, ... ,X<sub>t-1</sub>)P(X1, ... ,X<sub>t-1</sub>)
: : <a href="https://ko.wikipedia.org/wiki/%EB%B2%A0%EC%9D%B4%EC%A6%88_%EC%A0%95%EB%A6%AC">베이즈 정리</a>
: : t번째 X의 확률 = X의 t-1 번째까지의 확률 * t-1번째에 대한 X의 조건부 확률
: : 이를 반복적으로 적용하면, X1에서 X<sub>s-1</sub>까지의 조건부를 가지고 X<u>s</u>를 추론하는 조건부 확률을, S = 1 에서 t까지 쭉 곱해주는 형태로 표현할 수 있다.
: : 위 조건부확률은 과거의 모든 정보를 사용하지만, 시퀀스 데이터를 분석할 때 모든 과거 정보들이 필요한 것은 아니다.
: : 과거 어떤 시점의 정보는 필요하지 않을 때도 있다.
: : 시퀀스 데이터를 다루기 위해선 길이가 가변적인 데이터를 다룰 수 있는 모델이 필요하다.
: : 고정된 길이 𝝉 만큼의 시퀀스만 사용하는 경우 AR(𝝉)(Autoregressive Model) 자기회귀모델이라고 부른다.
: : 𝝉는 하이퍼파라미터이므로, 사전지식이 필요한 경우가 많다. 그래서 RNN을 배우는 것.
: : 이 방법은 바로 직전 정보를 제외한 나머지 정보들을 H<sub>t</sub>라는 잠재변수로 인코딩해서 활용하는 잠재 AR 모델이다.
: : 이 모델의 장점은 과거의 모든 정보를 활용해서 예측을 할 수 있으며, 또한 가변적인 데이터 문제를 고정된 길이의 문제로 바꿀 수 있다는 것이다.
: : 과거의 변수들을 어떻게 잠재변수로 인코딩할것인가를 해결하기 위해 등장한 모델이 바로 RNN(Recurrent Neural Network)이다.
: <br><b>2) Recurrent Neural Network 이해하기</b><br><br>
: : MLP모델의 경우 아래와 같다.
: : O = HW<sup>(2)</sup> + <sup>b</sup>
: : H = 𝜎(XW<sup>(1)</sup>+b<sup>(1)</sup>)
: : W<sup>(1)</sup>, W<sup>(2)</sup>는 시퀀스와 상관없이 불변인 행렬
: : 이 모델은 과거의 정보를 잠재변수에 다룰 수 없다.
: : RNN은 이전 순서의 잠재변수와 현재의 입력을 활용하여 모델링한다.
: : O<sub>t</sub> = H<sub>t</sub>W<sup>(2)</sup> + b<sup>(2)</sup>
: : H<sub>t</sub> = 𝜎(X<sub>t</sub>W<sup>(1)</sup><sub>X</sub> + H<sub>t-1</sub>W<sup>(1)</sup><sub>H</sub> + b<sup>(1)</sup>)
: : 입력으로부터 전달되는 W<sub>X</sub> 가중치 행렬과, 이전 잠재 변수로부터 정보를 전달받게 되는 W<sub>H</sub> 라는 새로운 가중치 행렬을 만들게 된다.
: : t번째 잠재변수는 현재 들어온 입력 벡터 X<sub>t</sub>와, 이전 시점의 잠재변수인 H<sub>t-1</sub>을 받아서, 현재 시점의 H<sub>t</sub>를 만들어 내는 것이고, 
: : 이 H<sub>t</sub>를 이용해 현재 시점의 출력인 O<sub>t</sub>를 만들어내는 것.
: : 이때 만들어진 H<sub>t</sub>를 복제해서 다음 순서의 잠재변수를 인코딩하는데 사용한다.
: : 여기서 가중치 행렬이 3개가 나오게 된다. 먼저 첫 번째 레이어에서는 W<sub>X</sub><sup>(1)</sup>, 즉 입력 데이터에서부터 선형모델을 통해 잠재변수로 인코딩하게 되는 W<sub>X</sub><sup>(1)</sup>과,
: : 이전 시점의 잠재변수로부터 정보를 받아서 현재 시점의 잠재 변수로 인코딩해주는 W<sub>H</sub><sup>(1)</sup>, 즉, 첫 번째 레이어의 가중치 행렬 W<sub>H</sub><sup>(1)</sup>를 사용하게 되는 것이고,
: : 이렇게 만든 잠재 변수를 통해 다시 출력으로 만들어주는 W<sup>(2)</sup> 가중치 행렬이 존재하게 되어, 총 3 개의 가중치 행렬이 있게 된다.
: : 명심할 점은, <u>W<sup>(2)</sup>, W<sub>(X)</sub><sup>(1)</sup>, W<sub>H</sub><sup>(1)</sup> 이렇게 세 개의 가중치 행렬은 t에 따라 변하지 않는 가중치 행렬이라는 사실을 기억하라.</u> 
: : 즉, t에 따라 변하는 것은 오로지 잠재변수와 입력 데이터에 해당하는 것이고, RNN에서 사용되는 가중치 행렬 세 개는 t에 따라 변하지 않는다는 것!
: : 이 가중치 행렬들은 동일하게 각각의 t 시점에 활용되어 모델링을 하게 되는 것이다.
: <br><b>* RNN의 역전파</b><br><br>
: : RNN의 역전파는 잠재변수의 연결그래프에 따라 <u>순차적으로</u> 계산한다. 이를 Backpropagation Through Time(BPTT)라 하며, RNN의 역전파 방법이다.
: : 잠재변수에는 두 개의 gradient가 들어오게 되는데, 다음 시점에서의 잠재변수에서 들어오게 되는 gradient vector와, 출력에서 들어오게 되는 gradient vector이다.
: : 이 잠재변수에 들어오는 gradient vector를 입력과 그 이전시점의 잠재변수로 전달하게 되고, 이를 반복해서 RNN의 학습이 이루어진다.
: <br><b>BPTT를 좀 더 살펴보자</b><br><br>
: : BPTT를 통해 RNN의 가중치행렬의 미분을 계산해보면, 아래와 같이 미분의 곱으로 이루어진 항이 계산된다. (PDF 수식 참조)
: : 시퀀스 길이가 곱해지는 term들이 불안정해지기 쉽다. (굉장히 커지거나 작아질 수 있음)
: : 기울기 소실(Vanishing gradient)의 해결책 - 시퀀스 길이가 길어지는 경우 BPTT를 통한 역전파 알고리즘의 계산이 불안정해지므로, 길이를 끊는 것이 필요하다.
: : 이를 truncated BPTT라고 부른다.
: : 이런 문제들 때문에 Vanilla RNN(기본적인 RNN 모형)은 길이가 긴 시퀀스를 처리하는데 문제가 있다.
: : 이를 해결하기 위해 등장한 RNN 네트워크가 LSTM과 GRU이다.



<h2>Git 기초</h2>

- (7강) <br><br>
: : 
<br><br>

- (8강) <br><br>
: : 


<h2>Python</h2>

- (2-2강) Function and Console I/O<br><br>
: : (Week 0) File System & Terminal
<br><br>

- (2-3강) Conditionals and Loops<br><br>
: : 파이썬의 역사와 기본 지식
<br><br>

- (2-4강) String and advanced function concept<br><br>
: :
