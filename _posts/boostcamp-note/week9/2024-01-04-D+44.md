---
layout: single
title:  "Day 44 학습정리"
categories: boostcamp-note-week9
sidebar:
  nav: "docs"
---

24/01/04 (목) 학습 내용

<h1>KLUE</h1>

<h2>(3강) BERT 언어모델 소개</h2>

- 1.1 BERT 모델 소개<br><br>
: : Seq2SEq -> Attention + Seq2Seq -> Transformer
: : 마스킹된 자연어를 원본 자연어로 복원하는 과정을 통해 학습함

- 1.2 BERT의 응용<br><br>
: : 감성 분석 (긍정/부정 분류)
: : 관계 추출
: : 의미 비교 (두 문장의 의미 유사 O/X)
: : 기계 독해 (KorQuAD 데이터셋을 사용)

- 1.3 한국어 BERT 모델<br><br>
: : ETRI의 KoBERT
: : tokenizing에 따라 성능이 달라짐
: : Advanced BERT model

- 실습<br><br>
: : 특정 역할을 위한 special token 추가
: : ex) [ENTITY] 태그 (3강 36:29)
: : 만약 vocab을 새롭게 추가했다면, 반드시 model의 embedding layer 사이즈를 늘릴 것.


<h2>(4강) BERT Pre-Training</h2>
1. BERT 학습하기<br><br>
: <br><b>1.1 BERT 모델 학습 단계</b><br><br>
: : 1) Tokenizer 만들기
: : 2) 데이터셋 확보
: : 3) Next sentence prediction (NSP)
: : 4) Masking
<br><br>
: : 왜 새로 학습해야 하나? -> 도메인 특화 task의 경우, 도메인 특화된 학습 데이터만 사용하는 것이, 기존의 BERT를 fine-tuning 하는 것보다 성능이 더 좋다는 연구 결과가 많이 나왔다.
<br><br>
: <br><b>1.2 BERT 모델 학습 실습</b><br><br>
: : 