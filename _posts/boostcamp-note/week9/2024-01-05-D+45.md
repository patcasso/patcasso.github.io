---
layout: single
title:  "Day 45 학습정리"
categories: boostcamp-note-week9
sidebar:
  nav: "docs"
---

24/01/05 (금) 학습 내용

<h1>KLUE</h1>

<h2>(5강) BERT 언어모델 기반의 단일 문장 분류</h2>

1. KLUE 데이터셋 소개<br><br>
: <br><b>1.1 KLUE 데이터셋</b><br><br>
: : 한국어 자연어 이해 벤치마크(Korean Language Understanding Evaluation, KLUE)
: : 업무를 하며 직면하게 되는 거의 모든 자연어 task 유형!
: <br><b>1.2 의존구문분석</b><br><br>
: : 단어들 사이의 관계를 분석하는 task
: : 지배소 - 의미의 중심이 되는 요소
: : 의존소 - 지배소가 갖는 의미를 보완해주는 요소 (수식)
: : 복잡한 자연어 형태를 그래프로 구조화해서 표현 가능 / 각 대상에 대한 정보 추출이 가능
<br><br>

2. 단일 문장 분류 task 소개<br><br>
: <br><b>2.1 문장 분류 task</b><br><br>
: : 1) 감정분석(Sentiment Analysis)
: : 혐오 발언 분류, 기업 모니터링 등
<br><br>
: : 2) 주제 라벨링(Topic labeling)
: : 문장의 내용을 이해하고 적절하게 범주를 분류하는 프로세스
: : 대용량 문서 분류, VoC(Voice of Customer)
<br><br>
: : 3) 언어 감지(Language Detection)
<br><br>
: : 4) 의도 분류(Intent Classification)
: : 챗봇 - 문장의 의도인 질문, 명령, 거절 등을 분석하고 적절한 답변을 주기 위해 활용
: <br><b>2.2 문장 분류를 위한 데이터</b><br><br>
: : Kor_hate (혐오 표현 데이터)
: : Kor_sarcasm (비꼬는 / 비꼬지 않은 표현의 문장)
: : Kor_sae (질문 / 명령 등의 유형 분류)
: : Kor_3i4k (평서문/질문/명령문 등 분류)
<br><br>

3. 단일 문장 분류 모델 학습<br><br>
: <br><b>3.1 모델 구조도</b><br><br>
: : BERT의 [CLS] token의 vector를 classification하는 Dense layer 사용
: <br><b>3.2 학습 과정</b><br><br>
: : Dataset 다운로드 -> Dataset 전처리 및 토큰화 -> Dataloader 설계 -> Train, Test Dataset 준비 -> TrainingArguments 설정 -> Pretrained Model import -> Trainer 설정 -> Model 학습 -> Predict 함수 구현 및 평가
<br><br>

<h2>(6강) BERT 언어모델 기반의 두 문장 관계 분류</h2>

1. 두 문장 관계 분류 task 소개<br><br>
: <br><b>1.1 task 소개</b><br><br>
: : 주어진 두 개의 문장에 대해, 두 문장의 자연어 추론과 의미론적 유사성을 측정하는 task
: <br><b>1.2 데이터 소개</b><br><br>
: : Natural Language Inference (NLI)
: : Semantic Text Pair

2. 모델 학습 실습<br><br>
: <br><b>2.1 BERT를 이용한 챗봇 만들기</b><br><br>
: : Information Retrieval Question and Answering (IRQA)

<h2>(7강) BERT 언어모델 기반의 문장 토큰 분류</h2>

1. 문장 토큰 분류 task 소개<br><br>
: <br><b>1.1 task 소개</b><br><br>
: : <u>NER (Named Entity Recognition)</u>
: : 주어진 문장의 각 token들이 어떤 범주에 속하는지 분류하는 task
: : 같은 단어라도 문맥에서 다양한 개체(Entity)로 사용됨
: : Pororo by kakaobrain
<br><br>
: : <u>POS(Part-of-speech) Tagging</u>
<br><br>
: <br><b>1.2 데이터 소개</b><br><br>
: : kor_ner - 한국해양대학교 자연어 처리 연구실에서 공개한 한국어 NER 데이터셋
: : 일반적으로, NER 데이터셋은 pos tagging도 함께 존재

2. 문장 토큰 분류 모델 학습 실습
: <br><b>2.1 문장 토큰 분류 모델 학습</b><br><br>
: : NER fine-tuning with BERT
: : 주의점) 형태소 단위의 토큰을 음절 단위의 토큰으로 분해하고, Entity tag 역시 음절 단위로 매핑시켜 주어야 한다.

<h2>(8강) GPT 언어 모델</h2>

1. GPT 언어 모델
: <br><b>1.1 GPT 모델 소개</b><br><br>
: : BERT가 자연어에 대한 임베딩 모델이라면, GPT 언어모델은 자연어의 생성에 특화된 모델이다.
: : BERT는 트랜스포머의 인코더를, GPT는 트랜스포머의 디코더를 사용한 모델이다.
: : GPT가 언어를 학습한 방법은, 다음에 나오기에 가장 적절한 단어(토큰)가 무엇인지 학습함
: : GPT-1의 경우 Classification, Similarity 측정 등에 사용되었다.
: : 엄청 큰 데이터셋을 사용하면 번역, 감정 분석 등의 자연어 task를 자연스럽게 학습한다. (ex. 영어와 프랑스어가 같이 나타나는 데이터를 학습하는 등의 경우, 혹은 특정 문장에 대해 화자가 느끼는 감정이 드러난 문장을 학습하는 경우 등)
: : Zero-shot, One-shot, Few-shot learning 을 통해 여러 자연어 task를 수행
<br><br>
: : GPT-3도 역시 트랜스포머의 디코더를 사용함
: : 다만 모델 사이즈, 학습 데이터를 크게 키운 차이가 있다.
<br><br>
: : <u>GPT-3의 한계</u>
: : GPT-3 역시 다음 단어 혹은 masked 단어를 예측하는 방식으로 학습한 언어 모델이다.
: : 이렇게 학습하는 방식으로 모든 것이 다 해결될까?
: : Weight update가 없다는 것은 모델에 새로운 지식 학습이 없다는 것.
: : 시기에 따라 달라지는 문제도 대응 불가
: : 갈수록 모델 사이즈만 키우면 되는 것인가? -> 다른 연구 방향 필요
: : 멀티 모달 정보가 필요! -> GPT는 글로만 세상을 배움

: <br><b>1.2 GPT의 응용</b><br><br>
: : What Is Next?
: : 한국어 GPT-2 모델 학습 실습


<h2>(9강) GPT 언어 모델 기반의 자연어 생성</h2>

- 강의 내용<br><br>
: : 실습 자료 참조