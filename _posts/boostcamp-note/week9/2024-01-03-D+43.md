---
layout: single
title:  "Day 43 학습정리"
categories: boostcamp-note-week9
sidebar:
  nav: "docs"
---

24/01/03 (수) 학습 내용

<h1>KLUE</h1>

<h2>(1강) 인공지능과 자연어 처리</h2>
1. 인공지능의 탄생과 자연어처리<br><br>
: <br><b>1.1 자연어처리 소개</b><br><br>
: : 앨런 튜링 '이미테이션 게임'
: : ELIZA 챗봇 - 심리 상담사의 역할을 하도록 설계된 최초의 대화형 챗봇
: <br><b>1.2 자연어처리의 응용분야</b><br><br>
: : 이미지 합성, 음성 합성 등 다양한 기술이 합쳐져서 application이 만들어진다.
: : 화자는 청자가 이해할 수 있는 방법으로 인코딩
: : 청자는 본인 지식을 바탕으로 디코딩
: : 마찬가지로, Encoder는 Decoder가 이해할 수 있는 벡터로 인코딩
: : Decoder는 본인 지식을 바탕으로 디코딩
: <br><b>1.3 자연어 임베딩</b><br><br>
: : Word2Vec - 자연어(특히 단어)의 의미를 벡터 공간에 임베딩
: : FastText - 한국어는 다양한 용언 형태를 가짐 ~ 기존의 word2vec과 유사하나, 단어를 n-gram으로 나누어 학습을 수행 (subword tokenization)
<br><br>

2. 딥러닝 기반의 자연어처리와 언어모델<br><br>
: <br><b>2.1 언어모델</b><br><br>
: : 모델이란? 일기예보 모델, 데이터 모델, 물리 모델 등 자연 법칙을 컴퓨터로 모사함으로써 시뮬레이션이 가능
: : 언어 모델이란 - '자연어'의 법칙을 컴퓨터로 모사한 모델 -> 언어 '모델'
: : 주어진 단어들로부터 그 다음에 등장한 단어의 확률을 예측하는 방식으로 학습
<br><br>
: : <u>마코프 기반의 언어 모델</u>
: : 마코프 체인 모델이라고도 함.
: : 초기의 언어 모델은 다음의 단어나 문장이 나올 확률을 통계와 단어의 n-gram을 기반으로 계산
: : 딥러닝 기반의 언어모델은 해당 확률을 최대로 하도록 하는 네트워크를 학습
<br><br>
: : <u>RNN 기반의 언어 모델</u>
: : 마지막 출력은 앞선 단어들의 '문맥'을 고려해서 만들어진 최종 출력 vector -> context vector
: : one-to-many, many-to-one, many-to-many 등의 여러 태스크 가능
: <br><b>2.2 Seq2Seq</b><br><br>
: : Encoder layer - RNN 구조를 통해 Context vector 획득
: : Decoder layer - 획득된 Context vector를 입력으로 출력을 예측
: : 음성 인식기, 이미지 캡셔닝, 형태소 분석기, 번역 모델, 챗봇 등 다양한 task 활용 가능
: <br><b>2.3 Attention</b><br><br>
: : 인간이 정보처리를 할 때, 모든 sequence를 고려하며 정보처리를 하는 것이 아니라 중요한 정보 위주로 처리함
: : 하지만, 여전히 RNN이 순차적으로 연산이 이뤄짐에 따라 연산 속도가 느림
: <br><b>2.4 Self-attention</b><br><br>
: : RNN의 시퀀스를 버리고, 모든 토큰을 All-to-all로 연결함
: : 인코더와 디코더를 분리해 놓는 것이 아니라, 하나의 구조 안에 둘 다 들어가 있는 형태


<h2>(2강) 자연어의 전처리</h2>

- 인트로<br><br>
: : 원시 데이터를 기계 학습 모델이 학습하는데 적합하게 만드는 프로세스
: : Task의 성능을 가장 확실하게 올릴 수 있는 방법!
: : 모델을 아무리 바꾸고 튜닝하더라도, 데이터 자체가 문제가 있다면 성능이 나올 수 없다. 가장 중요한 것은 데이터이다.
: : Garbage in, garbage out!
<br><br>

1. 자연어 전처리<br><br>
: <br><b>1.1 자연어처리의 단계</b><br><br>
: : Task 설계
: : 필요 데이터 수집
: : 통계학적 분석
: : 전처리
: : Tagging
: : Tokenizing
: : 모델 설계
: : 모델 구현
: : 성능 평가
: : 완료
: <br><b>1.2 Python string 관련 함수</b><br><br>
: : 강의자료 참조해서 다 실제로 써보자
: <br><b>1.3 전처리 실습</b><br><br>

2. 자연어 토크나이징<br><br>
: <br><b>2.1 한국어 토큰화</b><br><br>
: : 토큰이 되는 기준은 태스크마다 다를 수 있음 (어절, 단어, 형태소, 음절, 자소 등)
: : 한국어는 조사나 어미를 붙여서 말을 만드는 교착어로, 띄어쓰기만으로는 부족
: : 한국어에서는 어절이 의미를 가지는 최소 단위인 형태소로 분리
