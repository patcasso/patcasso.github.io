---
layout: single
title:  "Day 81 학습정리"
categories: boostcamp-note-week17
sidebar:
  nav: "docs"
---

24/02/26 (월) 학습 내용

<h1>Product Serving</h1>

<h2>(1강) Serving의 다양한 패턴</h2>
- 강의 키워드<br><br>
: - ML 모델 서빙
: - Batch Serving
: - Online Serving
: - Serving Pattern
<br><br><br>

1. Serving : 모델을 세상으로<br><br>
: <br><b>1.1 Serving의 정의</b><br><br>
: : 요청(Request)를 받아 예측 결과를 고객(Client, 웹이나 앱, 서버 등)에게 전달하는 행위
: : 데이터에 기반하여 모델 예측 결과를 제공하는 것
<br><br>
: : Research 단계와 Production 단계
: : Research 단계 - Data / Feature, Model
: : Production 단계 - Data / Feature, Model -> Client
<br><br>
: : Production(Real World) 환경에 모델을 사용할 수 있도록 배포하는 것
: : 머신러닝 모델의 서비스화
: : 연구 환경 이후에 진행되는 작업
: : Input을 Model에게 주입하면, 모델이 Output을 반환
: : Model은 머신러닝 모델, 딥러닝 모델, LLM 등 다양할 수 있음
: <br><b>1.2 Serving의 대표적인 예시</b><br><br>
: : 유튜브의 메인 화면 - 개인화 추천 알고리즘으로 영상 추천 (시청 기록 데이터 기반)
: : DeepL의 번역기 - Input -> Output 언어로 번역
: : 쏘카의 AI 기반 세차 효율화 : 차량 외관 사진 -> 차량의 상태 분류
: <br><b>1.3 Serving의 종류</b><br><br>
: : 1) Batch Serving
: : Batch - 일괄, 묶음
: : 데이터를 일정 묶음 단위로 서빙(예 : 14 ~ 15시 사이에 생성된 데이터(예측))
: : 정기 배송처럼 매일 20시에 서빙받기를 원하는 경우 - Batch Serving (많은 양을 일정 주기로 한꺼번에 서빙)
<br><br>
: : 2) Online(Real Time) Serving
: : Online - 연결, 실시간
: : 클라이언트가 요청할 때 서빙 (예: API Request)
: : 요청할 때 데이터를 같이 제공
: : 동시에 요청이 여러 개가 들어올 경우, 확장 가능하도록 준비해야 함
: <br><b>1.4 어떤 Serving을 사용해야 할까?</b><br><br>
: : <u>Serving 사용의 기준</u>
: : 정답은 없음
: : 문제의 상황, 문제 정의(문제 정의가 너무 크면 오래 걸린다), 제약 조건(법적인 이슈), 개발할 인력 수, 데이터 저장 형태, 레거시 유무 등에 따라 결정
<br><br>
: **Batch Serving**
<br><br>
: : <u>상황</u>
: : 실시간 응답이 중요하지 않은 경우 - 데이터 처리에 일정 시간이 소요되어도 괜찮은 경우
: : 대량의 데이터를 처리할 때
: : 정기적인 일정으로 수행할 때
<br><br>
: : <u>인력</u>
: : 인력이 적은 경우 Batch Serving이 Online Serving 보다 쉬울 수 있음
<br><br>
: : <u>데이터 저장 형태</u>
: : RDB, 데이터 웨어하우스 -> Batch Serving이 나음
<br><br>
: : <u>Batch Serving의 예시</u>
: : DoorDash의 레스토랑 추천
: : Netflix의 추천 (2021년)
: : 1시간 단위, 4시간 단위, 24시간 단위로 예측 후 DB에 저장 -> 서비스(앱/웹)는 DB에 있는 예측 결과 활용
<br><br>
: **Online Serving**
<br><br>
: : <u>상황</u>
: : 실시간 응답이 중요한 경우
: : 개별 요청에 대한 맞춤 처리가 중요할 때
: : 동적인 데이터에 대응할 때 (데이터가 지속적으로 변하는 경우)
<br><br>
: : <u>인력</u>
: : API 서버, 실시간 처리 등의 경험 필요
<br><br>
: : <u>데이터 저장 형태</u>
: : 요청할 때 같이 데이터 제공 (API, 메시지)
<br><br>
: : <u>예시</u>
: : 유튜브의 추천 시스템(새로고침)
: : 번역
: : 은행의 사기 탐지 시스템 - 실시간으로 거래의 이슈 탐지
: : 요청(Request)이 오면 바로 요청에 대한 응답(Response)을 제공
<br><br><br>

2. 단순한 Serving 패턴<br><br>
: <br><b>2.1 디자인 패턴</b><br><br>
: : 소프트웨어 개발 분야에도 패턴이 존재 -> 디자인 패턴
: : '설계하다'라는 의미에서의 디자인
: : 단순히 외관을 의미하는 것이 아니라 구조, 구성 요소의 관계, 시스템의 전반적인 행동 방식
: : 즉, 소프트웨어를 어떻게 구성하고 상호 작용할지를 담은 내용
: : 디자인 패턴 = 템플릿
: : 과거부터 문제를 해결한 사람들이 반복된 내용을 패턴으로 정리
: : 코드의 재사용성, 가독성, 확장성 등을 향상시키기 위한 목적으로 도입
: : 주로 객체 지향 프로그래밍에서 사용, but 다른 프로그래밍 패러다임에도 유용
: : 개발 과정 커뮤니케이션에서 이런 패턴 사용하기도 함
: : 안티 패턴 - 보통 좋지 않다고 알려진 패턴
: <br><b>2.2 Batch 패턴</b><br><br>
: : <u>머신러닝의 특수성</u>으로 별도의 디자인 패턴이 생김
: : 일반 소프트웨어는 Only Code이지만
: : ML에서는 Data, Model, Code가 존재
<br><br>
: : 대용량 Model Load
: : 모델의 버젼 관리
: : 데이터 대량으로 가져와서 전처리 필요
: : 데이터를 통계적으로 확인하여 이상치 제외
: : 예측 요청 후 반응 시간이 오래 소요될 수 있음(모델이 연산하는 과정 이슈)
: : 개발에도 설계가 중요 -> 처음에 어떻게 설계하느냐에 따라 바꿀 수 있고 없고가 결정됨
<br><br>
: : <u>예측 서버</u> - 추론에 집중하는 서버
: : <u>데이터베이스</u>
: : <u>서비스 서버</u> - 프론트엔드에 집중하는 서버
<br><br>
: : Story 강의자료 참고
: : 핵심 - 실시간성이 필요 없는 경우에 주기적으로 예측 결과를 DB에 저장하고, 활용하는 쪽은 DB 에서 결과를 읽어와 사용
: : <u>Job Management Server</u>
: : 작업을 실행하는 서버
: : Apache Airflow 등을 주로 사용
: : 특정 시간에 주기적으로 Batch Job을 실행시키는 주체
: : 작업을 실행하는 서버
: : Job이 실행되는 과정에 Model Load, Data Load도 포함
: : Python Script 그냥 실행시키는 경우도 있고, Docker Image로 실행하는 경우도 존재
<br><br>
: : <u>Data</u>
: : 서비스에서 사용하는 DB(AWS RDS 등) 또는 데이터 웨어하우스에 저장
: : 서비스 서버에서도 데이터를 불러오는 스케쥴링 Job이 존재 => 특정 시간 단위로 가져옴
<br><br>
: : Batch 패턴의 장점
: : 기존에 사용하던 코드를 재사용 가능
: : API 서버를 개발하지 않아도 되는 단순함
: : 서버 리소스를 유연하게 관리 가능 (오래 걸릴 Job에 서버 리소스 추가 투입)
: : Batch 패턴의 고민할 점:
: : 별도의 스케줄러(예: Apache Airflow) 필요
: <br><b>2.3 Web Single 패턴</b><br><br>
: : 웹 기반 애플리케이션 / 단일, 하나의 요청을 처리
<br><br>
: : <u>Story</u>
: : Batch 패턴으로 서빙했더니, 결과 반영에 시간 텀 존재
: : 더 실시간에 가깝게 할 방법은 없을까?
<br><br>
: : <u>방법</u>
: : 모델이 항상 Load된 상태에서 예측을 해주는 API 서버를 만들고, 추론 결과가 필요한 경우 서비스 서버에서 이 예측 서버에 직접 요청하기
<br><br>
: : <u>핵심</u>
: : API 서버 코드에 모델을 포함시킨 뒤 배포
: : 예측이 필요한 곳(클라이언트, 서버 등)에서 직접 Request 요청
<br><br>
: : <u>예측/추론 Server</u>
: : FastAPI, Flask 등으로 단일 REST API 서버를 개발 후 배포
: : ex) POST api-server-url/predicgt로 예측
: : API 서버가 실행될 때 모델을 로드
: : API 로직 내에 전처리도 같이 포함
<br><br>
<u>Load Balancer</u>
: : 트래픽을 분산시켜서 서버에 과부하를 걸리지 않도록 해줌
: : Nginx, Amazon ELB(Elastic Load Balancing) 등을 사용
: : 장점 - 보통 하나의 프로그래밍 언어로 진행, 아키텍처의 단순함, 처음 사용할 때 좋은 방식
: : 단점 - 구성 요소 하나(모델, 전처리 코드 등)가 바뀌면 전체 업데이트가 필요
: : 모델이 큰 경우, 로드에 시간이 오래 걸릴 수 있음
: : 요청 처리가 오래 걸리는 경우, 서버에 부하 걸릴 수 있음
<br><br>
: : <u>Usecase</u>
: : 예측 서버 빠르게 출시하고 싶은 경우
: : 실시간으로 예측 결과 얻을 필요 있는 경우
: <br><b>2.4 Synchronous 패턴</b><br><br>
: : (동기식 패턴)
: : 하나의 작업이 끝날 때까지 다른 작업을 시작하지 않고 기다리고, 작업이 끝나면 새로운 작업을 시작하는 방식
<br><br>
: : <u>Story</u>
: : FastAPI 모델을 Web Single 패턴으로 구현
: : 클라이언트는 API 서버로 요청을 한 뒤 요청이 끝날 때까지 기다려야 함
: : Web Single 패턴을 동기적으로 서빙
: : 기본적으로 대부분의 REST API 서버는 동기적으로 서빙
: : 장점 - 아키텍처의 단순함, 예측 완료시까지 프로세스가 다른 작업을 할 필요가 없어서 Workflow가 단순해짐
: : 단점 - 예측 속도가 병목이 됨 (동시에 1000개의 요청이 올 경우 대기 시간이 길어지거나 Drop 혹은 Timeout)
: : 예측 지연으로 사용자 경험이 악화될 수 있음
<br><br>
: : <u>Usecase</u>
: : 예측 결과에 따라 클라이언트의 로직이 즉각적으로 달라져야 하는 경우
: <br><b>2.5 Asynchronous 패턴</b><br><br>
: : 비동기식 패턴
: : 하나의 작업을 시작하고, 결과를 기다리는 동안 다른 작업을 할 수 있음
: : 작업이 완료되면 시스템에서 결과를 알려줌
<br><br>
: : <u>Story</u>
: : API 서버에서 수많은 요청을 감당할 수 없어짐
: : 응답이 느려지니, 클라이언트에서도 응답 받은 이후의 로직을 진행하지 못함
<br><br>
<u>핵심</u>
: : 앞에서 설명한 Web Single 패턴을 비동기적으로 구현
<br><br>
: : <u>Queue</u>
: : 클라이언트와 예측 서버 사이에 메시지 시스템(Queue)을 추가
: : 대표적인 메시지 프레임워크 - Apache Kafka
: : Push - 메시지를 저장
: : Pull - 메시지를 가져와서 작업(예측) 수행
<br><br>
: : <u>장점</u>
: : 클라이언트와 예측 프로세스가 분리 -> 관계가 의존적이지 않음
: : 클라이언트가 예측을 기다릴 필요가 없음
<br><br>
: : <u>고민할 점</u>
: : 메시지 Queue 시스템을 만들어야 함
: : 전체적으로 구조가 복잡해짐
: : 완전한 실시간 예측엔 적절하지 않음(메시지를 가져갈 때 시간이 소요될 수 있음)
<br><br>
: : <u>Usecase</u>
: : 예측과 클라이언트 진행 사이의 의존성이 없는 경우
<br><br><br>

3. Anti Serving 패턴<br><br>
: : 좋지 않은 예 두 가지
: <br><b>3.1 Online Bigsize 패턴</b><br><br>
: : 실시간 대응이 필요한 온라인 서비스에 예측이 오래 걸리는 모델을 사용하는 경우
: : 예) 서버가 실행되는데 몇 분씩 소요되고, 요청에 대해 응답이 몇 초씩 걸릴 경우
: : 문제)
: : 일반적으로 Bigsize 모델을 배포할 때 서버 실행과 서빙이 느림
: : 속도와 비용 Tradeoff를 조절해 <u>모델 경량화</u> 하는 작업이 필요
: : 대안)
: : 실시간이 아닌 Batch로 변경하는 것도 가능한지 검토
: : 중간에 캐시 서버를 추가하고, 전처리를 분리하는 것도 Bigsize를 탈피하는 방법
: <br><b>3.2 All-in-one 패턴</b><br><br>
: : 하나의 서버에 여러 예측 모델을 띄우는 경우
: : 예) predict1(얼굴인식), predict2(개체 탐지), predict3(자연어)
: : 라이브러리 선택 제한이 존재함
: : 장애가 발생할 경우 시스템이 마비됨 (SPOF, Single Point of Failure)
<br><br><br>

<h2>(2강)</h2>
- 강의 키워드<br><br>
: - 
: - 
: - 
<br><br><br>

- 소개<br><br>
: :
: : 
<br><br><br>

- 소제목1<br><br>
: :
: : 
<br><br><br>

- 소제목2<br><br>
: :
: :
<br><br><br>
