---
layout: single
title:  "Day 23 학습정리"
categories: boostcamp-note-week5
sidebar:
  nav: "docs"
---

23/12/06 (수) 학습 내용

<h1>NLP 이론</h1>

<h2>(9강) Self-supervised Pre-training Models</h2>
- 강의 키워드<br><br>
: - GPT-1
: - BERT
: - Transfer Learning


- 최신 발전 동향<br><br>
: : Transformer 및 Self-attention 블록은 범용적인 시퀀스 인코더 혹은 디코더로, NLP 뿐만 아니라 다른 분야에서도 활발히 사용되고 있다.
: : Self-attention block을 점점 더 많이 쌓아서, 12, 24, 혹은 그 이상의 layer를 깊이 쌓은 모델을 만들고, 대규모 학습데이터를 통해 학습할 수 있는 self-supervised learning framework로 만들어 transfer learning을 통해 NLP 태스크를 잘 처리
: : greedy decoding에서 벗어나지 못하고 있다는 한계점도 가지고 있다.<br><br>

- GPT-1<br><br>
: : OpenAI에서 나온 모델
: : \<S>, \<E>등 special token을 제안
: : Seq2Seq처럼 입력과 출력 시퀀스가 따로 있는 것이 아니라, 
: : 영어 데이터로 된 수많은 웹사이트를 다운받아서, 그 데이터를 가지고 추출된 문장들을 통해 순차적으로 다음 단어를 예측하는 Language Modeling 태스크를 통해 GPT-1 모델이 학습된다.
: : 단순 Language Modeling 뿐만 아니라, 문장 레벨이나 큰 데이터에 대해서도 잘 학습이 되도록 구성했다.
<br><br>
: : <u>Classification</u> 문장레벨의 감정분류 태스크를 학습하려면, Extract 토큰에 해당하는 인코딩 벡터를 최종 아웃풋 layer에 입력 벡터를 줌으로써 긍정인지 부정인지 판단하게 하는 학습 모델
<br><br>
: : <u>Entailment</u> 은 논리적인 내포관계 등을 파악하는데, 다수의 문장으로 이루어진 입력을 받아 예측을 수행해야 한다. 이런 경우를 대비하기 위해 Premise와 Hypothesis 사이에 Delimiter 토큰을 추가하고, 마지막에 Extract 토큰을 추가해 해당 벡터를 최종 아웃풋 레이어에 통과해주고, 이것이 논리적 관계에 대한 분류 태스크를 수행하게 된다.
: : Extract token이 학습 과정에서 Query로 사용되어서, 태스크가 필요로 하는 여러 정보들을 주어진 입력문장들로부터 추출할 수 있어야 한다.
<br><br>
: : 이외에도 <u>Similarity</u>, <u>Multiple Choice</u> 등의 task에 대한 학습
<br><br>
: : 이러한 통합적 학습이 완료된 GPT 모델을 소량의  데이터셋을 통한 특정 task를 위해 Transfer Learning으로 활용할 때는 다음과 같은 과정을 거친다.
: : 예컨대 글의 카테고리를 여러 선택지 중 하나로 분류해야 하는 모델의 경우, 마지막 output layer를 떼어버리고, 대신 우리의 task를 위한 추가적인 하나의 layer를 붙인다.
: : 이전에 기 학습된 모델에는 learning rate를 굉장히 작게 줘서, 많이 변화를 주지 않는 형태로 활용
: : 즉, self-supervised learning을 통해 얻을 수 있는 지식을, 라벨이 있는 소량의 데이터만 있는 ex. 분류 태스크에 적용해 활용할 수 있는 방식으로 사용되고 있다.
<br><br>
: : 이러한 전이학습 방식은 특정 task에 관련해 소량의 데이터만으로 트레이닝한 모델들보다 더 나은 성능을 보여준다.

- BERT<br><br>
: : 사전학습 모델의 두 번째 모델인 BERT
: : 현재까지도 가장 널리 쓰이는 Pre-training 모델
: : GPT와 마찬가지로, Language Modeling으로 pre-training을 한 모델이다.<br><br>

- Pre-training Tasks in BERT
: <br><b>Masked Language Model</b><br><br>
: : 몇 %의 단어를 MASK 할지를 hyperparameter로 정해야 한다. (e.g., use $k$ = 15%)
: : 너무 적게 masking하면 학습이 너무 효율이 떨어지고,
: : 너무 많이 masking하면 문맥을 파악할 수 있는 정보가 부족해진다.
<br><br>
: : 해당 15% 중에서도 다 [MASK]로 처리하지 않고,
: : 80%의 경우에는 [MASK]로 치환하고,
: : 10% 정도는 다른 무작위 단어로 치환하고, (문제의 난이도를 높임)
: : 나머지 10% 정도는 원래 문장을 그대로 유지한다.
<br><br>
: <br><b>Next Sentence Prediction</b><br><br>
: : 두 개의 문장을 연속으로 이어주고, 사이 및 끝에 [SEP] 토큰을 삽입해 구분해준다.
: : [CLS] 토큰을 문장 맨 앞에 추가하여 전체 sentence를 인코딩하고, [MASK] 자리의 토큰에는 예측을 수행하게 되고, [CLS] 토큰은 인코딩 벡터로 아웃풋 레이어 하나를 두어 binary classification 하나를 수행하게 한다. 
: : Ground Truth는 두 문장이 실제 인접한 문장인지 아닌지에 대한 label (IsNext, NotNext)
<br><br>

- BERT Summary<br><br>
: <br><b>1. Model Architecture</b><br><br>
: BERT BASE: L = 12, H = 768, A = 12
: BERT LARGE: L = 24, H = 1024, A = 16
<br><br>
: : L = self-attention 블록 개수, 
: : H = 각 self-attention 블록에서 항상 같게 유지하는 encoding vector의 차원 수, 
: : A = 각 레이어별 attention head 숫자
<br><br>
: <br><b>2. Input Representation</b><br><br>
: : 입력 Seqence를 넣을 때 word별로 embedding vector를 사용하는 것이 아니라, 
: : word를 좀 더 잘게 쪼개서 각각 sequence의 단위를 sub-word라고 불리는 단위로 임베딩하고, 그것을 입력 벡터로 넣어주는 방식을 취했다. (Word-piece embedding)
: : sub-word를 어떤 식으로 인코딩하고 사전을 구축하는지는 과제에서 다루었다!
<br><br>
: : Learned positional encoding
: : 기존 transformer에서는 사전에 정의된 함수값으로 positional embedding을 주었다면, 
: : positional embedding도 random init을 통해 임베딩 벡터 자체도 학습에 의해 최적화된 값으로 도출해낸다.
: : [CLS] - Classification Embedding
: : Packed sentence embedding [SEP]
: : Segment Embedding - BERT를 학습할 때, 단어별 MASK 단어를 예측하는 task와, 주어진 두 문장으로 이루어진 한 sequence가 실제 인접 문장인지 아닌지 예측하는 문장 레벨에서의 task가 있었다.
: :  두 개의 문장을 SEP 토큰으로 구분해서 넣는 경우, 단어별로 positional embedding이 더해지는데, SEP 토큰 이후에 나오는 첫 번째 단어는 '두 번째 문장의 첫 번째 단어이다' 라는 정보도 분리해서 넣어줘야 한다.
: : 그래서 포지션 임베딩은 그대로 순차적으로 부여하되, 여기에 또 다른 종류의 position embedding으로 segment embedding이라는 것을 넣게 된다. (첫 번째 문장에서는 모든 워드들에 대해 동일한 벡터, 두 번째 문장에서도 모든 워드들에 대해 동일한 벡터를 더해준다.)
<br><br>
- BERT와 GPT2의 차이점 더 자세히 보기<br><br>
: : GPT의 경우, 주어진 시퀀스를 인코딩 할 때, 바로 다음 단어를 예측하는 task를 수행해야 하기 때문에 특정한 timestep에서 다음에 나타나는 단어에 대한 접근을 허용하면 안된다.
: : 마치 transformer에서 decoder에서의 self-attention이 masked 형태의 self-attention으로 사용되었던 것과 같다.
: : 특정 timestep에서는 자기 자신을 포함하여 왼쪽에 있는 정보를 access 하는 패턴을 보인다.
: : 기본적으로 Seq 인코딩을 하기위해 쓰는 attention 블록은 masked self-attention을 사용한다.
: : 그러나 BERT의 경우, [MASK]단어를 포함하여 전체 주어진 모든 단어들에 대해 접근을 허용함으로써, attention 패턴은 모두가 모두를 볼 수 있도록 하는, transformer에서 인코더에서 사용하던 self-attention 모듈을 사용하게 된다.
<br><br>

- BERT: Fine-tuning Process<br><br>
: : Sentence pair에 대해 classification을 하는 task에서는, 두 개의 문장을 [SEP] 토큰으로 구분된 하나의 시퀀스로 인코딩을 한 후, 각각 단어에 대한 인코딩 벡터를 얻었다면, [CLS]토큰에 해당하는 벡터를 아웃풋 벡터에 줘서 다수 문장에 대한 예측 태스크를 수행할 수 있다.
: : 단일 문장에 대한 예측은 문장을 하나만 주고, 첫 번째 [CLS]토큰을 주고 해당 [CLS]토큰의 아웃풋을 최종 아웃풋 레이어에 넣어서 classification을 수행한다.
: : Qeustion Answering은 뒤에서 더 자세히.
: : Single Sentence Tagging Task(각 단어별로 태깅, ex. POS tagging)의 경우, CLS토큰을 포함해 각각의 단어에 대한 인코딩 벡터가 얻어지면, 해당 벡터를 공통의 아웃풋 벡터에 추가해서 태스크를 수행하게 된다.<br><br>

- BERT vs GPT-1<br><br>
: : 학습 데이터에 있어서는 GPT는 800M words(BookCorpus), BERT는 2,500M words(BookCorpus, Wikipedia)로 학습
: : BERT에서는 GPT-1과는 달리 SEP, CLS 토큰 및 segment embedding을 입력단에서 더해주어서 더 잘 구분할 수 있게 했다.
: : Hyperparmeter로 BERT와 GTP의 한 번에 학습하는 단어의 수인 Batch size는, BERT - 128,000 words > GPT - 32,000 words
: : 일반적으로 더 큰 사이즈의 배치를 사용하면 최종 모델 성능이 더 좋아지고, 학습도 더 안정화되는 현상을 보여준다.
: : 이는 gradient descent를 수행할 때, 일부의 데이터만으로 도출된 gradient로 파라미터를 업데이트 할때보다 다수의 데이터에서 나온 평균 gradient로 파라미터 업데이트 할때가 더 성능이 좋아지기 때문이다.
: : 그러나 batch size를 키우기 위해서는 더 많은 GPU 메모리를 필요로 한다.
: : GPT는 자연어처리의 여러 Downstream 태스크에 대한 fine-tuning에서 동일하게 5e-5라는 learning rate를 썼지만, BERT는 task별로 learning rate도 각각 optimize를 수행하여 학습하였다.
<br><br>
: : BERT도 Transfer Learning 과정에서 GPT와 비슷하게, 최종적인 Transformer 모델에서 나오는 인코딩 아웃풋 벡터들을 얻고 난 후에,
: : 이후 각 MASK 토큰에 대한 prediction을 수행하는 아웃풋 레이어를 제거하고,
: : main task를 위한 아웃풋 레이어를 추가적으로 접붙이는 식으로 모델을 구성했다. (가장 윗 레이어는 random initialization을 한 후 downstream task를 위한 데이터를 통해 학습하고, 기 학습된 트랜스포머 인코더의 파라미터들은 상대적으로 작은 learning rate를 사용해 조금만 변화하도록 하여, 일반화 가능한 지식이 최대한 많이 유지될 수 있도록 학습하였다.)<br><br>

- Machine Reading Comprehension (MRC), Question Answering<br><br>
: : 기계 독해에 기반한 질의응답 태스크
: : 이를 훈련하기 위한 대표적인 데이터셋 중 SQuAD 데이터가 있다. (크라우드 소싱을 통해 많은 사람들에게 task를 수행하도록 하여 수집된 데이터)
: : The Stanford Question Answering Dataset
: : <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD 공식 웹사이트</a>
: : ALBERT, ROBERTA 등 BERT를 조금씩 개선한 다양한 모델들이 태스크에서의 순위권에 올라있다.
<br><br>
: <br><b>BERT: SQuAD 1.1</b><br><br>
: : 질문과 답을 SEP 토큰을 통해 concat 해서 하나의 sequence로 만들어 인코딩을 진행한다.
: : 지문에서 답에 해당하는 문구가 시작하는 위치를 예측하기 위해, 각 워드별로 스칼라 값을 얻어낸다.
: : 여러 워드 중, 답에 해당하는 단어가 어느 단어에서 시작하는지를 먼저 예측한다.
: : softmax loss를 통해 모델을 학습한다.
: : 답이 끝나는 시점도 예측해야 하는데, 또 다른 fully connected layer를 만들어 이를 통과하여 각 워드 인코딩 벡터가 스칼라값이 나오게 하여 이를 softmax에 통과시킨 후, 두 번째 버젼의 fully connected layer를 통해 ending word 포지션을 예측하도록 한다.
<br><br>
: <br><b>BERT: SQuAD 2.0</b><br><br>
: : 질문에 대한 답을 찾을 수 없는 데이터셋까지도 포함되어있는 데이터셋.
: : 예측해야 하는 task가 먼저 일단 답이 있는지 없는지를 판단하고,
: : 만약 답이 있다면 이전 방식대로 답에 해당하는 문구를 예측하게 된다.
: : 이 경우 질문과 지문을 concat해 BERT를 통해 인코딩하고, 이렇게 나온 CLS 토큰을 binary classification을 수행하게 한 후 cross-entropy loss를 통해 학습시킨다.
: : 최종적으로 예측에 사용할때는, CLS 토큰을 통해 답이 있는지 없는지를 먼저 예측하고 이후에 앞에서 말한 starting/ending 포지션을 예측하는 output layer를 구동하여 답에 해당하는 문구를 예상할 수 있게 된다.
<br><br>

- BERT on SWAG<br><br>
: : 다음에 올 적절한 문장 고르기 task
: : 가능한 선택지를 차례대로 [CLS] 토큰을 통해 질문과 concat하여 fully connected layer를 통해 스칼라 값들로 예측하게 하고,
: : 이를 통해 선택지의 개수만큼 스칼라 값이 나오게 되면, 이를 softmax에 입력으로 주고, 정답인 ground truth의 확률이 100%가 나오게 하는 softmax loss를 통해 전체 모델을 학습하게 된다.
<br><br>

- BERT: Ablation Study<br><br>
: : 모델 사이즈를 점점 키울수록(파라미터 수) 여러 다운스트림 태스크에 대한 능력이 점점 좋아진다
: : 가능하다면 모델사이즈를 더 키우면 pre-training을 통한 downstream task에 적용했을 때의 성능이 점점 더 오를 것이라고 전망하고 있다.

<h2>(2강)</h2>
- 강의 키워드<br><br>
: - 
: - 
: - 

- 소개<br><br>
: :
: : 

- 소제목1<br><br>
: :
: : 

- 소제목2<br><br>
: :
: :



<h2>Peer Session</h2>
- 1) 주제 1<br><br>
: : 
: : 
- 2) 주제 2<br><br>
: : 
: : 