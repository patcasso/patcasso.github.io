---
layout: single
title:  "Day 25 학습정리"
categories: boostcamp-note-week5
sidebar:
  nav: "docs"
---

23/12/08 (금) 학습 내용

<h1>NLP 이론</h1>

<h2>(9강) Self-supervised Pre-training Models</h2>
- 강의 키워드<br><br>
: - GPT-1
: - BERT
: - Transfer Learning


- 최신 발전 동향<br><br>
: : Transformer 및 Self-attention 블록은 범용적인 시퀀스 인코더 혹은 디코더로, NLP 뿐만 아니라 다른 분야에서도 활발히 사용되고 있다.
: : Self-attention block을 점점 더 많이 쌓아서, 12, 24, 혹은 그 이상의 layer를 깊이 쌓은 모델을 만들고, 대규모 학습데이터를 통해 학습할 수 있는 self-supervised learning framework로 만들어 transfer learning을 통해 NLP 태스크를 잘 처리
: : greedy decoding에서 벗어나지 못하고 있다는 한계점도 가지고 있다.<br><br>

- GPT-1<br><br>
: : OpenAI에서 나온 모델
: : \<S>, \<E>등 special token을 제안
: : Seq2Seq처럼 입력과 출력 시퀀스가 따로 있는 것이 아니라, 
: : 영어 데이터로 된 수많은 웹사이트를 다운받아서, 그 데이터를 가지고 추출된 문장들을 통해 순차적으로 다음 단어를 예측하는 Language Modeling 태스크를 통해 GPT-1 모델이 학습된다.
: : 단순 Language Modeling 뿐만 아니라, 문장 레벨이나 큰 데이터에 대해서도 잘 학습이 되도록 구성했다.
<br><br>
: : <u>Classification</u> 문장레벨의 감정분류 태스크를 학습하려면, Extract 토큰에 해당하는 인코딩 벡터를 최종 아웃풋 layer에 입력 벡터를 줌으로써 긍정인지 부정인지 판단하게 하는 학습 모델
<br><br>
: : <u>Entailment</u> 은 논리적인 내포관계 등을 파악하는데, 다수의 문장으로 이루어진 입력을 받아 예측을 수행해야 한다. 이런 경우를 대비하기 위해 Premise와 Hypothesis 사이에 Delimiter 토큰을 추가하고, 마지막에 Extract 토큰을 추가해 해당 벡터를 최종 아웃풋 레이어에 통과해주고, 이것이 논리적 관계에 대한 분류 태스크를 수행하게 된다.
: : Extract token이 학습 과정에서 Query로 사용되어서, 태스크가 필요로 하는 여러 정보들을 주어진 입력문장들로부터 추출할 수 있어야 한다.
<br><br>
: : 이외에도 <u>Similarity</u>, <u>Multiple Choice</u> 등의 task에 대한 학습
<br><br>
: : 이러한 통합적 학습이 완료된 GPT 모델을 소량의  데이터셋을 통한 특정 task를 위해 Transfer Learning으로 활용할 때는 다음과 같은 과정을 거친다.
: : 예컨대 글의 카테고리를 여러 선택지 중 하나로 분류해야 하는 모델의 경우, 마지막 output layer를 떼어버리고, 대신 우리의 task를 위한 추가적인 하나의 layer를 붙인다.
: : 이전에 기 학습된 모델에는 learning rate를 굉장히 작게 줘서, 많이 변화를 주지 않는 형태로 활용
: : 즉, self-supervised learning을 통해 얻을 수 있는 지식을, 라벨이 있는 소량의 데이터만 있는 ex. 분류 태스크에 적용해 활용할 수 있는 방식으로 사용되고 있다.
<br><br>
: : 이러한 전이학습 방식은 특정 task에 관련해 소량의 데이터만으로 트레이닝한 모델들보다 더 나은 성능을 보여준다.

- BERT<br><br>
: : 사전학습 모델의 두 번째 모델인 BERT
: : 현재까지도 가장 널리 쓰이는 Pre-training 모델
: : GPT와 마찬가지로, Language Modeling으로 pre-training을 한 모델이다.<br><br>

- Pre-training Tasks in BERT
: <br><b>Masked Language Model</b><br><br>
: : 몇 %의 단어를 MASK 할지를 hyperparameter로 정해야 한다. (e.g., use $k$ = 15%)
: : 너무 적게 masking하면 학습이 너무 효율이 떨어지고,
: : 너무 많이 masking하면 문맥을 파악할 수 있는 정보가 부족해진다.
<br><br>
: : 해당 15% 중에서도 다 [MASK]로 처리하지 않고,
: : 80%의 경우에는 [MASK]로 치환하고,
: : 10% 정도는 다른 무작위 단어로 치환하고, (문제의 난이도를 높임)
: : 나머지 10% 정도는 원래 문장을 그대로 유지한다.
<br><br>
: <br><b>Next Sentence Prediction</b><br><br>
: : 두 개의 문장을 연속으로 이어주고, 사이 및 끝에 [SEP] 토큰을 삽입해 구분해준다.
: : [CLS] 토큰을 문장 맨 앞에 추가하여 전체 sentence를 인코딩하고, [MASK] 자리의 토큰에는 예측을 수행하게 되고, [CLS] 토큰은 인코딩 벡터로 아웃풋 레이어 하나를 두어 binary classification 하나를 수행하게 한다. 
: : Ground Truth는 두 문장이 실제 인접한 문장인지 아닌지에 대한 label (IsNext, NotNext)
<br><br>

- BERT Summary<br><br>
: <br><b>1. Model Architecture</b><br><br>
: BERT BASE: L = 12, H = 768, A = 12
: BERT LARGE: L = 24, H = 1024, A = 16
<br><br>
: : L = self-attention 블록 개수, 
: : H = 각 self-attention 블록에서 항상 같게 유지하는 encoding vector의 차원 수, 
: : A = 각 레이어별 attention head 숫자
<br><br>
: <br><b>2. Input Representation</b><br><br>
: : 입력 Seqence를 넣을 때 word별로 embedding vector를 사용하는 것이 아니라, 
: : word를 좀 더 잘게 쪼개서 각각 sequence의 단위를 sub-word라고 불리는 단위로 임베딩하고, 그것을 입력 벡터로 넣어주는 방식을 취했다. (Word-piece embedding)
: : sub-word를 어떤 식으로 인코딩하고 사전을 구축하는지는 과제에서 다루었다!
<br><br>
: : Learned positional encoding
: : 기존 transformer에서는 사전에 정의된 함수값으로 positional embedding을 주었다면, 
: : positional embedding도 random init을 통해 임베딩 벡터 자체도 학습에 의해 최적화된 값으로 도출해낸다.
: : [CLS] - Classification Embedding
: : Packed sentence embedding [SEP]
: : Segment Embedding - BERT를 학습할 때, 단어별 MASK 단어를 예측하는 task와, 주어진 두 문장으로 이루어진 한 sequence가 실제 인접 문장인지 아닌지 예측하는 문장 레벨에서의 task가 있었다.
: :  두 개의 문장을 SEP 토큰으로 구분해서 넣는 경우, 단어별로 positional embedding이 더해지는데, SEP 토큰 이후에 나오는 첫 번째 단어는 '두 번째 문장의 첫 번째 단어이다' 라는 정보도 분리해서 넣어줘야 한다.
: : 그래서 포지션 임베딩은 그대로 순차적으로 부여하되, 여기에 또 다른 종류의 position embedding으로 segment embedding이라는 것을 넣게 된다. (첫 번째 문장에서는 모든 워드들에 대해 동일한 벡터, 두 번째 문장에서도 모든 워드들에 대해 동일한 벡터를 더해준다.)
<br><br>
- BERT와 GPT2의 차이점 더 자세히 보기<br><br>
: : GPT의 경우, 주어진 시퀀스를 인코딩 할 때, 바로 다음 단어를 예측하는 task를 수행해야 하기 때문에 특정한 timestep에서 다음에 나타나는 단어에 대한 접근을 허용하면 안된다.
: : 마치 transformer에서 decoder에서의 self-attention이 masked 형태의 self-attention으로 사용되었던 것과 같다.
: : 특정 timestep에서는 자기 자신을 포함하여 왼쪽에 있는 정보를 access 하는 패턴을 보인다.
: : 기본적으로 Seq 인코딩을 하기위해 쓰는 attention 블록은 masked self-attention을 사용한다.
: : 그러나 BERT의 경우, [MASK]단어를 포함하여 전체 주어진 모든 단어들에 대해 접근을 허용함으로써, attention 패턴은 모두가 모두를 볼 수 있도록 하는, transformer에서 인코더에서 사용하던 self-attention 모듈을 사용하게 된다.
<br><br>

- BERT: Fine-tuning Process<br><br>
: : Sentence pair에 대해 classification을 하는 task에서는, 두 개의 문장을 [SEP] 토큰으로 구분된 하나의 시퀀스로 인코딩을 한 후, 각각 단어에 대한 인코딩 벡터를 얻었다면, [CLS]토큰에 해당하는 벡터를 아웃풋 벡터에 줘서 다수 문장에 대한 예측 태스크를 수행할 수 있다.
: : 단일 문장에 대한 예측은 문장을 하나만 주고, 첫 번째 [CLS]토큰을 주고 해당 [CLS]토큰의 아웃풋을 최종 아웃풋 레이어에 넣어서 classification을 수행한다.
: : Qeustion Answering은 뒤에서 더 자세히.
: : Single Sentence Tagging Task(각 단어별로 태깅, ex. POS tagging)의 경우, CLS토큰을 포함해 각각의 단어에 대한 인코딩 벡터가 얻어지면, 해당 벡터를 공통의 아웃풋 벡터에 추가해서 태스크를 수행하게 된다.<br><br>

- BERT vs GPT-1<br><br>
: : 학습 데이터에 있어서는 GPT는 800M words(BookCorpus), BERT는 2,500M words(BookCorpus, Wikipedia)로 학습
: : BERT에서는 GPT-1과는 달리 SEP, CLS 토큰 및 segment embedding을 입력단에서 더해주어서 더 잘 구분할 수 있게 했다.
: : Hyperparmeter로 BERT와 GTP의 한 번에 학습하는 단어의 수인 Batch size는, BERT - 128,000 words > GPT - 32,000 words
: : 일반적으로 더 큰 사이즈의 배치를 사용하면 최종 모델 성능이 더 좋아지고, 학습도 더 안정화되는 현상을 보여준다.
: : 이는 gradient descent를 수행할 때, 일부의 데이터만으로 도출된 gradient로 파라미터를 업데이트 할때보다 다수의 데이터에서 나온 평균 gradient로 파라미터 업데이트 할때가 더 성능이 좋아지기 때문이다.
: : 그러나 batch size를 키우기 위해서는 더 많은 GPU 메모리를 필요로 한다.
: : GPT는 자연어처리의 여러 Down-stream 태스크에 대한 fine-tuning에서 동일하게 5e-5라는 learning rate를 썼지만, BERT는 task별로 learning rate도 각각 optimize를 수행하여 학습하였다.
<br><br>
: : BERT도 Transfer Learning 과정에서 GPT와 비슷하게, 최종적인 Transformer 모델에서 나오는 인코딩 아웃풋 벡터들을 얻고 난 후에,
: : 이후 각 MASK 토큰에 대한 prediction을 수행하는 아웃풋 레이어를 제거하고,
: : main task를 위한 아웃풋 레이어를 추가적으로 접붙이는 식으로 모델을 구성했다. (가장 윗 레이어는 random initialization을 한 후 down-stream task를 위한 데이터를 통해 학습하고, 기 학습된 트랜스포머 인코더의 파라미터들은 상대적으로 작은 learning rate를 사용해 조금만 변화하도록 하여, 일반화 가능한 지식이 최대한 많이 유지될 수 있도록 학습하였다.)<br><br>

- Machine Reading Comprehension (MRC), Question Answering<br><br>
: : 기계 독해에 기반한 질의응답 태스크
: : 이를 훈련하기 위한 대표적인 데이터셋 중 SQuAD 데이터가 있다. (크라우드 소싱을 통해 많은 사람들에게 task를 수행하도록 하여 수집된 데이터)
: : The Stanford Question Answering Dataset
: : <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD 공식 웹사이트</a>
: : ALBERT, ROBERTA 등 BERT를 조금씩 개선한 다양한 모델들이 태스크에서의 순위권에 올라있다.
<br><br>
: <br><b>BERT: SQuAD 1.1</b><br><br>
: : 질문과 답을 SEP 토큰을 통해 concat 해서 하나의 sequence로 만들어 인코딩을 진행한다.
: : 지문에서 답에 해당하는 문구가 시작하는 위치를 예측하기 위해, 각 워드별로 스칼라 값을 얻어낸다.
: : 여러 워드 중, 답에 해당하는 단어가 어느 단어에서 시작하는지를 먼저 예측한다.
: : softmax loss를 통해 모델을 학습한다.
: : 답이 끝나는 시점도 예측해야 하는데, 또 다른 fully connected layer를 만들어 이를 통과하여 각 워드 인코딩 벡터가 스칼라값이 나오게 하여 이를 softmax에 통과시킨 후, 두 번째 버젼의 fully connected layer를 통해 ending word 포지션을 예측하도록 한다.
<br><br>
: <br><b>BERT: SQuAD 2.0</b><br><br>
: : 질문에 대한 답을 찾을 수 없는 데이터셋까지도 포함되어있는 데이터셋.
: : 예측해야 하는 task가 먼저 일단 답이 있는지 없는지를 판단하고,
: : 만약 답이 있다면 이전 방식대로 답에 해당하는 문구를 예측하게 된다.
: : 이 경우 질문과 지문을 concat해 BERT를 통해 인코딩하고, 이렇게 나온 CLS 토큰을 binary classification을 수행하게 한 후 cross-entropy loss를 통해 학습시킨다.
: : 최종적으로 예측에 사용할때는, CLS 토큰을 통해 답이 있는지 없는지를 먼저 예측하고 이후에 앞에서 말한 starting/ending 포지션을 예측하는 output layer를 구동하여 답에 해당하는 문구를 예상할 수 있게 된다.
<br><br>

- BERT on SWAG<br><br>
: : 다음에 올 적절한 문장 고르기 task
: : 가능한 선택지를 차례대로 [CLS] 토큰을 통해 질문과 concat하여 fully connected layer를 통해 스칼라 값들로 예측하게 하고,
: : 이를 통해 선택지의 개수만큼 스칼라 값이 나오게 되면, 이를 softmax에 입력으로 주고, 정답인 ground truth의 확률이 100%가 나오게 하는 softmax loss를 통해 전체 모델을 학습하게 된다.
<br><br>

- BERT: Ablation Study<br><br>
: : 모델 사이즈를 점점 키울수록(파라미터 수) 여러 다운스트림 태스크에 대한 능력이 점점 좋아진다
: : 가능하다면 모델사이즈를 더 키우면 pre-training을 통한 down-stream task에 적용했을 때의 성능이 점점 더 오를 것이라고 전망하고 있다.

<h2>(10강) Advanced Self-supervised Pre-training Models</h2>
- 강의 키워드<br>
: - Self-supervised pre-training
: - 경량화
: - 지식 그래프 integration

- 소개<br><br>
: : GPT-2, GPT-3, ALBERTA, ELECTRA, Light-weighted Models, Fusing Knowledge Graph into Language Model

- GPT-2<br><br>
: : Just a really big transformer LM (transformer layer 수 늘림)
: : 학습은 여전히 language modeling 으로 진행
: : Trained on 40GB of text
: : LM은 zero-shot setting에서 down-stream task들을 할 수 있다는 잠재력도 보여줌.
<br><br>
: : Language Models are Unsupervised Multi-task Learners
: : Motivation : decaNLP
: : The Natural Langauge Decathlon: Multitask Learning as Question Answering
: : 모든 NLP의 task들이 질문과 답변 형태로 수행될 수 있다는 것.
<br><br>
: : Reddit에서 3개 이상의 up-vote를 받은 외부 링크를 데이터셋으로 사용
: : Byte pair encoding (BPE)
: <br><b>Model</b><br><br>
: : Layer normalization 위치 변화,
: : random initializing 할때, layer의 index에 비례 혹은 반비례해서 더 작은 값으로 만들었다. Layer가 위로 갈수록 해당 레이어에서 선형변환에 쓰이는 값이 0에 가까워지도록, 즉 뒷단의 레이어가 하는 일이 점점 줄어들도록.
: <br><b>Down-stream tasks (without fine-tuning)</b><br><br>
: : Question Answering with CoQA dataset
: : Summarization
: : Translation
<br><br>

- GPT-3<br><br>
: <br><b>Language Modles are Few-shot Learners</b><br><br>
: : 구조의 변화보다는, 모델 사이즈를 비약적으로 크게 늘림 (Transformer의 self-attention block을 많이 쌓은 것)
<br><br>
: : <u>Zero-shot</u> 
: : (Task description : "Translate English to 
French")
: : (prompt: "cheese ==> ")
<br><br>
: : <u>One-shot</u> 
: : (Task description : "Translate English to 
French")
: : (example: "sea otter ==> loutre de mer")
: : (prompt: "cheese ==> ")
<br><br>
: : <u>Few-shot</u>
: : (Task description : "Translate English to 
French")
: : (example: "sea otter ==> loutre de mer" / peppermint ==> menthe poivree / plush girafe => girafe peluche)
: : (prompt: "cheese")
<br><br>
: : 큰 모델을 사용할수록, 동적인 적응 능력이 뛰어나게 된다는 사실을 보여줌<br><br>

- ALBERT<br><br>
: <br><b>A Lite BERT for Self-supervised Learning of Langauge Representations</b><br><br>
: : 기존 BERT는 GPU 메모리의 한계, 학습 속도의 제한이 있음
: : 성능의 큰 하락 없이 모델 사이즈와 학습 시간을 줄이는 경량화 모델<br><br>
: <br><b>Factorized Embedding Parameterization</b><br><br>
: : self-attention block을 쌓아나가면서, 점점 더 의미론적으로 유의미한 정보들을 계속 추출해나가는 과정이 바로 이 레이어를 쌓는 과정인데,
: : 임베딩 레이어에 디멘션을 줄이는 추가적인 기법을 제시하였다. (첫 레이어는 많은 정보가 담길 필요 없다고 생각)
: : Low-rank Matrix Factorization
: : 전체적으로 parameter 수를 줄여줄 수 있다.
<br><br>
: <br><b>Cross-layer Parameter Sharing</b><br><br>
: : Shared-FFN: Only sharing feed-forward network parameters across layers
: : Shared-attention: Only sharing attention parameters across layers
: : All-shared: Both of them
<br><br>
: <br><b>Sentence Order Prediction</b><br><br>
: : Negative samples the two consecutive segments, but with their order swapped
: : 같은 글 내에서 연속적으로 등장하는 문장을 가져와서, 원래 있던 순서대로 concat을 해서 주었을 때는 맞는 sentence order라고 분류하고,
: : 순서를 바꿔서 concat해서 주면 뒤집힌 order라고 분류하는 task를 학습시킴.
: : 기존에는 두 문장이 같은 글에 있는 것인지, 아니면 그냥 랜덤한 글에서 가져온 다른 두 문장인지만 분류하는 것이었음.
: : 이것과의 차이점의 핵심은 Negative samples 인데,
: : 아예 상관없는 글의 두 문장은 겹치는 단어가 거의 없으므로 너무 쉬운 태스크인데,
: : 연속한 두 문장을 같은 글에서 가져오면 겹치는 단어의 수가 순서를 바꿔도 똑같기 때문에, overlapping 단어 수로 파악할 수 없고 논리적인 흐름에 집중하고 파악해야 좋은 성능을 낼 수 있으므로 더 유의미하고 어려운 task를 학습하는 것이 됨.
<br><br>

- ELECTRA<br><br>
: : Efficiently Learning an Encoder that Classifies Token Replacements Accurately
<br><br>
: : 기존의 BERT나 GPT-2 등의 모델과는 다른 형태로 pre-training 한 모델
: : 실제로 BERT에서 사용한 Masked Language Modeling이나 GPT의 Standard Language Modeling에서 한 발 더 나아가,
: : LM을 통해 단어를 복원해주는 모델을 하나 더 두고(Generator - typically a small MLM),
해당 모델이 Masked LM으로 MASKED 단어 복원을 수행하는 경우, 
: : 각각의 단어가 replace된 단어인지, 아니면 원래 단어인지를 구분하는 구분자(Discriminator)가 ELECTRA의 핵심 부분이 된다.
<br><br>
: : Adversarial Learning(적대적 관계로 학습)이 이루어진다.
: : Generative Adversarial Network(GAN) 모델에서 사용된 아이디어에 착안해서, 자연어 처리에서의 Pre-training 모델을 제안한 것이다.
: : Discriminator를 pre-trained 모델로 다른 down-stream task들에 대해 fine-tuning 할 때의 pre-trained 모델로 사용하게 된다.

- Light-weight Models<br><br>
: : 휴대폰 등의 기기에서도 이런 경량화 모델을 사용할 수 있다.
: <br><b>DistillBERT</b><br><br>
: : HuggingFace에서 2019년 NeurIPS에서 발표한 논문
: : Teacher model과 student model이 있음.
: : Student model은 Teacher 에 비해 더 경량화된 모델
: : Teacher model이 내는 결과나 패턴을 잘 모사할 수 있도록 학습을 진행
: : Teacher model의 Softmax를 통한 확률 벡터를 student model이 학습하는 ground truth로 사용한다.
<br><br>
: <br><b>TinyBERT</b><br><br>
: : Knowledge distillation을 사용한다.
: : 마찬가지로 teacher, student 모델이 있다.
: : Target distribution을 모사해, 이를 ground truth로 softmax 적용해서 teacher 모델을 닮도록 학습하는 방식 뿐만 아니라,
: : Embedding Layer, 그리고 self-attention block 이 가지는 $W_Q, W_K, W_V$등의 attention matrix, 그리고 결과로 나오는 hidden state vector들까지도 유사해지도록, 
: : 가장 최종 output으로서의 예측값 뿐만 아니라,
: : 파라미터와 중간 결과물까지도 모두 student network이 닮도록 학습을 진행한다.

- Fusing Knowledge Graph into Language Modle<br><br>
: : 기존의 pre-training 모델과, 지식 그래프(Knowledge graph)라고 불리우는 외부적인 정보를 잘 결합하는 연구 방향
: : 다양한 개념, 개체 등의 관계를 잘 정형화해서 만들어 놓은 것이 바로 knowledge graph
: : BERT가 취약한 부분이었떤, 상식적인 외부 지식을 기존의 BERT 등의 LM에 결합해 활용하는 방법


<h2>Peer Session</h2>
- 1) 주제 1<br><br>
: : 
: : 
- 2) 주제 2<br><br>
: : 
: : 