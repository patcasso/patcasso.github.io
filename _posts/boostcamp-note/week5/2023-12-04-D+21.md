---
layout: single
title:  "Day 21 학습정리"
categories: boostcamp-note-week5
sidebar:
  nav: "docs"
---

23/12/03 (월) 학습 내용

<h1>NLP 이론</h1>

<h2>(7강) Transformer 1</h2>
- 강의 키워드<br><br>
: - Self-Attention
: - Transformer

- Transformer: High-level view<br><br>
: : 기존의 부분 모듈이던 Attention을 통해 RNN을 통째로 걷어내고 대체한다. (Attention is all you need)
: : RNN 리뷰
<br><br>
: <br><b>Bi-Directional RNNs</b><br><br>
: : Forward RNN - 정방향의 RNN
: : Backward RNN - 정보를 오른쪽(나중 timestemp)에서 왼쪽으로 전달하는 방법
: : Forward, Backward 에서 해당 타임스텝에 대해 나온 각각의 Hidden state vector를 concat한 것을 해당 타임스텝의 인코딩 벡터로 사용하는 방식<br><br>

- Transformer: Long-Term Dependency<br><br>
: : Self-attention Module (자기 자신과 내적이 된다)
: : 자기 자신과 내적을 하게 되면, 자기 자신에게 큰 가중치가 걸리는 형태로, attention 벡터의 양상이 나타난다.
: : 즉, 자기 자신의 원래 정보만을 주로 포함하는 벡터들로만 나오게 되는 것이다.
: : 따라서, 이러한 가장 기본적인 어텐션의 작동 과정을 조금 더 유연하게 확장하여 해당 문제를 개선한다.
<br><br>
: : <u>Query 벡터</u>
: : 주어진 벡터들 중, 어느 벡터를 선별적으로 가져올지에 대한 기준이 되는 벡터로서의 역할을 함
<br><br>
: : <u>Key 벡터</u>
: : Query벡터가 주어진 인코더 히든스테이트 벡터의 각 벡터와 내적을 통해 유사도가 구해지는데, 
: : 유사도가 구해지는, 즉, Query 벡터와 내적이 되는 각각의 재료 벡터들을 Key 벡터라고 한다.
<br><br>
: : <u>Value 벡터</u>
: : 각각 key와의 유사도를 구하고 softmax를 취한 후, 
: : 나온 가중치를 실제로 적용하여 가중 평균, 혹은 선형 결합을 했을 때 사용하는 재료 벡터들은,
: : 원래는 key벡터와 동일한 벡터로 취급하여 사용했지만, 한 번은 유사도를 구하는 key벡터로 사용됐고, 그 이후에는 가중치가 실제로 적용되어 가중평균이 구해지는 재료벡터로도 사용되는데, 이런 세 번째 역할을 하는 것을 Value 벡터라고 부른다
<br><br>
: : 따라서, 한 Sequence를 Attention을 통해 인코딩하는 과정에서, 각 벡터들이 Query, Key, Value로 세 가지 역할을 모두 다 하고 있는 것을 볼 수 있다.
: : 동일한 세트의 벡터에서 출발하였더라도, 역할에 따라 벡터가 서로 다른 형태로 변환할 수 있도록 하는 별도의 Linear Transformation Matrix (Q, K, V 각자에 대해 따로 정의된다)가 정의된다.
: : 주어진 같은 벡터에 대해서라도, Q, K, V 각각의 용도에 따라 다른 벡터로 변환될 수 있는 확장된 형태가 만들어지게 된다.
<br><br>
: : Q와 K 벡터간의 내적 값을 구하고, Softmax를 통과해 합이 1인 확률벡터를 구하고, 이것이 상대적인 가중치가 된다.
: : <span style="color:red">여기서는 비록 동일한 첫 번째 단어로부터 Query와 Key 벡터를 만들었다고 하더라도, 해당하는 내적 값이 다른 Key 와의 내적값보다 더 작을 수도 있게 되는 것이다. (Q. How? Query와 Key가 각각 다른 가중치 행렬을 통해 각각 다른 선형변환을 거치면 자동으로 이렇게 되는 것인가?)</span>
: : Q, K로의 서로 다른 변환이 존재하고, 그러므로 같은 벡터로 Q, K를 만들었을 때에도 조금 더 유연한 유사도를 얻어낼 수 있게 된다.
<br><br>
: : 이 과정을 통해 얻어낸 ex. 0.2 / 0.1 / 0.7의 가중치 벡터(이게 attention인듯)는 Value 벡터에 부여되는 가중치가 되어서, Value 벡터의 가중 평균 결과로 최종적인 벡터가 나오게 된다.
: : 해당 벡터(h1 벡터를 말하는 듯. 이게 인코딩 벡터?)는 결국 첫 번째 단어인 I에 해당하는 벡터를 Query 벡터로 사용해서, 주어진 시퀀스 내에서 필요로 하는 가중치를 시퀀스 내의 다른 단어에 적절히 분배함으로써, 각 단어들이 가지는 Value 벡터를 가중 평균해서 얻어낸 벡터라고 할 수 있다.
<br><br>
: : 세 개의 단어로 이루어진 Sequence라 할 때, 첫 번째 단어에 대한 인코딩 벡터를 만드는 과정에서 세 개의 단어가 모두 다 참여하는 것을 볼 수 있다.
: : 마찬가지로 두 번째, 세 번째 단어의 인코딩 벡터 역시 모든 단어에 대한 정보를 고려하여 결합한 결과 벡터를 만들어낸다.
: : sequence 길이가 굉장히 길다고 하더라도, self-attention 모듈을 적용해 시퀀스 내 모든 워드들을 인코딩 벡터로 만들게 되면, 타임스텝의 gap이나 차이가 멀다고 해도, 동일한 key와 value벡터들로 변환이 되고, 쿼리 벡터에 의한 내적의 유사도만 높다면, 멀리 있던 정보도 손쉽게 가져올 수 있다.
: : 이것이 기존 RNN이 가지던, 멀리 있는 timestep의 정보를 손실/변질 없이 가져오기 힘들었던 문제를 근본적으로 개선한 형태의 모델로 볼 수 있는 것이다.
: : Long-Term Dependency의 문제를 근본적으로 해결한 Sequence 인코딩 기법으로 볼 수 있다.

- Transformer: Scaled Dot-Product Attention<br><br>
: : Attention 모델의 입력은 Q와 K,V 페어들이 존재한다.
: : Output is weighted sum of values (가중치)
: : 가중치는 query 벡터와, value 벡터에 대응하는 key 벡터의 내적값을 통해 구해진다.
: : Query 벡터와 Key 벡터는 내적 연산이 가능해야 하므로, 같은 차원의 벡터여야 한다.
: : Value 벡터의 경우에는, query나 key 벡터들의 차원과는 꼭 같지는 않아도 된다.
<br><br>
: : $A(Q,K,V) = \text{softmax}(QK^T)V$
: : 위의 수식을 PDF 및 내가 예전에 그렸던 Transformer Google Slides를 통해 이해하자.
: : Softmax는 기본적으로 행렬이 주어지면 Row-wise softmax를 수행하게 된다 (각 줄에 대해 softmax 적용)
: : 결과 Matrix의 첫 번째 Row는, 결국 맨 처음 주어졌던 Query 행렬에서의 첫 번째 Row 벡터(즉, 첫 번째 query 벡터)에 대한, Attention module의 output 벡터를 의미하게 된다.
: : 이러한 행렬 곱을 통해, Query벡터도 여러개가 주어져서, 각각의 query에 대한 모듈의 아웃풋을 계산해야 하는 상황을 쿼리를 한데 모아서 행렬을 구성한 후, 해당 행렬연산을 수행하면 모든 query벡터에 대한 attention모듈의 아웃풋 벡터를 한번에 계산할 수 있는 것이다.
<br><br>
: : 여러 쿼리벡터에 대한 계산을 이처럼 행렬연산으로 바꾸어두면, 행렬연산을 굉장히 빠르게 병렬화해서 계산할 수 있는 GPU를 활용하여 효율적으로 계산하게 될 수 있는 것이다.
: : 이러한 병렬연산이 가능하다는 특성을 통해, RNN 등에 비해 상대적으로 학습이 빠른 특성을 가지게 된다.
<br><br>
: : 계산 과정에서 $\sqrt{d_k}$로 나눠주는 이유 (Scaling과정. 그래서 'Scaled' Dot-Product Attention인 것)
: : Q와 K 벡터가 가지는 공통된 dimension 수가 늘어나면, 
: : 분산이 차원의 개수($d_k$)만큼의 확률변수의 분산들의 합만큼으로 계속 늘어나게 된다.
: : 분산 or 표준편차가 더 클수록, softmax의 확률분포가 큰 값에 굉장히 몰리는 형태가 나타난다는 것을 알 수 있다.
: : 분산이 작은 경우에는, softmax의 확률분포가 조금 더 고르게 나타남을 알 수 있다. (균등분포에 가까운 형태로)
: : 내적에 참여하는 Q, K 벡터의 차원이 몇이냐에 따라, 내적값의 분산이 크게 좌우될 수 있고,
: : 이에 따라 softmax에서 나오는 확률분포가 한 key에만 100%에 가까운 확률이 몰리는 극단적인 형태의 확률이 나올지, 아니면 모든 key가 전반적으로 고른 확률값을 부여받은 형태로 나올지의 패턴이 의도치않게 Q, K의 dim 때문에 영향을 받게 된다는 것.
: : 내적값의 분산을 일정하게 유지시켜 학습을 안정화시키기 위해, dimension의 root값으로 나눠주는 연산을 추가적으로 넣게 되는 것이다.
: : 따라서, Q, K의 차원에 관계없이 분산이 일정하게 유지되는 것이다.
<br><br>
: : 결과적으로, Softmax의 값이 한쪽으로 몰리게 되는 경우는 Backpropagation 과정에서 gradient vanishing이 많이 일어날 수 있는 가능성이 있으므로,
: : 위와 같은 경우를 방지하기 위해 Scaling을 해주는 것이다.


<h2>(8강) Transformer 2</h2>
- 강의 키워드<br><br>
: - Multi-head Attention
: - Masked Self-Attention
: - Transformer

- Multi-head Attention<br><br>
: : 동일한 Q, K, V 에 대해 여러 버젼의 어텐션을 수행하는 것
: : 서로 다른 버젼의 attention 개수만큼, 동일한 쿼리 벡터에 대한 서로 다른 버젼의 인코딩 벡터가 나오고, 해당 인코딩 벡터를 모두 Concat 함으로써 해당 쿼리벡터에 대한 인코딩 벡터를 최종적으로 얻게 된다.
: : 여러 버젼의 attention 을 수행하기 위한 선형변환 행렬들을 서로 다른 "head"라고 부른다.
: : 다수의 버젼의 attention을 수행한다는 뜻에서 Multi-head attention이라 부른다.
<br><br>
: : 동일한 sequence에 대해서도 서로 다른 여러 기준으로 쿼리 벡터를 뽑아와야 할 수도 있다.
: : ex. I went to the school, I studied hard, I came back home and I took a rest.
: : 위의 네 가지 문장에서, I 라는 행동의 주체가 한 행동에 대한 정보를 추출해야 할 수도 있고, I 가 있던 장소에 대한 정보를 추출해야 할 수도 있다.
: : 이처럼, 서로 다른 정보들을 병렬적으로 뽑은 후 다 합치는 형태로 attention 모듈을 구성할 수 있다.
: : 각각 head가 이런 서로 다른 정보들을 상호보완적으로 뽑는 역할을 하게 된다.
<br><br>
: : Attention 모델의 계산량, 메모리 요구량 기존 모델들과 비교
: : $\text{softmax}( \dfrac{QK^T}{\sqrt{d_k}})V$ 에서 $QK^T$ 부분이 주된 연산량을 차지하게 된다.
: : 해당 경우 sequence 길이를 n, 차원이 d라고 하면, n개의 query와 n개의 key에 대한 d 차원의 모든 가능한 pair에 대한 내적값을 계산해야 한다.
: : 따라서 Layer당 계산복잡도가 $O(n^2 \cdot d)$가 된다.
: : 그러나 이러한 행렬 연산은 GPU코어 수가 충분히 많다면, 행렬 연산의 병렬화를 통해 $O(1)$로 처리할 수 있다.
<br><br>
: : 반면에 RNN은 레이어당 복잡도가 $O(n,d^2)$ 이고, 순서대로 연산해야 하므로 $O(n)$을 또 곱해(?)주어야 한다.
<br><br>
: : Forward propagation에서 해당 값들을 모두 메모리에 저장해놓아야 Backprop 과정에서 사용할 수 있게 된다.
: : d는 hidden state vector의 차원 수로, hyperparameter로 정할 수 있다.
: : 하지만 n은 임의로 길이를 조절할 수 없는 가변적인 값이다. 
: : 조금 더 긴 sequence를 처리하게 되면, sequence 길이의 제곱에 비례하는 계산량과 메모리 사이즈가 필요하게 된다.
: : 따라서 self-attention에서는 rnn보다 일반적으로 훨씬 많은 메모리 저장량을 필요로 하게 된다. (모든 Query 벡터와 Key 벡터간의 내적 값을 저장하고 있어야 한다.)
<br><br>
: : self-attention은 GPU 코어만 많으면 모든 연산을 병렬적으로 처리할 수 있다.
: : 그러나 RNN은 $h_{t-1}$를 계산해야 $h_t$를 계산할 수 있다. 즉, 병렬화가 불가능하다.
: : 따라서 Self-attention은 RNN 보다 훨씬 빨리 학습할 수 있지만, 메모리는 더 많은 양을 사용하게 된다.
<br><br>
: : 또, RNN은 가장 끝에 있는 단어가 가장 앞에 있는 단어를 참조하기 위해서는 n번의 timestemp을 거쳐야 한다.
: : 그러나 self-attention에서는 가장 끝에 있는 타임스텝끼리도 바로 옆의 타임스텝에서 정보를 얻어오는 것과 차이가 없다.
<br><br>

- Transformer: Block-Based Model<br><br>
: : Residual Connection으로 Add 라는 operation을 수행한 후, Layer Normalization을 추가하게 된다.
: : 이후 Feed Forward를 통과하고, 또 다시 Residual Connection 및 Layer Norm을 수행하게 된다.
: : Residual Connection은 CV쪽에서 깊은 레이어의 NN을 만들 때, Gradient vanishing을 해결하여 학습을 안정화시키고 레이어를 계속 쌓아감에 따라 더 높은 성능을 보일 수 있도록 하는 모델이다.
: : Multi-Head Attention 모듈을 거쳐 나온 아웃풋 벡터와 자기 자신을 한 번 더 더해준 후, Normalize함을 통해 Gradient vanishing도 해결하고, 학습도 더 안정화 시킬 수 있다.
: : Residual connection을 적용하기 위해서는, 입력 벡터와 출력 벡터간의 디멘션이 동일하도록 유지되어야 한다. (그래야 성분끼리 합이 가능)<br><br>

- Transformer: Layer Normalization<br><br>
: : Batch Norm, Layer Norm, Instance Norm, Group Norm 등의 방법이 존재한다.
: : 주어진 다수의 샘플들에 대해 평균을 0, 분산을 1로 만들어준 후, 원하는 평균과 분산을 주입할 수 있도록 하는 선형변환으로 이루어진다.
<br><br>
: : 먼저 각각 같은 상수를 더해 평균을 0으로 만들어주고, 각 값을 분산으로 나누어주어 분산도 1로 만들 수 있다.
: : 이후 원하는 평균, 분산을 만들어주기 위한 affine transformation, 즉, $y=ax+b$라는 연산을 수행할 때에는 각각의 $a, b$ 한 세트를 각 노드별로 여러 단어에 걸쳐 공통적인 변화를 적용해주게 된다. (Layer Norm의 경우만인가?)
<br><br>
: : Transformer에서의 한 블록을 구성하는 Multi-Head Attention, Residual Connection, Layer Normalization을 살펴보았고,
: : 해당 변환을 완료한 이후에는 word별로 가지는 인코딩 벡터를 추가적인 Fully connected layer를 하나 통과시켜서, 각 워드의 인코딩 벡터를 변환하고,
: : 이후에 또 동일하게 Residual Connection을 통해 아웃풋과 벡터를 더해주고, 마지막으로 Layer Norm을 통해 원하는 평균과 분산을 주입하는 방식으로, 하나의 블록이 정의가 된다.
: : 최종적으로 나오는 것은 동일한 차원을 가지는, 각각의 단어에 대응하는 인코딩 벡터가 되는 것이다.
<br><br>

- Transformer: Positional Encoding<br><br>
: : RNN과는 달리, self-attention 기반으로 시퀀스를 인코딩 하는 경우, 각 워드별로 출력된 인코딩 벡터에 대해 순서가 달라져도 결과에 차이가 없다.
: : 순서를 무시한다는 이러한 특성은, 입력 문장을 시퀀스 정보를 인코딩하지 못하고, 그냥 순서를 고려하지 않는 집합으로 보고 인코딩을 얻는 것으로 생각할 수 있다.
: : 따라서 positional encoding을 사용하게 된다.
: : 각 순서를 특정할 수 있는, 마치 지문과도 같은 unique한 상수 벡터를 각 순서에 등장하는 워드 입력 벡터에 더해주는 것이고, 이를 positional encoding이라 한다.
: : 그럼 더해주는 벡터는 어떻게 결정할까?
<br><br>
: : 위치에 따라 구별할 수 있는 벡터를 sin, cos 등으로 이루어진 주기 함수를 사용하고, 주기를 서로 다른 주기를 써서 여러 sin 함수를 만든 후, 특정 x값에서의 함수값을 모아서 위치를 나타내는 벡터로 사용하게 된다.
: : 차원 개수만큼의 서로 다른 주기, 혹은 sin/cos이 번갈아가면서 그래프가 생성되는 패턴을 만들어 놓은 후, 첫 번째 위치(e.g., 0번 인덱스)에서의 포지션 인코딩 벡터를 각각 디멘션에서 다른 함수에서 값을 얻어 더해주게 된다.
<br><br>
: : 이러한 방식에 따라 위치별로 서로 다른 벡터를 더해주게 함으로써, 위치가 달라지면 출력 인코딩 벡터도 달라지게 하는 방식으로 Trnasformer 모델이 순서라는 정보를 다룰 수 있도록 했다.
<br><br>

- Transformer: Warm-up Learning Rate Scheduler<br><br>
: : ADAM 등의 알고리즘을 통해 최적화를 수행하는 과정에서 learning rate를 보통 하나의 값으로 사용하게 되는데,
: : learning rate 값을 학습 중에 적절하게 변경해 사용하게 하는 방식이 learning rate scheduler이다.
<br><br>
: : 목적식을 최적화하려는 parameter값을 랜덤하게 초기화를 하게 되는데, 
: : 도달하고자 하는 점과는 상대적으로 거리가 먼 임의의 위치에서 시작할 확률이 높다.
: : 초반에는 gradient가 기울기 큰 지점일 가능성이 높으므로, learning rate를 작은 값을 설정하여 너무 많이 보폭이 발생하는 것을 방지한다.
: : 어느정도 iteration이 진행되면, 완만한 구간에 도달할 수 있는데, 도달해야 하는 optimal한 포인트가 멀리 있을 수 있기 때문에, 동력을 주는 차원에서 learning rate를 iter 수에 비례해서 올려주게 된다.
: : 최저점 근처까지 다 온 상황에서는 다시 잘 수렴할 수 있도록 learning rate를 다시 낮춰준다.
<br><br>
: : 이로서 Transformer에서 인코더 부분을 모두 커버했다.
: : 해당 인코딩 블럭을 N번, 주로 6, 12, 24번 등 사용하여 (각각 독립적인 파라미터를 가지는 동일 블록을 N번을 쌓아서), 인코딩 벡터가 점차 high-level의 벡터로 인코딩이 진행된다.
: : 이렇게 다 쌓고 나면, 최종 마지막 레이어의 마지막 아웃풋으로 나오는 각 word별 인코딩 벡터를 얻게 되고, 이게 인코딩 과정에 전부 관여된다.
<br><br>
: : (p.24) 각 헤드별로 어떤 정보에 주목하는지를 살펴볼 수 있다.

- Transformer: Decoder<br><br>
: : Multi-Head Attention에 Masked라는 추가적인 단어가 들어가있는 것을 알 수 있다. (추후 다시 설명)
: : 인코딩 벡터가 디코더의 hidden state vector로 얻어진 후에는, 다시 Multi-head Attention에 해당 단어를 입력으로 준다. 
: : 그러나 이때에는 Q, K, V 각각의 입력에 대해,
: : 디코더에서 만들어진 Hidden state vector가 쿼리로 사용되고, 인코딩 단의 최종 출력 벡터가 Key와 Value로 들어가서 Attention이 수행됨을 알 수 있다.
: : 인코더/디코더 간의 어텐션 모듈이 바로 디코더의 두 번째 Multi-Head Attention 부분에 해당하는 것이다.
: : 그 이후에는 다시 Residual Connection과 Layer Norm, Feed Forward, Residual Connection, Layer Norm을 거쳐 최종적으로 나온 결과에, 타겟 언어의 vocabulary size만큼의 크기로 linear transformation을 거친 후, 거기에 Softmax를 취하여 확률값을 뽑아서, 다음 워드에 대한 예측을 수행하게 된다.
: : 그렇게 나온 단어에 대한 확률 분포는 ground truth 단어와의 softmax loss를 통해 Backpropagation에 의해 전체 네트워크의 학습이 진행되게 된다.
<br><br>

- Masked Multi-Head Attention<br><br>
: : Masked Self-Attention의 기본 아이디어는, 디코딩 과정에서 입력으로 단어들을 주고 다음 단어들을 예측하는 과정에서, 주어진 시퀀스에서 정보의 접근의 가능 여부와 관련이 있다.
: : <span style="color:gray">이후 타임스텝의 단어들의 정보들에 대해 학습 과정에서 access 할 수 없도록 만드는 과정인 것 같다.</span>
: : Masked의 Self-attention의 Masking이라는 의미는, attention을 모두가 모두를 볼 수 있도록 허용한 후, 후처리적으로 보지 말아야 하는 단어의 어텐션 가중치를 0으로 만들어 주고, 그 이후에 Value 벡터와 가중 평균을 내는 방식으로 어텐션을 변형한 방식이 바로 디코더에서 사용되는 Masked Self-Attention이 된다.
<br><br>
: : 자연어처리에서 가장 활발하게 연구되고 있는 기계번역 분야에서 Transformer가 처음으로 제안이 되었고,
: : Transforemr가 나오면서 기존에 가장 좋은 성능을 내던 Seq2Seq w/ attention보다 더 좋은 성능을 내게 된다. (BLUE Score, Training Cost in FLOPs)

- 그 외 생각<br><br>
: : 트랜스포머에서 인코더의 읽기, 디코더의 역할은 쓰기와 같다고 생각된다.


<h2>Peer Session</h2>
- 1) 주제 1<br><br>
: : 
: : 
- 2) 주제 2<br><br>
: : 
: : 