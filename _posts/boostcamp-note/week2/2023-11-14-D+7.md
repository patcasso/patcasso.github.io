---
layout: single
title:  "Day 7 학습정리"
categories: boostcamp-note-week2
sidebar:
  nav: "docs"
---

# 23/11/14 (화) 학습 내용

<h2>PyTorch</h2>

- <b>(4강) AutoGrad & Optimizer</b><br><br>
: <br><b>강의 키워드</b><br><br>
: - AutoGrad
: - Optimizer
: - torch.nn.Module
: - nn.Parameter
: <br><b>소개</b><br><br>
: : 프로젝트 구조 안에 들어가는 코드 하나하나에 대한 자세한 설명
: : Optimizer의 구성, 프로세스의 흐름 설명
: <br><b>(상황 가정)논문을 구현해 보자!</b><br><br>
: : ex) ResNet의 복잡한 도식 - 수많은 반복의 연속
: : Layer = Block
: : ex) Transformer Layer에는 Softmax, Linear, Normalization, Multi-Head Attention 등 여러 Layer들이 있다.
: : Encoder, Decoder Layer 처럼 큰 단위의 레이어들도 있다.
: : 딥러닝 아키텍쳐 - 블록 반복의 연속
: <br><b>torch.nn.Module</b><br><br>
: : 딥러닝을 구성하는 Layer의 base class
: : Input, Output, Forward, Backward(AutoGrad) 정의
: : 학습의 대상이 되는 parameter(tensor) 정의
: : Forwardpass - x, y -> f(x,y) -> z
: : Backwardpass - 미분값 (dL / dw, dL / db)
: <br><b>nn.Parameter</b><br><br>
: : Weight값을 보통 파라미터 클래스 안에 정의함
: : nn.Module 내에 attribute가 될 때는 requires_grad=True로 지정되어 학습 대상이 되는 Tensor
: : 우리가 직접 지정할 일은 잘 없음
: : 대부분의 layer에는 weights 값들이 지정되어 있음.
: : forward는 _y를 구하는 것이라 생각하면 편하다.
: : Parameter를 직접 선언할 일은 없을 것이다.
: <br><b>Backward</b><br><br>
: : Layer에 있는 Parameter들의 미분을 수행
: : Forward 결과값 (model의 output = 예측치)과 실제값간의 차이(loss)에 대해 미분을 수행 후 해당 값으로 Parameter 업데이트
: : optimizer.zero_grad()하는 이유 - 학습할 때 gradient 값이 계속 업데이트 되는데, 이전의 grad값이 지금의 학습에서 영향을 받지 않도록 epoch 마다 초기화 하는 것
: : 한 epoch을 구성하는 단계 (필수 4단계)
: : 1) 초기화 (optimizer.zero_grad())
: : 2) 예측치 계산 (outputs = model(inputs))
: : 3) 손실 함수 계산 (loss = criterion(outputs, labels), loss.backward())
: : 4) Parameter 업데이트 (optimizer.step())
: <br><b>Backward from the scratch ? </b><br><br>
: : 실제 backward는 Module 단계에서 직접 지정가능
: : Module에서 backward와 optimizer 오버라이딩
: : 사용자가 직접 미분 수식을 써야 하는 부담 -> 쓸 일은 없으나 이해는 할 필요
: : backward()에서는 미분값을 적어주면 됨
: : optimize()는 미분값만큼 업데이트 해주는 함수 (ex. self.w -= self.lr * self.grads["dw"])
: : 직접 Linear regression 구현 (실습 코드 참고)
: <br><b>정리</b><br><br>
: : 중요한 것은, PyTorch라는 도구는 여러 복잡한 수식을 코드로 바꿔주는 굉장히 편리한 도구이다! 라고 이해하면 된다.
: : 지금 시점에서는 계속 좋은 도구들이 많이 개발되다 보니, NLP의 경우 Hugging Face Repository를 많이 쓴다.
: : 그래서 밑단까지 optimizer 만질 일이 적은 것 같다. (지난 1년 간 거의 없을 정도)
: : 지금 정도의 비교적 low-level의 코드 만질 일은 점점 없을 것이나, 어떻게 작동되는지 알아야 custom optimizer를 만지거나 AI의 core에 가까운 연구를 하게 된다면, optimizer 직접 짜야하는 일이 많으므로 잘 이해해 놓는 것이 좋다.<br><br>



- <b>(5강) PyTorch Dataset & DataLoader</b><br><br>
: <br><b>키워드</b><br><br>
: - Datasets
: - Dataloaders
: - Data Augmentation
: - Data Preprocessing
: <br><b>인트로</b><br><br>
: : Data Centric에서 중요한 PyTorch의 Dataset API를 배워본다.
: : 각각 NLP, Image 등의 repo들이 존재하는데, 그런 데이터셋 구조들의 가장 기본이 되는 것이 Dataset API 이다.
: : 이를 이용해 쉽게 모델에 데이터를 feeding 할 수 있다.
: <br><b>모델에 데이터를 먹이는 방법</b><br><br>
: : Dataset 이라는 클래스에서, 길이가 얼마나 되는지, map-style (불러올 때 어떻게 반환해 줄지) 등을 선언하게 된다.
: : 데이터를 전처리 해주는 부분과, 데이터를 Tensor로 바꿔주는 부분이 구분되는데, 해당 역할은 Transform 에서 많이 한다.
: : 데이터 로더는 정의된 데이터를 묶어서(batch, shuffle) Model에 feeding 해주는 역할을 한다.
: <br><b>Dataset 클래스</b><br><br>
: : 데이터 입력 형태를 정의, 입력 방식의 표준화
: : Image, Text, Audio 등에 따른 다른 입력정의
: <br><b>Dataset 클래스 생성시 유의점</b><br><br>
: : 데이터 형태에 따라 각 함수를 다르게 정의함
: : ex. NLP에서 다음 단어를 예측하는 모델에서 길이가 가변적임
: : 모든 것을 데이터 생성 시점에 처리할 필요는 없음
: : image의 Tensor 변화는 학습에 필요한 시점에 변환 (cpu는 데이터 변환, gpu는 학습. 둘을 병렬적으로 진행 가능)
: : 데이터셋에 대한 표준화된 처리방법 제공 필요 (후속 연구자 혹은 동료에게 도움)
: : 최근에는 HuggingFace등 표준화된 라이브러리 사용
: <br><b>DataLoader 클래스</b><br><br>
: : Data의 Batch를 생성해주는 클래스
: : 학습직전(GPU feed 전) 데이터의 변환을 책임 
: : Tensor 변환 + Batch 처리가 메인 업무
: : parameter들 중, sampler는 인덱스를 정해주는 기법
: : collate_fn => 0으로 패딩 해줄때, 동일하게 패딩하는 방법 등을 이곳에 적용한다 (시퀀스형 데이터에서 많이 쓰는 기능)
: <br><b>Case Study</b><br><br>
: : 실습 코드 참조
: <br><b>정리</b><br><br>
: : OOP쪽 개념을 보강하고, 파이토치 기본 코드를 뜯어보며 잘 이해하면 큰 도움이 된다.



<h2>과제 수행 내용</h2>
- 기본 과제 2
: : collate 함수 정의하는 문제가 오래 걸렸다(거의 40분?). torch.stack() 함수와 torch.cat()함수에 대해 배웠고, 이를 잘 익혀놓자.
: : 파이썬의 내장함수 iter, next에 대해 알아본다. (<a href="https://zephyrus1111.tistory.com/235">링크</a>)



<h2>Peer Session</h2>
<br><b>기본 과제 1 토론</b><br><br>
: <br><b>hook</b><br><br>
: : hook에 여러 종류가 있는데 어떤 차이?
: - 일단 공식문서에는 그냥 backward_hook이 deprecated됐고, full_backward_hook을 사용한다고 되어있음.
: - forward, backward과정에서 디버깅을 위한 메서드.
: : hook은 일반적 용어
: - 네트워크에서도 각 단계별로 훅이 걸려있어서 각 사이 단계에 발동한다
: - 리액트 훅(Hook)도 같은 용어에서 나온 것
: <br><b>full_backward_hook</b><br><br>
: : backward_hook으로 역전파 gradient 값 변경하는 문제에서 grad_input과 grad_output변수가 의미하는 바는? 
: : gradient 출력이라고 했는데 변수명은 grad_input이어서 헷갈린다!
: : => register_full_backward_hook을 보면 인자로 (module, grad_input, grad_output)을 받는데, 뒤의 두 인자는 순서대로 "input값의 gradient", "output값의 gradient" 라는 의미를 갖는다.
: : 역전파 과정에서는 입력과 출력의 순서가 반대이므로, 결국 그래디언트 출력은 grad_input이 되는 것이다!
