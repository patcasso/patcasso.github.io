---
layout: single
title:  "Day 6 학습정리"
categories: boostcamp-note-week2
sidebar:
  nav: "docs"
---

# 23/11/13 (월) 학습 내용

<h2>PyTorch</h2>

- <b>(1강) Introduction to PyTorch</b><br><br>
: <br><b>강의 키워드</b><br><br>
: - Dynamic Computation Graph
: - Tensors
: - Autograd system
: <br><b>소개</b><br><br>
: : 딥러닝을 할 때 코드는 처음부터 다 짠다?
: : 지금은 남이 만든 걸 쓴다 (자료도 많고, 관리 잘 되고, 표준이다)
: : ex) TensorFlow - 구글에서 내부적으로 쓰던 프레임워크인데, 공개한 이유는 문서화를 잘 하기 위해서이다
: : 프레임워크를 공부하는 것이 곧 딥러닝을 공부하는 것이다
: : ex2) MS - CNTK, mxnet, Caffe, Theano...
: : 리더는 단 두개! PyTorch, TensorFlow
: : 원래 Torch라는 도구가 있었는데, Python이 AI 분야의 표준 언어로 자리잡으면서 Torch에 Python을 붙여 만든 것임
: : Keras라는 언어도 있었는데, 이는 wrapper임 (UI 같은 것)
: : Keras의 하단에는 TensorFlow, PyTorch를 선택할 수 있었음. (그래서 Keras를 High level API 라고 함)
: : 이제 TensorFlow 2.0에 Keras가 병합됨
: : PyTorch도 High Level로 쓸 수 있다.
: : PyTorch의 큰 특징중 하나는 Dynamic Computation Graphs (TensorFlow는 Static graphs)
: : 실행 시점에서 그래프가 그려지는 언어
: : PyTorch는 Dynamic Computation Graph, TensorFlow는 Define and run
: <br><b>Computational Graph</b><br><br>
: : 연산의 과정을 그래프로 표현
: : Define and Run - 그래프를 먼저 정의 -> 실행 시점에 데이터  feed (TensorFlow에서 쓰는 기법)
: : Define by Run (Dynamic Computational Graph, DCG) (실행을 하면서 그래프를 생성하는 방식) (PyTorch 방식)
: : 가장 편해진 것이 디버깅 과정. 중간중간에 값을 확인할 수 있다.
: : 2018년 정도부터 자리를 잡기 시작했음.
: : Andrej Karpathy - "I've been using PyTorch and I have more energy. My skin is clearer. My eye sight has improved."<br><br>
: : TensorFlow는 구글이기 때문에 프로덕션, 클라우드, 멀티 GPU 등에서 장점을 보인다.
: : PyTorch는 논문을 작성하거나 연구를 할 때는 PyTorch가 장점을 가진다 (조금 더 배우기 쉽다)
: <br><b>Why PyTorch</b><br><br>
: : Define by Run의 장점
: : 즉시 확인 가능 -> pythonic code
: : GPU support, Good API community
: : 사용하기 편한 장점
: : TF는 production 와 scalability의 장점
: : PyTorch에서 중요한 3요소 : Numpy + AutoGrad + Function
: : Numpy 구조를 가지는 Tensor 객체로 array 표현
: : 자동 미분을 지원하여 DL 연산 지원
: : 다양한 형태의 DL을 지원하는 함수와 모델을 지원 (Dataset, Multi-GPU, Data augmentation, ...)
: : 복잡하게 해야 할 것을 간단하게 할 수 있는 장점을 가진다!

- <b>(2강) PyTorch Basics </b><br><br>
: <br><b>키워드</b><br><br>
: - Tensor handling
: - Tensor operations
: - AutoGrad
: <br><b>인트로</b><br><br>
: : Numpy와 거의 유사하다
: : 자동미분(Auto Gradient)의 표현이 조금 다르다.
: : 기본적인 PyTorch 구성 문법과, 나중에 실제 수식들을 PT로 표현할 때 어떻게 쓸 수 있을지 배워본다.
: <br><b>numpy + AutoGrad</b><br><br>
: : 대부분의 파이썬 딥러닝 라이브러리들은 numpy 기반이므로, numpy를 잘 공부해두자.
: <br><b>Tensor</b><br><br>
: : 다차원 Arrays를 표현하는 PyTorch 클래스
: : Tensor 생성은 list나 ndarray를 사용 가능
: <br><b>Tensor data types</b><br><br>
: : numpy와 동일하나, GPU를 쓸 수 있게 해주냐 아니냐에 차이가 있다.
: <br><b>numpy like operations</b><br><br>
: : 기본적으로  numpy의 대부분 사용법이 그대로 적용됨
: : indexing, slicing, x_data.flatten(), torch.ones_like(), x_data.numpy()
: : pytorch의 tensor는 GPU에 올려서 사용 가능
: : x_data.device() 는 cpu 사용하면 cpu, gpu 사용하면 cuda 라고 출력
: : Colab은 하나의 가상 컴퓨터이기 때문에, 생성이 되고 꺼지고 다시 생성이 되면 리소스가 다시 리셋이 된다.
: : 그래서 항상 런타임을 바꾸고 실행하면 다시 한번 실행을 다 해줘야 한다.
: <br><b>Tensor handling</b><br><br>
: : view, squeeze, unsqueeze 등으로 tensor 조정 가능
: : view - reshape과 동일하게 tensor의 shape을 변환
: : squeeze - 차원의 개수가 1인 차원을 삭제 (압축)
: : unsqueeze - 차원의 개수가 1인 차원을 추가<br><br>
: 1) view
: : view는 보장을 해주고, reshape는 보장을 안 해준다(메모리에서 모양을?)
: : 그냥 view를 쓰면 된다.
: : <a href="https://jimmy-ai.tistory.com/151">reshape vs view 참고 글</a><br><br>
: : view와 reshape 함수는 모두 tensor의 차원 크기를 변경할 때 사용되는 함수다. 
: view와 reshape의 차이로는 contiguity 차이가 존재하며, view는 tensor가 메모리에 연속적으로 존재할 때만 사용할 수 있다. reshape는 메모리에 연속적으로 존재하지 않을 때, 새로 copy를 하여 차원의 크기를 변경하며 메모리에 연속적으로 tensor가 존재하는 경우에는 원래 tensor와 메모리를 공유한다.<br><br>
: : tensor가 메모리에 연속적으로 존재하는지를 체크할 수 있는 함수로는 is_contiguous 함수를 사용하여 체크할 수 있다.<br><br>
: : tensor가 contiguous가 깨지는 경우는 대표적으로 `t()`와 같은 transpose 연산이나 `permute()`과 같은 연산에서 tensor의 contiguous가 깨진다.<br><br>
: : view는 tensor가 contiguous하다는 것을 암시적으로 나타내므로, copy 없이 더 빠른 연산을 가능하다는 측면에서 view를 권장한다.<br><br>
: 2) squeeze, unsqueeze
: : x_data.unsqueeze()안에 axis를 0, 1, 2, ... 어떻게 넣어주냐에 따라 어떤 shape가 되는지 잘 숙지하자.
: <br><b>Tensor operations</b><br><br>
: : numpy operations와 거의 동일함 (덧셈, 뺄셈, 스칼라 덧셈 등)
: : shape이 맞아야 서로 덧셈 등 된다
: : 행렬곱셈 연산 함수는 numpy와 달리 dot이 아닌 mm(matirx multiplication) 사용 (양쪽 다 벡터행렬이어서 내적일 때는 dot을 쓰면 되는데, 이외에는 mm으로 구분하여 사용한다)
: : mm을 사용하라 (matmul도 되긴 하는데..?)
: : matmul은 broadcasting 지원 처리 (mm은 안되고 matmul은 된다. 그래서 헷갈리지 않게 mm 사용을 권장하시는 듯)
: <br><b>Tensor operations for ML/DL formula</b><br><br>
: : torch.nn.functional 모듈을 통해 다양한 수식 변환을 지원한다. (ex. softmax, one_hot 등)
: <br><b>AutoGrad</b><br><br>
: : PyTorch의 가장 핵심적인 기능
: : PT의 핵심은 자동 미분의 지원 -> backward 함수 사용

- <b>(3강) PyTorch 프로젝트 구조 이해하기 </b><br><br>
: <br><b>키워드</b><br><br>
: - PyTorch Project Template
: <br><b>ML 코드는 언제나 Jupyter에서?</b><br><br>
: : 영원히 세발자전거를 탈 수는 없다.
: : (개발) 초기 단계에서는 대화식 개발 과정이 유리 - 학습과정과 디버깅 등 지속적 확인
: : 배포 및 공유 단계에서는 notebook 공유가 어려움 (실행순서 꼬임)
: : DL 코드도 하나의 프로그램 - 개발 용이성 확보와 유지보수 향상 필요
: : 코드도 레고블럭처럼 (OOP + 모듈 -> 프로젝트)
: <br><b>PyTorch Project Template Overview</b><br><br>
: : 다양한 프로젝트 템플릿 존재
: : 사용자 필요에 따라 수정하여 사용
: : 실행, 데이터, 모델, 설정, 로깅, 지표, 유틸리티 등 다양한 모듈들을 분리, 프로젝트를 템플릿화하여 레고 블럭처럼 끼워넣는다.
: : ex) victoresque 템플릿
: : 실행, 설정, base-abstract module, data, model-architecture, loss, metric, 저장소-로그, 모델 상태, 학습 수행, 로깅 설정, 유틸리티<br><br>
: : train.py로 시작
: : config.json에 여러 세팅 확인 가능 (하이퍼파라미터 여기에서 다 설정 가능)
: : get_logger('train') - train 단계에서 로깅을 한다
: : init_obj - object를 불러오는 것
: : config.json에서 get attribute 방식으로 불러오고, 기존 코드는 바꾸지 않으려는 형태로 템플릿이 만들어져있다.
: : valid data loader, model 등도 다 동일한 방식으로 불러온다.
: : train.py의 trainer가 중요한데, 모델을 뭘 쓰고, loss 값은 뭘 쓰고 등을 정한 후 trainer.train() 불러주면 그때부터 학습이 시작된다.
: : trainer 파일이 실제 트레이닝이 되는 기능이므로 잘 봐둔다.

<h2>과제 수행 내용</h2>
- 확인하기 퀴즈
: : 2강 확인하기 - 2번 문제 view 와 reshape에 대한 설명 다시 숙지하기
- 기본 과제 1
: : VS Code 환경에서 기본과제 1 수행 (Colab은 런타임이 끝나면 윗단 코드를 다시 실행해야 해서, 이렇게 긴 문서의 작업에는 불편하다)



<h2>Peer Session</h2>
: <br><b>1) 강의</b><br><br>
: : 3강에서 프로젝트 템플릿 구조 설명 부분 따라가기 어려웠음.
: <br><b>2) 기본 과제 1</b><br><br>
: : nn.Module에서 pass 는 무슨 역할을 하는가? => 함수나 클래스의 구현을 미룰 때 쓰는 pass 문
: : torch.addmm() 함수 <a href="https://pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm">공식 문서</a>
: : Parameter 클래스에 대해 잘 공부해보기 <a href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html">링크</a>
: : get_submodule 함수 다시 이해해보기