---
layout: single
title:  "Day 29 학습정리"
categories: boostcamp-note-week6
sidebar:
  nav: "docs"
---

23/12/14 (목) 학습 내용

<h1>NLP 기초 프로젝트</h1>

<h2>(6강) Huggingface</h2>
- 강의 키워드<br><br>
: - Huggingface
: - Pre-trained model 
: - Model load

- 소개<br><br>
: : 1) Transfer Learning
: : 2) Why HuggingFace
: : 3) What is HuggingFace?
: : 4) How do we use it?
<br><br>

- Transfer Learning <br><br>
: : 전이 학습
: : 바닥부터 시작하지 말고, 한 분야에 전문적인 모델을 다른 분야로 어떻게 부드럽게 transfer 할지 고민하는 방법론
: <br><b>Pretraining & Fine-tuning</b><br><br>
: : 특정 분야의 label된 데이터는 구하기 힘들고 비싸고, 양도 적다.
: : 이전에는 이런 Small-Scale Tagged Data를 항상 바닥부터 시작해서 학습해나가고 있었다.
: : Small-scale로 매번 이렇게 바닥부터 하지 말고, Large-Scale Raw Data를 통해 먼저 학습한 모델의 지능을 전이시키는 방법론
: : 이렇게 Fine-tuning한 모델이 SOTA(State Of The Art)의 성능을 보이는 것을 확인
: : 이것이 2018년 정도인데, ELMO, BERT 등이 발표되었다.
<br><br>
: <br><b>BERT</b><br><br>
: : Pretraining & Fine-tuning을 잘 입증한 모델
: : 대규모 말뭉치들을 활용해 오랜 기간 pretraining을 수행
: : Masked language model로 훈련시킴
<br><br>
: : 각각의 언어에 맞게 잘 훈련된 후 배포된 transformer들을 우리가 이용해서 fine-tuning 해야 한다는 것!
: : 내가 가진 Small-scale의 현업 데이터를 training 시키면 되는 것이다.
<br><br>

- Why HuggingFace?<br><br>
: : 모델을 훈련시키는 사람과 사용자 간의 공통된 Interface의 존재 필요성
: : cf) 기찻길의 규격에 맞는 여러 호환 가능한 기차
<br><br>
: : 이미 좋은 성능을 내도록 만들어진 기존의 architecture들을 다른 사람들이 활용할 수 있도록 도와주는 것
<br><br>
: : ML 기반 자연어 처리 모듈에 필요한 3가지 자원
: : 1) Model(현재 대부분은 Transformer) Parameter
: : 2) 어휘 사전 (Token, embedding value)
: : 3) Tokenization 방법 (코드)
<br><br>
: : 이전에는 논문을 저자로부터 어렵게 구해서, 내 컴퓨터에서 직접 구현했어야 하는데
: : 이제는 좋은 논문이면 HuggingFace에 동시에 발표하는 것이 일반화 되어있다.
<br><br>

- What is Huggingface?<br><br>
: : 다양한 분야에 대한 Pretrained 모델들이 많으므로, 어떤 프로젝트를 시작하기 전에 먼저 여기서 검색해보면 많은 수고를 줄일 수 있다.

- How do we use it?
: : Google의 BERT를 HuggingFace에서 PyTorch용으로 transformers 모듈로 만들어놓았다.
: : 자주 반복되는 주요 Utility (예 - 문장생성) 등이 제공됨 (Beam Search 등)

<h2>(7강) N21</h2>

- 강의 키워드<br><br>
: - N21
: - 분류 문제(Classification)

- Neural Network based Classification<br><br>
: <br><b>접근 방법</b><br><br>
: : Supervised Learning + Classification
: : 이 테크닉은 거의 비슷한 형태를 취한다.
<br><br>
: <br><b>감성 분류기 구현 예</b><br><br>
: : 전형적인 N21 문제 중 하나
: : 기획 단계에서 종류(Classification)을 몇 가지로 정의할지부터 시작해야 한다.
: : ex) Positive, Negative, Neutral, Objective
: : 일단 데이터의 라벨을 사람이 직접 분류해야 한다. 굉장히 비싼 데이터가 된다.
<br><br>
: <br><b>신경망 디자인</b><br><br>
: : N개 Class 정의 후
: : Big & Deep Network 구성
<br><br>
: <br><b>신경망 분류 기법</b><br><br>
: : 정답을 어떻게 표현할 수 있을까?
: : 좋은 데이터를 넘겨줘야 좋은 답이 나올 것이다.
: : 문장과, 이에 해당하는 Label
<br><br>
: : 그런데 점수를 과연 일관되게 부여할 수 있을까?
: : 정답 데이터를 만들 때, Quality에 영향을 주는 여러 지표 중 하나가 일관성이다.
: : One-Hot Representation (ex. Positive이면 해당 값만 1로 해서, [1, 0, 0, 0])
: : 명확한 한계가 있지만, 일관성은 있다.
<br><br>
: : Score Normalization
: : 예측과 정답을 서로 비교하기 위해서 서로 같은 Scale의 값이어야 한다.
: : Softmax Function (0.0 ~ 1.0 값으로 변환)
<br><br>
: : cf) 지수 함수 $f(x)=e^x$
: : 어떤 x값이 들어오더라도 양수 값으로 바꿔주고
: : 큰 값일수록 더욱 크게 만들어줌
<br><br>
: : * Loss Function, Cost Function, Object Function 모두 같은 말이다.
: : 만들어도 되고 가져와서 써도 되지만, 만들어진 Loss Function이 반드시 미분 가능해야 한다.
: : 분류에 있어서는 Cross Entropy Loss가 좋다.
: : 어떤 값을 가진 여러 개의 값의 덩어리인데, 모두 더했을 때 1.0을 넘지 않는다 -> 확률분포!
: : 숫자 n 개의 덩어리를 확률분포처럼 다루는 것이다.
: : A라는 확률분포(예측)와 B라는 확률분포(정답)라고 봤을 때, 
: : 예측과 정답이 얼마나 같은지에 대한 문제를, A라는 확률분포와 B라는 확률분포가 얼마나 유사한가로 바꾸어서 접근하는 것
: : 이것이 바로 KL-Divergence(쿨백-라이블러 Divergence)
: : KL-Divergence 는 원래 방향성이 있는 수식인데 (A->B != B->A), 이 방향성을 없애 놓은 것이 바로 CrossEntropy이다.
<br><br>
: : 오류가 작아지는 양 & 방향
: : "미분"
: : "기울기 = Gradient"
<br><br>
: <br><b>정리</b><br><br>
: 1) Feed Forward and Prediction : 최종 결과물을 얻고
: 2) Cost Function : 그 결과물을 우리가 원하는 결과물과의 차이점을 찾은 후
: 3) Differentiation (미분) : 그 차이가 무엇으로 인해 생기는지
: 4) Back Propagation : 역으로 내려가면서 추정하여
: 5) Weight Update : 새로운 Parameter 값을 배움
<br><br>
: : 이 한 번의 전체 과정을 하나의 step이라고 부른다. 한 번의 step마다 parameter가 업데이트 되는 것이다.
: : 위 프로세스의 흐름이 전형적인 신경망 기반 훈련 / 예측의 구조가 됨.
<br><br>