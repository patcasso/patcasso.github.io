---
layout: single
title:  "Day 27 학습정리"
categories: boostcamp-note-week6
sidebar:
  nav: "docs"
---

23/12/12 (화) 학습 내용

<h1>NLP 기초 프로젝트</h1>

<h2>(3강) NL Data 관리 및 처리 툴 소개</h2>
- 강의 키워드<br><br>
: - Pandas
: - PyTorch Dataset

- 소개<br><br>
: : Pandas는 Excel 같은 것!
<br><br>

- Pandas 소개<br><br>
: : Series & DataFrame
: : Series는 column 하나인 데이터 구조
: : DataFrame은 여러 column과 row 로 이루어진 행렬 형태의 데이터 구조
<br><br>
: : Column 명과 Row의 범위를 이용하여 데이터 선택
: df['A'] - 'A' 칼럼만 선택
: df.loc['가'] - 해당 Index에 해당하는 데이터만 가져옴
: df.loc['가','B']
: df.iloc[3]
<br><br>
: : 조건을 이용하여 데이터 선택 (ex. df[df > 0])
: : 행방향으로, 또는 열방향으로의 합 구하기
<br><br>

- Pandas with NLP<br><br>
: : 다양한 확장자 / 형태의 데이터를 읽고 쓰기
: : 해당 데이터 로더들을 다 직접 구현하려면 쉽지 않음
: : read_csv, read_excel, read_html 처럼 자주 쓰는 데이터들은 다 있음
: : 다양한 Column 명으로 데이터 탐색 가능
: : 조건을 이용한 특정 단어가 있는 데이터 검색 (isin 활용)
: : 시각화 툴을 기본적으로 제공해준다


<h2>(4강) Tokenization</h2>
- 강의 키워드<br>
: - token
: - Tokenization 
: - Byte Pair Encoding (BPE)
<br><br>

- 소개<br><br>
: Text를 숫자로 바꾸는 방법에는 2가지 문제를 해결해야 한다.
: : 1. text를 어떤 단위(token)로 나눌 것인가? -> Tokenization
: : 2. token을 어떤 숫자로 바꿀 것인가? -> Embedding
<br><br>

- Tokenization<br><br>
: : Text를 숫자로 변환하려는 시도
<br><br>
: **Bag of Words Representation**
: : 단어가 나타난 횟수(frequency)를 세어 text를 숫자로 변환
: : 차원 수는 어휘 개수
<br><br>
: **TF-IDF(Term Frequency-Inverse Document Frequency)**
: : 단어의 빈도와 역 문서 빈도를 사용하여 DTM내의 각 단어들마다 중요한 정도를 가중치 변환
: : 다른 문서에는 많이 안 나타나지만, 내 문서에는 많이 나타나는 단어들을 찾아내는 알고리즘
: : 전통적 정보검색기가 많이 써왔던 방식
<br><br>
: **Word2Vec**
: : 최근의 자연어 처리 발전은 임베딩 테크닉의 발달 덕분인데, 
: : Word2Vec은 수많은 데이터에서 a라는 단어와 c라는 단어 사이에 b가 있으면, b는 a, c와 연관되어 움직인다는 정보를 수많은 데이터를 통해 잡아내려고 하는 시도
: : 단어 벡터 간 유의미한 유사도를 반영할 수 있도록 단어의 의미를 수치화
<br><br>

- Embedding<br><br>
: : 토큰화된 Text에 구체적인 숫자를 부여하여 신경망이 이해할 수 있도록 하는 과정을 Embedding이라고 한다.
<br><br>

- Tokenization Methods<br><br>
: <br><b>1. Character-based Tokenization</b><br><br>
: : Character 단위로 토큰화, whitespace도 토큰화하여 정보로 사용
: : 상대적으로 긴 length가 만들어지므로, 많은 메모리 및 계산량 필요
: <br><b>2. Word-based Tokenization</b><br><br>
: : 기준점이 띄어쓰기 단위 (delimiter : space)
: <br><b>3. Subword-based Tokenization</b><br><br>
: : 데이터를 통해 가장 많이 묶여다니는 덩어리, 즉 중요한 덩어리들을 하나로 다루겠다는 아이디어
: : 문장 혹은 단어를 통계적으로 의미있는 단위로 묶거나 분할해서 처리
: : tokenization이 2개의 sub 어휘([token, ization])로 나눠짐
: : 언어, 구조적인 특징보다 데이터가 토큰의 단위를 결정한다고 생각하는 접근
<br><br>
: : BPE 방식
<br><br>

- Tokenization Tools<br><br>
: : 한국어 자연어처리에서 많이 쓰이는 툴들
<br><br>
: <br><b>1. KoNLPy</b><br><br>
: : 래핑 형태로 묶어서 제공하는 서비스
: <br><b>2. SentencePiece</b><br><br>
: : 구글이 배포하고 있는 SentencePiece
: : 한국어 뿐 아니라 영어, 이외 다양한 언어들 지원
: : Subword 형태를 통계적인 기준을 가지고 문장을 분할하려고 하는 접근 방식 중 가장 많이 쓰이는 도구
: <br><b>3. Tokenizers by Huggingface</b><br><br>
: : Huggingface의 Tokenizers의 경우 기배포된 다양한 언어모델들의 tokenization 방법과 어휘사전 등을 지원함