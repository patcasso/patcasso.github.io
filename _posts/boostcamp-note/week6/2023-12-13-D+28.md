---
layout: single
title:  "Day 28 학습정리"
categories: boostcamp-note-week6
sidebar:
  nav: "docs"
---

23/12/13 (수) 학습 내용

<h1>NLP 기초 프로젝트</h1>

<h2>(5강) Transformer</h2>
- 강의 키워드<br><br>
: - Sequence Data
: - Attention Mechanism
: - Transformers

- 소개<br><br>
: : 1) 기존 NN의 문제점
: : 2) Attention Mechanism
: : 3) Transformer
<br><br>

- 기존 NN의 문제점<br><br>
: : 기존 신경망은 input이 하나 들어오면 output이 하나 나간다.
: : Sequence data에 대해서 처리하기 어려움
: : RNN의 등장
: : 순차적 모델링이기 때문에 Long Term Dependency 문제 (Addressed by LSTM, GRU)
: : 과거에 의존성을 갖지 않고 빠르게 동시에 해결하는 방법을 고민한 결과가 Transformer
<br><br>

- Attention<br><br>
: : "Sequence를 서로 Blending 해보는 건 어떨까?"
: : Query, Key, Value 벡터를 Blending Material에 비유
: : Query vector와 multiple vectors(material vectors)의 반응 정도를 확인해보고,
: : 반응하는 정도에 따라 서로 적절히 섞어준다면,
: : 내가 원하는 반응들로 Blending된 vector를 얻을 수 있다!
<br><br>
: : Self-attention
: : Scaled-Dot Product - 약간 Normalize된 형태의 메커니즘
: : 진정한 Transformer의 힘은 Multi-head에서 온다. (다각도에서 문장을 처리하여 높은 성능을 기대할 수 있음)
: : Encoder Block에는 Multi-head self attention 및 Residual connection, normalizing 등이 들어가 있다.
<br><br>
: : Decoder Block의 가장 큰 차이는 가장 아래에 들어오는 input이 Masked Multi-Head Attention 이라는 것
: : Decoder에 특화된 개념인데, 훈련/예측 단계에서 미래의 정보는 모른다고 masking을 해 놓고 시작해야 한다.
: : Decoder의 두 번째 레이어의 Multi-Head Attention은 Self-attention이 아니고,
: : V, K값을 현재 인코더의 hidden state로부터 가져온다
: : 이를 통해 encoder와 decoder가 연결되어, encoder-decoder block처럼 연결된다.
: : Decoder only로 auto-regressive로 활용되는 경우도 있는데 이런 경우가 GPT 1, 2, 3
<br><br>

- Transformer<br><br>
: <br><b>NLP에의 적용</b><br><br>
: : 1) N21
: : 처음의 [CLS] 토큰에도 어텐션이 적용되므로, 얘로 나중에 classification 적용하면 된다.
<br><br>
: : 2) N2N
: : output 값만 각각 추출해 class 개수만큼 projection 시키면 됨
<br><br>
: : 3) N2M
: : N개의 input을 self-attention으로 묶어주고, 디코더를 인코더 결과와 묶어서 하층부에는 Decoder Masked Self-attention, 중반부에는 인코더와 연결하여 cross attention을 써서
: : 디코딩 라벨 개수만큼의 projection을 매번 수행하게 되면 N2M 아키텍쳐가 된다.

<h2>(2강)</h2>
- 강의 키워드<br><br>
: - 
: - 
: - 

- 소개<br><br>
: :
: : 

- 소제목1<br><br>
: :
: : 

- 소제목2<br><br>
: :
: :



<h2>Peer Session</h2>
- 1) 주제 1<br><br>
: : 
: : 
- 2) 주제 2<br><br>
: : 
: : 