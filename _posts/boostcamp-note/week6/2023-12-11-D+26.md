---
layout: single
title:  "Day 26 학습정리"
categories: boostcamp-note-week6
sidebar:
  nav: "docs"
---

23/12/11 (월) 학습 내용

<h1>NLP 기초 프로젝트</h1>

<h2>(1강) NLP Overview</h2>
- 강의 키워드<br><br>
: - NLP Tasks 
: - N21 Problem 
: - N2N Problem 
: - N2M Problem 
<br><br>

- 강좌 소개<br><br>
: <br><b>1) 신경망 기반 NLP 개요</b><br>
: <br><b>2) NLP 도구 소개</b><br><br>
: : Pytorch Lightning
: : NL Data 관리 및 처리 도구
: : Tokenization
: : Transformer
: <br><b>3) NLP 구현</b><br><br>
: : **N21** (문장이 들어가서 하나의 분류가 나오는 문제)
: : **N2N** (문장이 들어가서 각 토큰별로 분류, regression이 이루어지는 것)
: : **N2M** (N개의 토큰이 들어왔을 때, M개의 서로 다른 결과물이 나오는 '대화', '번역' 문제 등)
<br><br>
: <br><b>목표</b><br><br>
: : NLP 서비스의 입출력 설계 가능
: : 모델링 방법 선택 및 디자인 가능
: : 훈련 및 예측기 개발
: : 어떻게 해야 성능 끌어올릴 수 있을지
: : 서비스 형태로 배포할 수 있다 (웹 or 앱)

- NLP Tasks<br><br>
: : NLP in Real Life - 재난이 있는 문장인지 아닌지를 1, 0으로 분류
: : 어떤 Tweet이 진짜 재난인지 분류
: : 전문 용어가 있는 문서에서, 우리가 관심있어할 개체명 등이 분류되는 문제
: : 코드 유사성 판단
: : 이미지 캡션 생성기
<br><br>
: : 주제분류 / 감성분석 / 개체명분석 / 형태소분석 / 대화 / 번역
: : 이렇게 다양한 문제를 어떻게 Neural Network로 풀어낼까?
<br><br>
: : Sequence to Sequence(S2S) Learning
: : 입력과 출력 사이의 무언가를 스스로 데이터를 통해 학습할 수 있다면, 이러한 모든 기술의 총합을 Sequence to Sequence Learning이라고 부르기로 했다. (문제 정의의 framework)
<br><br>
: : Input $X$, Output $Y$라 할 때 $N, M$이 각각 입출력의 숫자
: : N != M 의 경우
: : M = 1이면 N21 의 경우임
: : M > 1이면 N2M의 문제
<br><br>
: : N21 - Class prob. distribution for a sentence
: : N2N - Class prob. distribution for each tokens
: : N2M - Class prob. distribution for decoder outputs
<br><br>
: : N21, N2N은 보통 인코더만 있어도 되나, N2M은 디코더가 필요
: : 현 시점에서 가장 성능이 좋고 범용적인 신경망 구조는 Transformer
<br><br>

- N21 Problem<br><br>
: <br><b>Topic Classification (w/ Transformer)</b><br><br>
: : 주어진 테스트 조각의 주제를 예측
: : 핵심은 Encoder (각 인코더에 따라 성능이 천차만별)
: <br><b>Sementic Textual Similarity (w/ Transformer)</b><br><br>
: : 먼저 마치 하나의 문장인 것처럼 전처리를 한다. (맨 앞에 CLS 토큰, 가운데 SEP 토큰)
: : 분류에서는 확률분포가 아웃풋으로 나왔지만, 이 문제에서는 숫자 하나가 튀어나오도록 regression 비슷한 방식으로 처리하면 된다.
: <br><b>Natural Language Inference (w/ Transformer)</b><br><br>
: : NLI(Natural Language Inference)라고 부른다.
: : 거짓 혹은 모순 등의 형태를 이해하는 지능
: : Multi-label 문제로 바꿔서 처리하는 것이 일반적.
<br><br>

- N2N Problem<br><br>
: : Input - Sentence or Multiple Sentences
: : Output - Class prob. distribution for each tokens
: <br><b><span style="color:blue">Named Entity Recognition (for Comet!)</span></b><br><br>
: : 구조화되지 않은 텍스트에서 개체명의 경계를 감지하고 유형을 분류
: : 현업에서의 문제, 즉 우리 회사만 갖고 있는 데이터셋 안에서 수행하면 Domain Named Entity Recognition이라고 한다.
: <br><b>형태소 분석기(Morphology Analysis)</b><br><br>
: : 현업에서의 대부분의 문제는 N21, N2N으로 분류된다.
<br><br>

- N2M Problem<br><br>
: : 가장 확장성이 높다. (N2N, N21 문제도 해결할 수 있기 때문.)
: : Input - 문장, 혹은 여러 개의 문장
: : Output - 확률 분포인데, output에 해당하는 모든 확률 분포가 나온다.
: : 그래도 대부분 N21, N2N은 인코더만 이용해 해결하는 경우가 많고, N2M은 Generation Task에서 중요하다.
: : 그렇다면 생성의 대상은? -> 사람의 말
: : 디코더가 End of sentence 나올 때까지 계속 생성한다.
: <br><b>Machine Translation</b><br><br>
: : 소스 언어의 인코딩이 먼저 진행되고,
: : 문장 생성하라는 명령을 디코더에 넣어주면,
: : 인코더의 결과와 "start"가 합쳐져서 weather가 생성되고,
: : 다시 디코더가 이를 받으면 인코딩이 또 한 번 실행되고 이에 따라 good이 생성되고,
: : 이런 식으로 계속 반복해서 "end"가 생성될 때까지 계속해서 생성해낸다.
: : n개가 입력으로 들어와도 m이 자유로울 수 있는 이유가, "end"가 나올 때까지 계속 생성하기 때문이다.
: : 물론 이렇게 순차적으로 생성 안 하는 기술도 있지만, 일반적인 auto-regressive의 경우를 설명한 것이다.
: <br><b>Dialogue Model</b><br><br>
: : 입력과 대답의 길이가 같지 않은 경우가 많기 때문
: <br><b>Summarization</b><br><br>
: : 추출(Extractive summarization)
: : 추상화(Abstractive summarization) - 본문에는 한 번도 나타나지 않은 단어가 나올 수 있는 방식 (기존에 학습해놓은 데이터 등을 통해)
: <br><b>Image Captioning</b><br><br>
: : a.k.a. Image-To-Text
: : 이미지를 벡터공간으로 추상화시켜줄 수 있는 인코더가 필요
: : 인코더를 이미지용 인코더, 디코더를 텍스트용 디코더를 쓴다.
: : 반대로 text->image 는 인코더를 텍스트용 인코더, 디코더를 이미지용 디코더를 사용하면 된다.
<br><br>

- NLP Benchmarks and Problem Types<br><br>
: <br><b>GLUE</b><br><br>
: : 대부분 N21의 문제
: <br><b>Super GLUE</b><br><br>
: : 역시 대부분 N21의 문제
: <br><b>KLUE</b><br><br>
: : N21, N2N, N2M 문제를 골고루 섞어놓았음
<br><br>
: : 이런 자료들을 보고, 대회 나가거나 논문을 쓸 때 내가 해결하고자 하는 문제가 어떤 카테고리에 해당하며,
: : 해당 카테고리에 해당하는 가장 좋은 최근의 인코더, 디코더들은 무엇이 있을지,
: : 그것들을 훈련시킬 수 있는 데이터들은 무엇이 있을지를 먼저 고민하면
: : 해당 문제를 빠르고 쉽게 모델링 할 수 있다.

<h2>(2강) PyTorch Lightning 이론</h2>

- Deep Learning Blocks<br><br>
: <br><b>Deep Learning Process</b><br><br>
: <br><b>1) 전처리 과정 (Data Preparation)</b><br><br>
: : 입출력 데이터를 담는 Tensor를 생성하는 과정
<br><br>
: : 1. Load Data
: : 2. Data Separation (Split data to Train / Valid / Test)
: : 3. Prepare data feeding function
: : 텐서화 시키는 노하우가 필요 - matrix 형태로 그대로 넣어주거나, 일렬로 flatten해서 벡터화 해도 된다.
: : 4. Make Batch data
: : 덩어리 데이터가 필요한 이유 : Training, Validation, Test 데이터 각각에 대해 컴퓨터가 처리할 수 있는 단위로 처리해주는 작업이 필요함
: : 정확히는 Mini-batch마다 parameter를 업데이트 하는 건데, 여기서 보통 mini는 떼고 batch_size라고 하는 것임.
<br><br>
: <br><b>2) 신경망의 구조를 디자인하는 부분</b><br><br>
: : Model Implementation - 모델을 구현하는 과정
<br><br>
: <br><b>3) Loss Implementation</b><br><br>
: : 후처리 과정에 대한 부분 - 모델이 얼마나 틀렸는지를 측정하기 위한 함수 설정
<br><br>
: <br><b>4) Updater Implementation</b><br><br>
: : 초기에 설정된 가중치에서 최적의 가중치로 조정하는 옵티마이저를 구현하는 과정
<br><br>
: <br><b>5) Iterative Learning</b><br><br>
: : 데이터 Feeding을 통한 모델 반복 학습 및 검증
<br><br>

- PyTorch Lightning<br><br>
: : DL Interface
: : 운전자가 자동차의 부품을 다 알 필요가 없듯이, 단순화되고 표준화된 인터페이스로 딥러닝을 구현할 수 있도록 하는 High-level 인터페이스
: : Keras도 이와 비슷한, TensorFlow의 high-level 인터페이스
: : 복수의 컴퓨터, 즉 Multi-node 를 이용해 조금 더 빠르고 효율적으로 훈련하게 하는, 인프라단계에서의 지원도 PyTorch Lightning이 자동으로 해준다.
: : 본질에 해당하는 부분만 정의하면, 나머지 부분은 PL이 알아서 지원해줌
<br><br>
: <br><b>Keras & PyTorch Lightning</b><br><br>
: : PyTorch가 지원하는 모든 하부 라이브러리, 하드웨어를 지원한다.
: : PyTorch 혹은 TensorFlow 중 무엇을 쓸지만 결정하면, 둘 다 굉장히 효율적인 도구가 될 수 있다.
<br><br>
: <br><b>Structure Organized</b><br><br>
: : 추상화를 통해 구조화된 코드 작성 가능
<br><br>

- Deep Learning Process<br><br>
: : 1) Data Preparation 
<br>**-> PyTorchLightning.LightningDataModule + PyTorch.utils.data.Dataset**
<br><br>
: : 2) Model Implementation, 3) Loss Implementation, 4) Updater Implementation
<br>**-> PyTorchLightning.LightningModule**
<br><br>
: : 5) Iterative Learning
<br> **-> PyTorchLightning.Trainer**
<br><br>
위 네 가지만 알아도 Neural Network Application 만드는 데 큰 문제가 없다!
<br><br>

- PyTorch만 사용 vs PyTorchLightning 사용 <br>
: <br><b>Data Preparation</b><br><br>
: : Lightning이 준비 과정을 더 직관적으로 알 수 있는 코드를 짤 수 있다.
: : data module 쓸 때 주의할 것 - prepare_data()는 process에서 한 번 호출됨
: : setup()은 gpu마다 호출하여 처리
: : PL에서의 DataLoader는 PyTorch의 DataLoader를 한 번 감싸놓은 형태
<br>
: <br><b>Model Implementation</b><br><br>
: : nn.Module과 거의 비슷하다
: : pl.LightningModule은 nn.Module을 상속하여 작동한다.
: : Optimzer 또한 해당 모듈 안에 넣어두면 필요한 시점에 알아서 불러온다.
: : Train & Loss 과정도 하나의 스텝으로 정리됨
: : Validation & Loss, Test & Loss 마찬가지
<br><br>

- Plus<br>
: <br><b>Other API</b><br><br>
: : 소프트웨어(하드웨어)적으로 구현 난이도가 어려운 것들 혹은 귀찮은 것들을 PyTorchLightning이 알아서 구현해준다는 장점이 있다
: <br><b>Torch metrics</b><br><br>
: : 평가 지표를 표준화하여 제공
: : ex) NLP metrics (BERT Score, BLEU Score, ...)
: : 몇몇 평가지표는 GPU 지원이 가능해졌다 (대용량 데이터에 대해 빠르게 평가 가능)
