---
layout: single
title:  "D-3 학습일지"
categories: boostcamp-note-onboarding
sidebar:
  nav: "docs"
---

# 23/11/03 학습 내용

<h2>AI Math</h2>

- (5강) 딥러닝 학습방법 이해하기<br><br>
: <b>1) 신경망 수식으로 분해하기</b><br><br>
: : 신경망을 수식으로 분해해보고, 선형 모델의 동작 방식과, 신경망이 선형 모델 기반으로 어떻게 해야 비선형 패턴을 배울 수 있는지 공부
: : 선형 모델 먼저 잘 이해하기 (<b>O = XW + b</b>)
: : 출력벡터의 차원이 데이터들이 모여있는 X 행렬의 d차원에서 가중치 행렬인 W 행렬의 p차원으로 바뀐다는 사실을 잘 기억하라.
: : "d개의 변수로 p개의 선형모델을 만들어서, p개의 잠재변수를 설명하는 모델을 상상해볼 수 있다."
: : 도식에서의 화살표들은 w 값들이다.
: <br><b>2) Softmax 연산자</b><br><br>
: : 분류(Classification) 문제를 풀 때는 softmax라는 연산자가 필요하다. 모델의 출력을 확률로 해석할 수 있게 변환해 주는 연산
: : softmax는 exponential, 즉 지수 함수를 통해 계산을 하게 된다. 즉, 이전 그림의 O 벡터를 softmax에 넣으면 확률 벡터로 변환할 수 있다. (즉 선형모델과 비선형 함수의 결합)
: : 분모(numerator)는 각 출력 벡터들의 값에 exp, 지수 함수를 씌운 것을 모두 더해준 것
: : 분자(denumerator)는 각각의 출력 값에 지수함수를 씌워서 분자에 넣게 됨
: : 코드에서 numpy의 max를 사용하는 이유는 너무 큰 벡터가 들어오는 경우 overflow가 일어나지 않도록 하기 위해서임
: : 그러나 추론을 할 때는 one-hot vector로 최대값을 가진 주소만 1로 출력하는 연산을 사용하면 되므로 softmax 필요 없이 최대값만 뽑으면 된다. (즉, 추론을 하는 경우에는 one-hot vector로 만드는 함수만 필요)
: : 다만 학습을 하는 경우에는 softmax가 필요하다.
: : softmax가 아닌 다른 활성 함수(activation function)을 사용하여 분류 문제가 아닌 다른 문제에 쓸 수 있다.
: : 활성함수는 벡터가 아닌 하나의 실수 값만을 input으로 받는다.
: : 이렇게 변형시킨 벡터를 잠재 벡터(hidden vector)라 부르며, 이를 neuron으로 부르게 되고, 이로 이루어진 네트워크를 neural network라고 부르는 것.
: : 활성함수를 씌운 기본적인 NN을 역사적으로는 perceptron이라 불렀음.
: <br><b>3) 활성함수가 뭐에요?</b><br><br>
: : 활성함수는 R 위에 정의된 비선형 함수(실수 값을 input으로 받고 다시 실수 값을 output으로 뱉는)
: : 전통적으로 많이 사용된 함수는 시그모이드(sigmoid)나 하이퍼볼릭탄젠트(tanh) 함수이나, 딥러닝에선 ReLU를 많이 쓰고 있다. (수식 pdf 참조)
: : 신경망은 선형모델과 활성함수를 합성한 함수임.
: : 맨 처음 input X에 선형 모델을 통해 Z라는 출력을 뱉게 되면, Z에 활성함수를 씌우면 새로운 잠재 벡터인 H를 만들 수 있고, 이 잠재 벡터 H를 다시 출력으로 연결시키는 선형 모델을 연결할 수 있다. 이렇게 2개의 선형모델을 연결한 형태를 2층(2-layers) 신경망이라 하고, 오늘날 사용하는 딥 러닝의 가장 기본적인 모형에 해당함. 
: : 그 중간에 활성함수를 쓰지 않으면 그냥 선형모델과 차이가 없음에 주의할 것
: : 이를 반복적으로 적용하게 되면, 다층(multi-layer)신경망으로 부르고, 이것이 오늘날 사용하는 딥 러닝의 가장 기본적인 모델
: : H 행렬은 기본적으로 Z 행렬과 모양은 같고, H의 구성요소는 Z의 각 구성요소에 활성함수를 씌운 행렬임.
: : MLP(Multi-layer Perceptron)의 패러미터는 L개의 가중치 행렬 W(L), ... ,W(1)과 b(L), ... , b(1)의 y절편 파라미터로 이루어져 있다.
: : 1~L까지의 순차적인 신경망 계산을 순전파(forward propagation)이라고 부른다. 이는 학습이 아니라, 주어진 입력에 대해 출력을 내뱉는 연산이다.
: <br><b>* 왜 층을 여러개 쌓나요?</b><br>
: : 이론적으로는 2층 신경망으로도 연속함수를 근사할 수 있지만... (엄청 많은 뉴런이 필요할 수 있음)
: : 층이 깊을수록 목적함수를 근사하는 데 필요한 뉴런(노드)의 숫자가 훨씬 빨이 줄어들어 좀 더 효율적으로 학습이 가능함.
: : 다만 층이 깊어질수록 최적화는 더 많은 노력을 투자해야 함. 이후 residual block에서 다시 다루기로.
: <br><b>4) 딥러닝 학습원리: 역전파 알고리즘</b><br>
: : 딥러닝은 역전파(backpropagation) 알고리즘을 이용하여 각 층에 사용된 패러미터 W와 b를 학습한다.
: : 경사하강법을 적용해 각각의 가중치 행렬을 학습시킬 때, 각각의 가중치에 대한 그래디언트 벡터를 계산해야 경사하강법을 적용할 수 있다.
: : 가중치 업데이트시, 행렬들의 원소의 모든 개수만큼 경사하강법이 적용되게 됨. (그냥 선형 모델모다 훨씬 많이 적용되는 것)
: : 딥러닝은 순차적으로 층별로 쌓아 계산을 하기 때문에, 그래디언트를 한 번에 계산할 수는 없고, 역순으로 순차적으로 한 번씩 계산하게 된다.
: : Forward propagation 과 마찬가지로, Backpropagation도 미분을 계산할 때 순차적으로 하게 된다.
: : 각 층 parameter의 그래디언트 벡터는 윗층부터 역순으로 계산하게 된다.
: : 합성함수의 미분법인 연쇄법칙을 사용해 그래디언트 벡터를 전달하게 된다.
: <br><b>5) 역전파 알고리즘 원리 이해하기</b><br>
: : 역전파 알고리즘은 합성함수 미분법인 연쇄법칙(chain-rule) 기반 자동미분(auto-differentiation)을 사용하는 것으로, 이것이 오늘날 딥 러닝 framework에서 딥러닝을 학습시키는 방법이다. (수식 pdf 참조)
: : 각 뉴런에 해당하는 값을 tensor라 표현하는데, 각각의 tensor값은 컴퓨터의 메모리 값에 저장해 주어야 backpropagation 알고리즘이 작동한다. 따라서 backpropagation은 forward propagation보다 더 많은 메모리를 사용하게 된다.
: <br><b>* 예제: 2층 신경망에서의 BPP</b><br>
: : <i>수식 pdf 참조. 이 수식을 잘 이해해야 할 듯. 아직은 BPP 전체 수식 과정이 잘 이해가 안 된다.</i>
: : 딥러닝 학습시에는 이렇게 계산한 각각의 가중치 행렬에 대한 gradient vector를 SGD를 이용해, 각각의 패러미터들을 미니배치들을 번갈아가며 학습하게 되고, 이로서 주어진 목적식을 최소화하는 패러미터들을 찾을 수 있고 이 원리가 오늘날 사용하게 되는 딥 러닝의 학습 원리이다.
<br><br>

- (6강) 확률론 맛보기
: <br><b>1) 딥러닝에서 확률론이 왜 필요한가?</b><br>
: : 기계학습(machine learning)에서 사용되는 손실함수(loss function)들의 작동 원리는 데이터 공간을 통계적으로 해석해서 유도하게 된다
: : 예를 들어, 회귀 분석에서 손실함수로 사용되는 L2-norm은 예측오차의 분산을 가장 최소화하는 방향으로 학습하도록 유도
: : 분류 문제에서 사용되는 교차엔트로피(cross-entropy)는 모델 예측의 불확실성을 최소화하는 방향으로 학습하도록 유도
: <br><b>* 확률분포는 데이터의 초상화</b><br>
: : 데이터는 확률변수로 (**x**,y) ~ D 로 표기
: : 결합분포 P(**x**,y)는 D를 모델링한다.
: : P(x)는 입력 x에 대한 주변확률분포로, y에 대한 정보를 주지는 않는다.
: : 조건부확률분포 P(**x**\|y)는 데이터 공간에서 입력 x와 출력 y사이의 관계를 모델링한다. (즉, 특정 클래스가 주어진 조건에서 데이터의 확률분포를 보여줌)
: : ex) p(X\|Y = 1) -> Y가 1일 때 X의 확률분포
: <br><b>2) 이산확률변수 vs 연속확률변수</b><br>
: : 확률변수는 확률분포 D에 따라 <u>이산형(discrete)</u>과 <u>연속형(continuous)</u> 확률변수로 구분한다.
: : 이산형 확률변수는 <u>확률변수가 가질 수 있는 경우의 수를 모두 고려</u>하여 <u>확률을 더해서</u> 모델링한다. (수식 pdf 참조) (확률 질량 함수라고 부른다)
: : 연속형 확률변수는 <u>데이터 공간에 정의된 확률변수의 밀도(density)</u>위에서의 <u>적분을 통해 모델링</u>한다.
: : 밀도는 누적확률분포의 변화율을 모델링한다.
: <br><b>3) 조건부확률과 기계학습</b><br>
: : 조건부확률 P(y\|**x**)는 입력변수 **x**에 대해 정답이 y일 확률을 의미함
: : cf) P(y\|**x**)는 확률이 아니고 밀도로 해석하는 것을 주의
: : <a href="https://charmingham.tistory.com/151">선형 회귀와 로지스틱 회귀 차이 정리 링크</a>
: : 회귀 문제의 경우는 확률로 해석하기는 어렵고 밀도 함수로 해석하는데, 조건부확률이 아닌 조건부기대값(\|E) 사용
: : 조건부기대값은 목적식으로 사용하게 되는 L2-norm을 최소화하는 함수이기 때문에 사용하게 되는 것.
: : 딥러닝은 다층신경망을 사용하여 데이터로부터 특징패턴을 추출함
: <br><b>* 기대값이 뭔가요?</b><br>
: : <u>기대값(expectation)은 데이터를 대표하는 통계량</u>이면서, 동시에 확률분포를 통해 다른 통계적 범함수를 계산하는데 사용된다.
: : 평균과 동일한 개념이나, 기계학습에서는 데이터 해석 측면에서 훨씬 폭넓게 활용된다.
: : (기대값 수식 설명) 연속확률분포의 경우에는 주어진 함수의 기대값을 계산할 때, 주어진 함수에 확률밀도함수를 곱한 후에 적분하게 되고,
: : 이산확률분포의 경우 확률질량함수를 곱한 후 급수(summation)을 사용해 기대값을 정의하게 된다.
: : 기대값을 이용해 <a href="https://ko.wikipedia.org/wiki/%EB%B6%84%EC%82%B0">분산(Variance)</a>, <a href="https://ko.wikipedia.org/wiki/%EC%B2%A8%EB%8F%84">첨도(Skewness)</a>, <a href="https://ko.wikipedia.org/wiki/%EA%B3%B5%EB%B6%84%EC%82%B0">공분산(Covariance)</a> 등 여러 통계량을 계산할 수 있다.
: : 연속확률변수인 경우는 밀도함수를, 이산확률변수에서는 질량함수를 곱해야 한다는 사실을 기억할 것.
: <br><b>4) 몬테카를로 샘플링</b><br>
: : 기계학습의 많은 문제들은 확률분포를 모를 때가 많음
: : 데이터를 이용해 기대값을 계산해야 하는데, 이 때 사용하는 방법이 바로 <u>몬테카를로(Monte Carlo) 샘플링 방법</u>이다.
: : 타겟으로 하는 함수 f(x)의 x자리에 샘플링한 데이터를 대입한 후, 데이터들에 따른 f(x(i))값들의 산술평균을 구하면 이것이 구하려고 하는 기대값에 근사하게 됨.
: : 몬테카를로 샘플링 방법은 <u>이산형이든 연속형이든 상관없이 성립</u>한다. (독립적으로 샘플링해야 몬테카를로 방법이 작동한다.)
: : 독립추출만 보장된다면 대수의 법칙(law of large number)에 의해 수렴성을 보장한다.
: <br><b>* 몬테카를로 예제: 적분 계산하기</b><br>
: : pdf 문제 참조 (균등분포에서 데이터를 샘플링해서 함수에 값을 대입하고, 대입된 함수의 데이터를 산술평균 취해준 후에 구간의 길이를 곱하면 함수의 몬테카를로 적분값을 계산할 수 있다.)
: : 반복적으로 계산하면 분산도 구할 수 있고, <a href="https://en.wikipedia.org/wiki/Margin_of_error">오차범위</a>도 구할 수 있게 된다.
: : 만약 샘플 사이즈가 작다면 몬테카를로도 오차범위가 커지고 참값에서 멀어질 수 있기 때문에, 적절한 샘플링 개수를 조절해야만 구하고자 하는 적분값을 구할 수 있으므로 주의.

<h2>Git 기초</h2>

- (5강) git push<br><br>
: : VS Code 상에서 Commit 후에 push까지 할 수 있다.
: : 그런데, VS Code 상에서 계정 두 개를 동시에 사용할 수는 없는 것 같다. 그래서 내 개인 계정으로 로그인하려면 <a href="https://velog.io/@chldmswnl/%EB%A7%A5%EB%B6%81%EC%97%90%EC%84%9C-%EA%B9%83%ED%97%99-%EA%B3%84%EC%A0%95-%EB%B0%94%EA%BE%B8%EA%B8%B0-feat.-visual-studio-code">링크</a>의 방법을 따라 했다.
: : 이렇게 키체인에서 토큰을 통해 로그인 계정을 바꾸면, 다른 기존 계정 프로젝트에서는 또 계정을 바꿔주어야 하므로, 효율적이지 않다.
: : Boostcamp의 여러 프로젝트들을 참여하기 위해서는, ssh로 각 프로젝트 폴더에 로그인 정보를 적용해서 push 할 수 있는 방법을 알아놔야겠다. (<a href="https://velog.io/@jay/multiplegithubaccounts">링크</a>)
: : ssh 이용해서 위의 방법으로 push 성공!
<br><br>

- (6강) git pull<br><br>
: : 로컬에서 개발 하던 도중에 다른 팀원이 본인의 작업물을 push해서 원격저장소에 변화가 생겼다면?
: : 이때 원격저장소의 내용을 현재 내 로컬환경에 반영하고 합쳐 개발을 계속하기 위한 명령어가 git pull이다.
: : 내가 작업한 내용을 commit & push 한 후, 다른 사용자가 pull하면 그 사용자가 작업하던 내용은 계속 uncommitted changes로 뜨고, 그 이전까지 내가 작업해서 push 한 내용이 그 사용자의 로컬 환경에 저장된다.
: : Command Palette(cmd + shift + p) 사용해서 git commit, push, pull 모두 가능하니 익혀두자.
: : <u><i>(*push 한 이후에 git graph가 자동으로 업데이트 되지 않고 새로고침을 눌러야 최신 상태로 업데이트되는데 왜 그럴까?)</i></u>



<h2>Python</h2>

- (2-1강) Variable & List<br><br>
: <br><b>1) 변수(Variables)</b><br>
: : student = Thomas 로 변수를 할당할 때 컴퓨터에서 일어나는 일은, "student라는 <u>변수</u>에 Thomas라는 <u>값</u>을 넣어라" 라는 의미임
: : 변수가 물리적으로 저장되는 곳은?
: : 프로그래밍에서 변수는 <u>값을 저장하는 장소</u>
: : 변수는 <u>메모리 주소</u>를 가지고 있고, 변수에 들어가는 <u>값</u>은 <u>메모리 주소</u>에 할당됨
: <br><b>* 컴퓨터의 구조 - 폰 노이만 아키텍처</b><br>
: : 폰 노이만 아키텍처에서는 값을 입력하거나 프로그램을 실행하면 그 <u>정보를 먼저 메모리에 저장</u>시키고,
: : <u>CPU가 순차적으로 그 정보를 해석하고 계산</u>하여 사용자에게 결과값을 전달
: : 변수의 경우 RAM에 저장되는 것
: <br><b>* 메모리와 변수</b><br>
: : 변수 - 프로그램에서 사용하기 위한 특정 값을 저장하는 공간
: : 선언되는 순간 <u>메모리 특정 영역에 물리적 공간</u>이 할당됨
: <br><b>* 변수 이름 작명법</b><br>
: : 알파벳, 숫자, 언더스코어(_)로 선언 가능
: : 변수명은 대소문자가 구분됨
: : 특별한 의미가 있는 <u>예약어</u>는 쓰지 않는다. (<i>ex.</i> for, if, else 등)
: <br><b>2) Basic Operation</b><br>
: : 기본 자료형
: : 수치자료형 - 정수형(integer) / 실수형(float), 문자형(string), 논리 / 불린(boolean) 자료형
: : 자료형(data type)에 따라 쓸 수 있는 메모리가 다르다.
: : ex) integer 는 32bit
: : 숫자에 . 찍어주면 자동으로 float type으로 선언된다
: <br><b>* Dynamic Typing</b><br>
: : 코드 실행시점에 Data Type을 결정하는 방법 (cf. 컴파일러 언어들은 선언시에 다 적어줘야 한다. 인터프리터 언어는 반대)
: <br><b>* 연산자(Operator)와 피연산자(Operand)</b><br>
: : +, -, *, /, % 와 같은 기호들을 연산자라고 함
: : "3 + 2"에서 3과 2는 피연산자, +는 연산자임
: : 변수가 좌변에 있으면 저장되는 공간, 우변은 그 값을 가져오게 되는 것
: <br><b>* 데이터 형 변환: 정수형 <-> 실수형</b><br>
: : 정수 a를 실수형으로 변환하고 싶으면, a = float(a)식으로 재할당 해야 한다.
: : 실수형에서 정수형으로 변환 시 반올림이 아닌 내림이 일어난다.
: : <i><u>(팁) terminal에서 python 실행 중 ctrl + l 누르면 clear 됨</u></i>
: : 문자열끼리 + 연산자 하면 concatenation이 일어남
: : 데이터형 확인 함수 : type()
: <br><b>2) List</b><br>
: : 여러 개의 데이터를 집합으로 하나의 변수에 저장하기 위한 시퀀스 자료형
: : int, float같은 다양한 자료형 포함
: : list에 있는 값들은 주소(offset)를 가짐
: <br><b>* Python 리스트만의 특징</b><br>
: : 다양한 Data Type이 하나의 List에 들어감 (ex. a = ["color", 1, 0.2, [item1, item2]])
: : a = b 라는 식으로 선언하는 순간, 같은 메모리 공간을 갖게 된다.
: : b = a[:] 라는 식으로 복사를 하면, a와 b가 각각 다른 공간을 갖게 되어 함께 변형되지 않는다.
: : 패킹과 언패킹 -> t = [1,2,3]일 때, a, b, c = t라고 하면 a, b, c 에 각각 1, 2, 3이 할당된다.


<!-- - (2-2강) Function and Console I/O<br><br>
: : (Week 0) File System & Terminal
<br><br>

- (2-3강) Conditionals and Loops<br><br>
: : 파이썬의 역사와 기본 지식
<br><br>

- (2-4강) String and advanced function concept<br><br>
: : -->
