---
layout: single
title:  "D-2 학습일지"
categories: boostcamp-note-onboarding
sidebar:
  nav: "docs"
---

# 23/11/04 학습 내용

<h2>AI Math</h2>

- (7강) 통계학 맛보기<br><br>
: <b>1) 모수가 뭐예요?</b><br><br>
: : 통계적 모델링은 적절한 가정 위에서 확률분포를 추정(inference)하는 것이 목표. 기계학습과 통계학의 공통적 목표.
: : 유한한 개수의 데이터로 모집단의 분포를 정확하게 알아낼 수는 없으므로, <u>근사적으로 확률분포를 추정</u>해야 한다.
: : 데이터가 특정 확률분포를 따른다고 선험적으로(a priori) 가정한 후 분포를 결정하는 모수(parameter)를 추정하는 방법을 <u>모수적(parametric) 방법론</u>이라 한다. (ex. 정규분포에서의 모수는 평균과 분산)
: : 특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌면 <u>비모수(nonparametric) 방법론</u> (기계학습의 방법론은 대부분 비모수 방법론)
: : 비모수 방법론이라고 해서 모수가 없는 것이 아니라, 무한히 많거나 데이터에 따라 바뀌는 방법론을 말한다.  
: <br><b>2) 확률분포 가정하기: 예제</b><br>
: : 우선 히스토그램을 통해 모양을 관찰
: : 데이터가 2개의 값(0 또는 1)만 가지는 경우 -> <u>베르누이분포</u>
: : 데이터가 n개의 이산적인 값을 가지는 경우 -> <u>카테고리분포 (or 다항분포)</u>
: : 데이터가 [0,1] 사이에서 값을 가지는 경우 -> <u>베타분포</u>
: : 데이터가 0 이상의 값을 가지는 경우 -> 감마분포, <u>로그정규분포</u>
: : 데이터가 R(실수) 전체에서 값을 가지는 경우 -> <u>정규분포, 라플라스분포</u>
: : 기계적으로 확률분포를 가정하면 안 되며, <u>데이터를 생성하는 원리를 먼저 고려하는 것이 원칙!</u>
: <br><b>3) 데이터로 모수를 추정해보자!</b><br>
: : 데이터의 확률분포를 가정하면 모수를 추정해볼 수 있다.
: : 가장 흔히 사용하는 정규분포의 경우, 정규분포의 모수는 평균과 분산으로 이를 추정하는 통계량(statistic)은 다음과 같다.
: : 표본평균 -> 주어진 데이터들의 산술 평균
: : 표본분산 -> 주어진 데이터에서 표본평균을 빼서 제곱을 한 후 여기에 산술평균을 취해준다. 단, N이 아니라 N-1로 정의해서 분산을 정의해주면 원래 모집단의 분산 시그마 제곱과 일치하게 된다. (그 이유는 불편(unbiased) 추정량을 구하기 위해서이다)
: : 이를 통해 데이터의 확률분포의 모수를 추정해 볼 수 있고, 이를 통해 예측을 하거나 의사 결정을 내릴 수 있다.
: : <u>통계량의 확률분포를 표집분포(sampling distribution)</u>라 부르며, 특히 표본평균의 표집분포는 N이 커질수록 정규분포를 따른다 (=중심극한정리, Central Limit Theorem)
: : <a href="https://ko.wikipedia.org/wiki/%EC%A0%95%EA%B7%9C_%EB%B6%84%ED%8F%AC">정규분포(normal distribution (= Gaussian distribution))란?</a>
: <br><b>4) 최대가능도 추정법</b><br>
: : 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나는 <u>최대가능도 추정법(maximum likelihood estimation, MLE)</u>이다. (수식 pdf 참조)
: : 데이터 집합 X가 <u>독립적으로 추출되었을 경우 로그가능도를 최적화</u>한다.
: <br><b>* 왜 로그가능도를 사용하나요?</b><br>
: : 데이터의 숫자가 수억 단위로 된다면 컴퓨터의 정확도로는 가능도를 계산하는 것이 불가능하다. (컴퓨터의 연산 오차 때문)
: : 데이터가 독립인 경우, 로그를 사용하면 가능도의 곱셉을 로그가능도의 덧셈으로 바꿀 수 있기 때문에 컴퓨터로 연산이 가능
: : 경사하강법으로 가능도를 최적화할 때 미분 연산 사용하게 되는데, 로그가능도를 사용하면 연산량을 O(n<sup>2</sup>)에서 O(n)으로 줄여준다.
: : 대개의 손실함수는 경사하강법을 사용하므로, <u>음의 로그가능도(negative log-likelilhood)를 최적화</u>하게 됨
: <br><b>*) 최대가능도 추정법 예제: 정규분포</b><br>
: : 최대가능도 추정법은 현재 주어진 데이터를 이용해 Likelihood 함수를 최적화하는 θ를 찾는 것
: : <a href="https://namu.wiki/w/%ED%99%95%EB%A5%A0%EB%B0%80%EB%8F%84%ED%95%A8%EC%88%98">확률밀도함수</a>
: : MLE는 불편추정량을 보장하지는 않는다.
: <br><b>*) 최대가능도 추정법 예제: 카테고리 분포</b><br>
: : 베르누이 분포의 다차원(d차원)으로 확장한 개념
: : one-hot vector로 보통 x 값이 표현되게 된다.
: : 여기서 모수 p<sub>1</sub> ~ p<sub>d</sub>까지를 추정하는 문제. 각각의 차원에서 값이 1 또는 0이 될 확률을 의미하는 모수. 다 더했을 때 1이 되어야 하는 성질이 있음.
: : 유도 과정 pdf 수식 참조
: : 라그랑주 승수법을 이용해 람다를 곱해준 새로운 목적식을 통해 제약식도 만족하면서 log-likelihood를 최대화하는 모수 p<sub>1</sub> ~ p<sub>k</sub>를 구할 수 있다.
: : 유도 과정을 따르다 보면, 결국 <u>카테고리 분포의 MLE는 경우의 수를 세어서 비율을 구하는 것</u>이다.
: <br><b>5) 딥러닝에서 최대가능도 추정법</b><br>
: : 최대가능도 추정법을 이용해서 기계학습 모델을 학습할 수 있다.
: : 딥러닝 모델의 가중치를 Θ = (W<sup>(1)</sup>, ..., W<sup>(L)</sup>)라 표기했을 때, 분류 문제에서 소프트맥스 벡터는 카테고리분포의 모수 (p<sub>1</sub>, ... ,p<sub>k</sub>)를 모델링한다.
: : 원핫벡터로 표현한 정답레이블을 관찰데이터로 이용해 확률분포인 소프트맥스 벡터의 로그가능도를 최적화할 수 있다.
: <br><b>확률분포의 거리를 구해보자</b><br>
: : 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리릍 통해 유도한다.
: : 데이터공간에 두 개의 확률분포 P(x), Q(x)가 있을 경우, <u>두 확률분포 사이의 거리(distance)</u>를 계산할 때 다음과 같은 함수들을 이용한다.
: : 본 강의에서는 <u>쿨백-라이블러 발산 (Kullback-Leibler Divergence, KL)</u>을 살펴본다.
: : 쿨백 라이블러는 크로스 엔트로피 + 엔트로피로 분해된다.
: : 분류 문제에서 정답 레이블을 P, 모델 예측을 Q라고 두면, <u>최대가능도 추정법은 쿨백-라이블러 발산을 최소화</u>하는 것과 같다. (즉, 확률분포 사이의 거리를 최소화시키는 것과 로그가능도 함수를 최대화시킨다는 개념이 밀접하게 연결되어있다.)
: : 즉, <u>기계학습의 원리가 데이터로부터 확률분포 사이의 거리를 최소화하는 것과 동일한 것</u>임을 명료하게(?) 이해할 수 있다!
<br><br>

- (8강) 베이즈 통계학 맛보기<br><br>
: : 지난주 내용은 빈도주의 통계학(frequentist statistics)에 가깝고, 베이즈 통계학(Bayesian statistics)은 이와 달리 어떤 식으로 모델의 모수를 추정하고, 모수를 추정할 때 사용되는 베이즈 정리에 대해 배워본다.
: <br><b>1) 조건부 확률이란?</b><br>
: : P(A∩B) = P(B)P(A\|B)
: : 조건부확률 P(A\|B)는 사건 B가 일어난 상황에서 사건 A가 발생할 확률을 의미한다.
: : P(B\|A) = P(A∩B) / P(A) = P(B) * P(A\|B) / P(A)
: : 즉, A라는 새로운 정보가 주어졌을 때 P(B)로부터 P(B\|A)를 계산하는 방법을 제공한다.
: <br><b>* 베이즈 정리: 예제</b><br>
: : D : 새로 관찰하는 데이터, θ = hypothesis, 모델링하는 이벤트, 모수(parameter)
: : P(θ\|D) = P(θ) * P(D\|θ) \ P(D)
: : 사후확률(posterior) = 사전확률(prior) * 가능도(likelihood) / Evidence
: : likelihood와 evidence를 통해 사전확률을 사후확률로 업데이트 할 수 있다는 뜻
: : ex) COVID-99의 발병률이 10%로 알려져있다. COVID-99에 실제로 걸렸을 때 검진될 확률은 99%, 실제로 걸리지 않았을 때 오검진될 확률이 1%라고 할 때,
: : 어떤 사람이 질병에 걸렸다고 검진결과가 나왔을 때 정말로 COVID-99에 감염되었을 확률은?
: : 이 문제는 사전확률, 민감도(Recall), 오탐율(False alarm)을 가지고 정밀도(Precision)을 계산하는 문제이다.
: : 사전확률 -> 발병률 10%, 가능도 -> P(D\|Θ) = 0.99 P(D\|ㄱΘ) = 0.01
: : Evidence = 0.99 * 0.1 + 0.01 * 0.9 = 0.108
: : P(Θ\|D) = 0.1 * 0.99 / 0.108 ≈ 0.916
: : 만일 오검진될 확률이 10%라고 하면, P(Θ\|D) = 0.1 * 0.99 / 0.189 ≈ 0.524라는 아주 낮은 확률이 나온다.
: : 따라서, 오탐율(False alarm)이 오르면 테스트의 정밀도(Precision)가 떨어진다.
: : <u>조건부 확률의 시각화 (PDF 도식 참조. 중요!)</u>
: : 의료 문제에서는 False Negative를 많이 신경써야 한다(오히려 False Positive는 조금 덜 중요할 수도 있다).
: : 사전확률을 알아야 베이즈 통계학에서 분석이 가능하고, 모르는 경우는 임의로 설정할 수는 있으나 신뢰가 떨어지므로 주의해야 한다.
: <br><b>2) 베이즈 정리를 통한 정보의 갱신</b><br>
: : 베이즈 정리를 통해 새로운 데이터가 들어왔을 때 앞서 계산한 사후확률을 사전확률로 사용하여 <u>갱신된 사후확률을 계산할 수 있다.</u>
: : 이 프로세스를 반복함을 통해, 데이터를 새로 관찰할 때마다 hypothesis나 모델의 패러미터 Θ를 점점 업데이트 하는 방식으로 모델의 예측력을 향상시킬 수 있다.
: : 앞서 COVID-99 판정을 받은 사람이 두 번째 검진을 받았을 때도 양성이 나왔을 때 진짜 True Positive일 확률은?
: : False alarm이 10%여도, 이번에는 91.7로 높은 확률이 되고, 세번째 검사해도 양성이 나오면 정밀도가 99.1%까지 갱신된다.
: <br><b>* 조건부 확률 -> 인과관계?</b><br>
: : 조건부 확률은 유용한 통계적 해석을 제공하지만, <u>인과관계(causality)</u>를 추론할 때 함부로 사용해서는 안 된다.
: : 데이터가 아무리 많아도 조건부 확률만 가지고 인과관계를 추론하는 것은 불가능하다.
: : 인과관계는 <u>데이터 분포의 변화에 강건한 예측모형</u>을 만들 때 필요하다.
: : 조건부확률 기반 예측모형은 데이터분포가 바뀌게 되는 경우 예측정확도가 떨어질 수 있다.
: : 인과관계를 기반으로 한 예측 모형은 데이터분포가 바뀌어도 예측정확도가 크게 떨어지지 않을 수 있다.
: : 단, 인과관계만으로는 높은 예측정확도를 담보하기는 어렵다.
: : 인과관계를 실제로 알아내기 위해서는 <u>중첩요인(confounding factor)의 효과를 제거</u>하고 원인에 해당하는 변수만의 인과관계를 계산해야 한다.
: : 만일 Z의 효과를 제거하지 않으면 <u>가짜 연관성(spurious correlation)</u>이 나온다.
: : 가령, R을 지능지수라 하고, T를 키라고 하면, 둘은 실제로는 상관없지만 데이터분석을 하면 상관 있다고 나온다.
: : 이는 '나이'라는 중첩요인을 제거하지 않았기 때문이다. 나이에 따른 키의 효과를 제거하지 않으면, 당연히 어른이 어린아이보다 지능지수가 높다고 나오기 때문. 즉, '키'가 '나이'라는 요인이 되기 때문임.
: <br><b>* 인과관계 추론: 예제</b><br>
: : 치료법에 따른 완치율의 원인과 결과를 확인하고 싶은 것.
: : 전체적으로 보면 b가 더 높은 완치율을 가진 것 같지만, 각각 사례를 보면 a가 더 높은 완치율을 가진다.
: : 이것이 simpson's paradox라는 역설 문제이다.
: : 수식 PDF 참조하여 이해하자.
: : 중첩효과를 제거하면 a가 전체적으로 더 높은 확률로 나온다?

<!-- 
<h2>Git 기초</h2>

- (5강) git push<br><br>
: : VS Code 상에서 Commit 후에 push까지 할 수 있다.
<br><br>

- (6강) git pull<br><br>
: : 로컬에서 개발 하던 도중에 다른 팀원이 본인의 작업물을 push해서 원격저장소에 변화가 생겼다면?


<h2>Python</h2>

- (2-1강) Variable & List<br><br>
: <br><b>1) 변수(Variables)</b><br>
: : 패킹과 언패킹 -> t = [1,2,3]일 때, a, b, c = t라고 하면 a, b, c 에 각각 1, 2, 3이 할당된다. -->

<!-- - (2-2강) Function and Console I/O<br><br>
: : (Week 0) File System & Terminal
<br><br>

- (2-3강) Conditionals and Loops<br><br>
: : 파이썬의 역사와 기본 지식
<br><br>

- (2-4강) String and advanced function concept<br><br>
: : -->
