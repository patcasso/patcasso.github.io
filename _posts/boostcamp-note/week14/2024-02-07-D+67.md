---
layout: single
title:  "Day 67 학습정리"
categories: boostcamp-note-week14
sidebar:
  nav: "docs"
---

24/02/05 (월) 학습 내용

<h1>MRC (Machine Reading Comprehension)</h1>

<h2>(1강) MRC Intro & Python Basics</h2>
1. Introduction to MRC<br><br>
: <br><b>MRC의 개념</b><br><br>
: : 기계 독해
: : 주어진 지문(Context)를 이해하고, 주어진 질의의 답변을 추론하는 문제
: : 검색 엔진, 혹은 대화 시스템 등에서 널리 사용됨 (ex. 인공지능 스피커)
: <br><b>MRC의 종류</b><br><br>
: : 1) Extractive Answer Datasets
: : 질의에 대한 답이 항상 주어진 지문의 segment(or span)으로 존재
: : Close Tests라고 함
: : Span Extraction (ex. SQuAD, KorQuAD, NewsQA, Natural Questions, etc)
<br><br>
: : 2) Descriptive/Narrative Answer Datasets
: : 답이 지문 내에서 추출한 span이 아니라, 질의를 보고 생성된 sentence (or free-form)의 형태
: : ex) MS MARCO, Narrative QA
<br><br>
: : 3) Multiple-choice Datasets
: : 질의에 대한 답을 여러 개의 answer candidates 중 하나로 고르는 형태
: : ex) MCTest, RACE, ARC, etc
: <br><b>Challenges in MRC</b><br><br>
: : 단어들의 구성이 유사하지는 않지만 동일한 의미의 문장을 이해
: : ex) DuoRC (paraphrased paragraph) / QuoRef (coreference resolution)
: : <u>Unanswerable questions</u>
: : No answer인 질문들에 대해 답변을 내놓는 경우
: : Multi-hop reasoning - 여러 개의 document 질의에 대한 supporting fact를 찾아야지만 답을 찾을 수 있음
: <br><b>MRC의 평가방법</b><br><br>
: : 1) Exact Match / F1 Score (For extractive answer and multiple-choice answer datasets) => Free-form match에는 이 방법을 쓰기 힘들다
: : 2) ROUGE-L / BLUE : For <u>descriptive</u> answer datasets
<br><br>

2. Unicode & Tokenization<br><br>
: <br><b>Unicode 소개 및 Python에서 Unicode 다루기</b><br><br>
: : <u>Unicode란?</u>
: : 전 세계 모든 문자를 일관되게 표현하고 다룰 수 있도록 만들어진 문자셋
: : 각 문자마다 숫자 하나에 매핑한다
: : "U+" : 유니코드를 뜻하는 접두어
: : "AC00" : 16진수 (hexadecimal) Code point
<br><br>
: : <u>인코딩 & UTF-8</u>
: : 인코딩 - 문자를 컴퓨터에서 저장 및 처리할 수 있게 이진수로 바꾸는 것
: : UTF-8는 현재 가장 많이 쓰이는 인코딩 방식
: : 문자 타입에 따라 다른 길이의 바이트를 할당한다.
<br><br>
: : <u>Python에서 Unicode 다루기</u>
: : Python3부터 string 타입은 유니코드 표준을 사용한다.
: : **ord** - 문자를 유니코드 code point로 변환한다
: : **chr** - Code point를 문자로 변환한다
<br><br>
: : <u>Unicode와 한국어</u>
: : 완성형 - 현대 한국어의 자모 조합으로 나타낼 수 있는 모든 완성형 한글 11,172자(가, 각, ..., 힢, 힣) (U+AC00 ~ U_D7A3)
: : 조합형 - 조합하여 글자를 만들 수 있는 초.중.종성
: : (U+1100 ~ U+11FF, U+A960 ~ U+A97F, U+D7B0 ~ U+D7FF)
: <br><br><b>Tokenization</b><br><br>
: : 텍스트를 토큰 단위로 나누는 것
: : 단어, 형태소, subword 등 여러 토큰 기준이 사용된다.
<br><br>
: : <u>Subword 토크나이징</u>
: : 자주 쓰이는 글자 조합은 한 단위로 취급하고, 자주 쓰이지 않는 조합은 subword로 쪼갠다.
: : ##은 디코딩을 할 때 해당 토큰을 앞 토큰에 띄어쓰기 없이 붙인다는 것을 뜻한다.
: : Tokenization 방법은 모델의 일부라고 보면 된다.
<br><br>
: : <u>BPE (Byte-Pair Encoding)</u>
: : 데이터 압축용으로 제안된 알고리즘.
<br><br>

3. Looking into the Dataset<br><br>
: <br><b>KorQuAD 살펴보기</b><br><br>
: : LG CNS가 AI 언어지능 연구를 위해 공개한 질의응답/기계독해 한국어 데이터셋
: : 인공지능이 한국어 질문에 대한 답변을 하도록 필요한 학습 데이터셋
: : 데이터 수집 과정) 대상 문서 수집 -> 질문/답변 생성 -> 2차 답변 태깅
: <br><b>HuggingFace datasets 라이브러리 소개</b><br><br>
: from datasets import load_dataset
: dataset = load_dataset('squad_kor_v1', split='train')
<br><br>
: : HuggingFace에서 만든 datasets는 자연어처리에 사용되는 대부분의 데이터셋과 평가 지표를 접근하고 공유할 수 있게끔 만든 라이브러리
: : Numpy, Pandas, PyTorch, TensorFlow2와 호환
<br><br>


<h2>(2강)</h2>
- 강의 키워드<br><br>
: - 
: - 
: - 

- 소개<br><br>
: :
: : 

- 소제목1<br><br>
: :
: : 

- 소제목2<br><br>
: :
: :
