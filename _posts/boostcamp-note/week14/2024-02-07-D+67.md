---
layout: single
title:  "Day 67 학습정리"
categories: boostcamp-note-week14
sidebar:
  nav: "docs"
---

24/02/07 (수) 학습 내용

<h1>MRC (Machine Reading Comprehension)</h1>

<h2>(1강) MRC Intro & Python Basics</h2>
1. Introduction to MRC<br><br>
: <br><b>MRC의 개념</b><br><br>
: : 기계 독해
: : 주어진 지문(Context)를 이해하고, 주어진 질의의 답변을 추론하는 문제
: : 검색 엔진, 혹은 대화 시스템 등에서 널리 사용됨 (ex. 인공지능 스피커)
: <br><b>MRC의 종류</b><br><br>
: : 1) Extractive Answer Datasets
: : 질의에 대한 답이 항상 주어진 지문의 segment(or span)으로 존재
: : Close Tests라고 함
: : Span Extraction (ex. SQuAD, KorQuAD, NewsQA, Natural Questions, etc)
<br><br>
: : 2) Descriptive/Narrative Answer Datasets
: : 답이 지문 내에서 추출한 span이 아니라, 질의를 보고 생성된 sentence (or free-form)의 형태
: : ex) MS MARCO, Narrative QA
<br><br>
: : 3) Multiple-choice Datasets
: : 질의에 대한 답을 여러 개의 answer candidates 중 하나로 고르는 형태
: : ex) MCTest, RACE, ARC, etc
: <br><b>Challenges in MRC</b><br><br>
: : 단어들의 구성이 유사하지는 않지만 동일한 의미의 문장을 이해
: : ex) DuoRC (paraphrased paragraph) / QuoRef (coreference resolution)
: : <u>Unanswerable questions</u>
: : No answer인 질문들에 대해 답변을 내놓는 경우
: : Multi-hop reasoning - 여러 개의 document 질의에 대한 supporting fact를 찾아야지만 답을 찾을 수 있음
: <br><b>MRC의 평가방법</b><br><br>
: : 1) Exact Match / F1 Score (For extractive answer and multiple-choice answer datasets) => Free-form match에는 이 방법을 쓰기 힘들다
: : 2) ROUGE-L / BLUE : For <u>descriptive</u> answer datasets
<br><br>

2. Unicode & Tokenization<br><br>
: <br><b>Unicode 소개 및 Python에서 Unicode 다루기</b><br><br>
: : <u>Unicode란?</u>
: : 전 세계 모든 문자를 일관되게 표현하고 다룰 수 있도록 만들어진 문자셋
: : 각 문자마다 숫자 하나에 매핑한다
: : "U+" : 유니코드를 뜻하는 접두어
: : "AC00" : 16진수 (hexadecimal) Code point
<br><br>
: : <u>인코딩 & UTF-8</u>
: : 인코딩 - 문자를 컴퓨터에서 저장 및 처리할 수 있게 이진수로 바꾸는 것
: : UTF-8는 현재 가장 많이 쓰이는 인코딩 방식
: : 문자 타입에 따라 다른 길이의 바이트를 할당한다.
<br><br>
: : <u>Python에서 Unicode 다루기</u>
: : Python3부터 string 타입은 유니코드 표준을 사용한다.
: : **ord** - 문자를 유니코드 code point로 변환한다
: : **chr** - Code point를 문자로 변환한다
<br><br>
: : <u>Unicode와 한국어</u>
: : 완성형 - 현대 한국어의 자모 조합으로 나타낼 수 있는 모든 완성형 한글 11,172자(가, 각, ..., 힢, 힣) (U+AC00 ~ U_D7A3)
: : 조합형 - 조합하여 글자를 만들 수 있는 초.중.종성
: : (U+1100 ~ U+11FF, U+A960 ~ U+A97F, U+D7B0 ~ U+D7FF)
: <br><br><b>Tokenization</b><br><br>
: : 텍스트를 토큰 단위로 나누는 것
: : 단어, 형태소, subword 등 여러 토큰 기준이 사용된다.
<br><br>
: : <u>Subword 토크나이징</u>
: : 자주 쓰이는 글자 조합은 한 단위로 취급하고, 자주 쓰이지 않는 조합은 subword로 쪼갠다.
: : ##은 디코딩을 할 때 해당 토큰을 앞 토큰에 띄어쓰기 없이 붙인다는 것을 뜻한다.
: : Tokenization 방법은 모델의 일부라고 보면 된다.
<br><br>
: : <u>BPE (Byte-Pair Encoding)</u>
: : 데이터 압축용으로 제안된 알고리즘.
<br><br>

3. Looking into the Dataset<br><br>
: <br><b>KorQuAD 살펴보기</b><br><br>
: : LG CNS가 AI 언어지능 연구를 위해 공개한 질의응답/기계독해 한국어 데이터셋
: : 인공지능이 한국어 질문에 대한 답변을 하도록 필요한 학습 데이터셋
: : 데이터 수집 과정) 대상 문서 수집 -> 질문/답변 생성 -> 2차 답변 태깅
: <br><b>HuggingFace datasets 라이브러리 소개</b><br><br>
: from datasets import load_dataset
: dataset = load_dataset('squad_kor_v1', split='train')
<br><br>
: : HuggingFace에서 만든 datasets는 자연어처리에 사용되는 대부분의 데이터셋과 평가 지표를 접근하고 공유할 수 있게끔 만든 라이브러리
: : Numpy, Pandas, PyTorch, TensorFlow2와 호환
<br><br>


<h2>(2강)</h2>
1. Extraction-based MRC<br><br>
: : 답변이 지문 내에 span으로 존재
: : e.g. SQuAD, KorQuAD, NewsQA, Natural Questions, etc.
: <br><b>Extraction-based MRC 평가 방법</b><br><br>
: : 1. EM - Exact Match
: : 2. F1 - 부분적으로 겹쳐도 일부 점수를 줄 수 있는 방식
: : Precision = num(same_token) / num(pred_tokens)
: : Recall = num(same_token) / num(ground_tokens)
<br><br>

2. Pre-processing<br><br>
: <b>입력 예시</b><br><br>
: : Title, Context, Question-Answer Pairs
: <br><b>Tokenization</b><br><br>
: : 최근에는 OOV 문제를 해결해주고, 정보학적으로 이점을 가진 BPE(Byte Pair Encoding)을 주로 사용함
: : 본 강의에서는 BPE 방법론 중 하나인 WordPiece Tokenizer를 사용
: : e.g. 미국 군대 내 두번째로 높은 직위는 무엇인가? -> ['미국', '군대', '내', '두번째', '##로', '높은', '직', '##위는', '무엇인가', '?']
: <br><b>Special Tokens</b><br><br>
: : [CLS], [SEP], [PAD], ....
: <br><b>Attention Mask</b><br><br>
: : 입력 시퀀스 중에서 attention을 연산할 때 무시할 토큰을 표시
: : 0은 무시, 1은 연산에 포함
: : 보통 [PAD]와 같은 의미가 없는 특수토큰을 무시하기 위해 사용
: <br><b>Token Type IDs</b><br><br>
: : 입력이 2개 이상의 시퀀스일 때 (예: 질문 & 지문), 각각에게 ID를 부여하여 모델이 구분해서 해석하도록 유도
: <br><b>모델 출력값</b><br><br>
: : 정답은 문서 내 존재하는 연속된 단어토큰(span)이므로, span의 시작과 끝 위치를 알면 정답을 맞출 수 있음.
: : Extraction-based에선 답안을 생성하기보다, 시작위치와 끝위치를 예측하도록 학습함. 즉, Token Classification 문제로 치환.
<br><br>

3. Fine-tuning<br><br>
: : BERT에 Embedding이 들어오고, 또 Embedding이 나오는 구조
: : 지문 내에서 정답에 해당하는 각 임베딩을 실제로 linear transformation(이 역시 학습할 대상)을 통해서, 각 단어마다 하나의 숫자가 나올 수 있도록 바꿔준다.
: : 각 지문 토큰마다 하나의 숫자가 아웃풋으로 나오게 되고, 해당 숫자를 점수로 볼 수 있게 된다.
: : 실제 학습할 때는, 수치에 Softmax를 적용해서 negative log-likelihood로 학습하는 방식을 취함

4. Post-processing<br><br>
: <br><b>불가능한 답 제거하기</b><br><br>
: : End poistion 이 start position보다 앞에 있는 경우
: : 예측 위치가 context를 벗어난 경우 (e.g. question 위치쪽에 답이 나온 경우)
: : 미리 설정한 max_answer_length 보다 길이가 더 긴 경우
: <br><b>최적의 답안 찾기</b><br><br>
: : Start/end position pred에서 score(logits)가 가장 높은 N 개를 찾는다.
: : Score가 가장 큰 조합을 답안으로 선정한다.
: : Top-k가 필요한 경우 차례대로 내보낸다.
<br><br>

