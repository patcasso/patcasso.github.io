---
layout: single
title:  "Day 68 학습정리"
categories: boostcamp-note-week14
sidebar:
  nav: "docs"
---

24/02/08 (목) 학습 내용

<h1>MRC</h1>

<h2>(3강) Generation-based MRC</h2>
1. Generation-based MRC<br><br>
: <br><b>문제 정의</b><br><br>
: : Generation-based는 extraction-based와 달리 생성(generation) 문제이다.
: <br><b>평가 방법</b><br><br>
: : 동일하게 EM과 F1으로 평가한다.
: <br><b>Extraction-based vs Generation-based</b><br><br>
: : 1) 모델 구조 - Generation은 seq-to-seq, Extraction은 PLM(Pre-trained Language Model) + Classifier 구조
: : Loss 계산을 위한 답의 형태 - Generation은 Free-form 형태, Extraction은 지문 내 답의 위치
<br><br>

2. Pre-processing<br><br>
: <br><b>입력 표현 - 데이터 예시</b><br><br>
: : Extraction의 경우에는 정답의 시작점과 끝점을 확인했어야 하는데, 이제는 answer_start 부분은 사용하지 않고 정답 'text'파트만 학습에서 활용하게 된다.
: <br><b>입력 표현 - 토큰화</b><br><br>
: : T5의 경우 SentencePiece라는 다른 방법을 사용하지만, 일단 WordPiece와 비슷한 것으로 생각하기
: <br><b>입력 표현 - Special Token</b><br><br>
: : Pre-trained Language Model별로 다를 수 있으니 꼭 확인해보기
: <br><b>입력 표현 - Additional Information</b><br><br>
: : Attention mask는 동일하게 사용됨
: : Token type ids -> 이런 정보는 별도로 표기하지 않아도 잘 표현하는 것으로 생각되어, BART나 T5에는 토큰 타입 아이디가 들어가지 않는다.
: <br><b>출력 표현 - 정답 출력</b><br><br>
: : Extraction과 달리 Text가 출력되는 것이기 때문에, 조금 더 심플한 형태
: : 모델의 출력을 선형 레이어에 넣고 -> seq_length내 각 위치마다 들어가야 할 단어를 하나씩 선택 -> 정해진 횟수 또는 전체 길이의 수만큼 반복
<br><br>

3. Model<br><br>
: <br><b>BART</b><br><br>
: : 기계 독해, 기계 번역, 요약, 대화 등 seq-to-seq 문제의 pre-training을 위한 denoising autoencoder
: <br><b>BART Encoder & Decoder</b><br><br>
: : BART의 인코더는 bi-directional attention
: : BART의 디코더는 GPT처럼 uni-directional(autoregressive)
: <br><b>Pre-training BART</b><br><br>
: : BART는 텍스트에 노이즈를 주고 원래 텍스트를 복구하는 문제를 푸는 것으로 pre-training 함
: <br><b>T5 (Text-to-Text Transfer Transformer)</b><br><br>
: : 모든 텍스트 처리 문제를 "text-to-text" 문제로 취급
: :  텍스트를 입력으로 취급하고 새로운 텍스트를 생성하는 것을 출력으로 취급
: <br><b>Text-to-Text Framework</b><br><br>
: : Relative position encoding
: : 토큰 사이의 거리에 따라 position encoding이 정의되는 형태를 취함
: : 번역, 요약, 질의응답 등의 모든 task들은 자연어 문장이 들어가고 자연어 문장이 나온다는 점에 착안, 입출력 포맷을 'Text-to-Text'task로 정의
: : Downstream task에 fine-tuning시, prefix를 사용
: : e.g. Input - "translate English to German: That is good" -> Output - "Das ist gut"
: <br><b>Pre-training T5</b><br><br>
: : C4(Clossal Clean Crawled Corpus)를 제작해서 사용함
: : Common Crawl를 기반으로 다음과 같은 기준 등으로 전처리
: - 구두점으로 끝나는 (쉼표, 마침표 등) 문장만 선택
: - 불건전한 페이지의 모든 문장 제외
: - javascript, lorem ipsum, { 등이 포함된 모든 문장 제외
: - langdetect 활용해 0.99의 확률로 영어인 페이지만 사용
=> 750GB에 달하는 방대한 양의 텍스트 코퍼스에 대해 사전학습
: : Span 단위의 Masking(Replace corrupted spans)을 통해 원래 텍스트를 순차적으로 복구하는 것으로 pre-training 함
: : Pre-training dataset과 self-supervised task로 변환된 downstream task들의 dataset을 섞어, 한번에 학습하는 Multi-task Pre-training을 사용함
: : 허깅페이스 google/mt5-base와 같은 형태로 쉽게 불러와 사용할 수 있다.
<br><br>

4. Post-processing<br><br>
: <br><b>Searching</b><br><br>
: : Greedy Search
: : Exhaustive Search
: : Beam Search
<br><br>


<h2>(4강) Passage Retrieval-Sparse Embedding</h2>
- 강의 키워드<br><br>
: - Introduction to Passage Retrieval
: - Passage Embedding and Sparse Embedding
: - TF-IDF

1. Introduction to Passage Retrieval<br><br>
: <br><b>Passage Retrieval</b><br><br>
: : 질문(query)에 맞는 문서(passage)를 찾는 것.
: : 웹 상에서 관련된 문서를 가져오는 시스템을 만드는 것
: <br><b>Passage Retrieval with MRC</b><br><br>
: : 질문과 관련있는 것 같은 문서를 찾은 후, 해당 문서를 잘 보고 다시 질문을 봐서 더 정확히 답변을 하는 것
: <br><b>Overview of Passage Retrieval</b><br><br>
: : Query와 Passage를 임베딩 후, 유사도로 랭킹을 매기고 유사도가 가장 높은 Passage를 선택함
<br><br>

2. Passage Embedding and Sparse Embedding<br><br>
: <br><b>Passage Embedding Space</b><br><br>
: : Passage Embedding의 벡터 공간
: : 두 개의 질문, 혹은 질문과 문서의 유사성을 벡터 상에서의 유사도로 표현한 것이라고 생각하면 됨
: <br><b>Sparse Embedding 소개</b><br><br>
: : Sparse -> Dense의 반대
: : Bag-of-Words (BoW)
: : 1. BoW를 구성하는 방법 -> n-gram
: : unigram (1-gram), bigram (2-gram) or n-gram
: : 2. Term value를 결정하는 방법
: : Term이 몇 번 등장하는지 (term frequency) 등 (e.g. TF-IDF)
: <br><b>Sparse Embedding 특징</b><br><br>
: : 1. Dimension of embedding vector = number of terms
: : 등장하는 단어가 많아질수록, N-gram의 n이 커질수록 증가
: : 2. Term overlap을 정확하게 잡아내야 할 때 유용
: : 3. 반면, 의미(semantic)가 비슷하지만 다른 단어인 경우 비교가 불가
<br><br>

3. TF-IDF (Term Frequency - Inverse Document Frequency)<br><br>
: : Term Frequency (TF):  단어의 등장 빈도
: : Inverse Document Frequency (IDF): 단어가 제공하는 정보의 양
: <br><b>Term Frequency</b><br><br>
: : 1. Raw count
: : 2. Adjusted for doc length: raw count / num words (TF)
: : 3. Other variants: binary, log normalization, etc
: <br><b>Inverse Document Frequency</b><br><br>
: : 단어가 제공하는 정보의 양
: : IDF(t) = log( N / DF(t))
: : 'the'와 같이 모든 문서에 등장하는 단어는 IDF 점수가 0이 된다.
: : a, the 등 관사는 거의 0에 가까운 TF-IDF
: : 자주 등장하지 않는 고유 명사 (ex. 사람 이름, 지명 등) => High TF-IDF
: <br><b>BM25 란?</b><br><br>
: : TF-IDF의 개념을 바탕으로, 문서의 길이까지 고려하여 점수를 매김
: : TF값에 한계를 지정해두어 일정한 범위를 유지하도록 함
: : 평균적인 문서의 길이보다 더 작은 문서에서 단어가 매칭된 경우 그 문서에 대해 가중치를 부여
: : 실제 검색 엔진, 추천 시스템 등에서 아직까지도 많이 사용되는 알고리즘
<br><br>

<h2>(5강) Passage Retrieval-Dense Embedding</h2>

1. Introduction to Dense Embedding<br><br>
: <br><b>Passage Embedding</b><br><br>
: : Sparse Embedding - TF-IDF 벡터는 Sparse하다 (0이 아닌 숫자는 적게 있다 - BoW를 사용하기 때문에 90% 이상의 dimension이 보통 0이 됨)
: <br><b>Dense Embedding 이란?</b><br><br>
: : 더 작은 차원의 고밀도 벡터 (length = 50-1000)
: : 각 자원이 특정 term에 대응되지 않음
: : 대부분의 요소가 non-zero 값
: <br><b>Retrieval: Sparse vs Dense</b><br><br>
: : Sparse - 중요한 term들이 정확히 일치해야 하는 경우 성능 뛰어남, 임베딩이 구축되고 나서는 추가적인 학습 불가능함
: : Dense - 단어의 유사성 또는 맥락을 파악해야 하는 경우 성능이 뛰어남, 학습을 통해 임베딩을 만들며 추가적인 학습 또한 가능
: <br><b>Overview of Passage Retrieval with Dense Embedding</b><br><br>
: : 1. Dense Embedding을 생성한 인코더 훈련 (BERT 부분)
: : 2. 질문과 문서를 비교하여 관련 문서 추출
: : 두 개의 output을 비교해서 score 순으로 유사한 문서를 찾는다
<br><br>

2. Training Dense Encoder<br><br>
: <br><b>What can be Dense Encoder?</b><br><br>
: : BERT와 같은 Pre-trained Language Model이 자주 사용
: : BERT as dense encoder -> [CLS] token의 output 사용
: <br><b>Dense Encoder 구조</b><br><br>
: : Question, Passage를 Bert를 활용해 벡터 두 개를 내보내게 되고, 이 두 벡터 간의 유사도를 이용해 Passage를 찾아내게 된다.
: <br><b>Dense Encoder 학습 목표와 학습 데이터</b><br><br>
: : 학습 목표 - 연관된 question과 passage dense embedding 간의 거리를 좁히는 것 (또는 inner product를 높이는 것) => High similarity
: : Challenge - 연관된 question / passage를 어떻게 찾을 것인가? => 기존 MRC 데이터셋을 활용
: <br><b>Negative Sampling</b><br><br>
: : 연관된 question과 passage 간의 dense embedding 거리를 좁히는 것 (Positive)
: : 연관되지 않은 question과 passage 간의 dense embedding 거리는 멀어야 함 (Negative)
: : 뽑는 방법 - Corpus 내에서 랜덤하게 뽑거나 / 좀 더 헷갈리는 negative 샘플들 뽑기 (ex. 높은 TF-IDF 스코어를 가지지만 답을 포함하지 않는 샘플)
: <br><b>Object Function</b><br><br>
: : Positive passage에 대한 negative log likelihood(NLL) loss 사용
: <br><b>Evaluation Metric for Dense Encoder</b><br><br>
: : Top-k retrieval accuracy - retrieve 된 passage 중에 답을 포함하는 passage의 비율
<br><br>

3. Passage Retrieval with Dense Encoder<br><br>
: <br><b>From dense encoding to retrieval</b><br><br>
: : Inference - passage와 query를 각각 embedding 한 후(Passage는 미리 오프라인으로 인코딩을 해놓는다), query로부터 가까운 순서대로 passage의 순위를 매김
: <br><b>From retrieval to open-domain question answering</b><br><br>
: : Retriever를 통해 찾아낸 passage를 활용, MRC(Machine Reading Comprehension) 모델로 답을 찾음
: <br><b>How to make better dense encoding</b><br><br>
: : 학습 방법 개선(DPR 등)
: : 인코더 모델 개선 (BERT 보다 큰, 정확한 PLM)
: : 데이터 개선 (더 많은 데이터, 전처리 등)