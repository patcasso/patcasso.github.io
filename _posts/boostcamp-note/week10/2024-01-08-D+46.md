---
layout: single
title:  "Day 46 학습정리"
categories: boostcamp-note-week10
sidebar:
  nav: "docs"
---

24/01/08 (월) 학습 내용

<h1>KLUE</h1>

<h2>(10강) LLM</h2>
- 인트로<br>
: <br><b>GPT-3 이후...</b><br><br>
: : 모델의 파라미터와 데이터 크기를 키울수록 성능이 좋아지는 것을 발견
: : 이를 Scaling Law라고 부르게 됨
<br><br>

- LLM의 한계<br><br>
: : 천문학적인 학습 및 운영 비용
: : 프롬프트에 대한 연구 - 프롬프트에 들어가는 단어 하나 때문에도 성능이 왔다갔다 하는 문제
: : AutoPrompt에 대한 연구도 있음 (Prompt를 자동으로 생성해낼 수 있는 method)
<br><br>
: : P-tuning - 어떤 vector 값을 마치 token처럼 LM에 넣어줌으로써 원하는 목적을 달성하도록 학습
: : GPT와 NLU application 간의 다리를 이어주는 학습 가능
<br><br>

- Fine-tuning에 관한 연구<br>
: <br><b>Adapter를 이용한 fine-tuning 기법</b><br><br>
: : 전체 모델은 freeze 시켜놓고, adapter layer만 학슴함으로써 원하는 task에 적용시킨다.
: : LoRA - 모델 전체 재학습은 많은 비용이 들기 때문에, 모델 일부만 학습

- LLM의 한계 2<br><br>
: : 모델 입장에서 최선을 다한 결과를 고치기 어렵다.
<br><br>

- Human-in-the-loop<br><br>
: : 1) 질문에 대한 올바른 답변을 사람이 작성 -> 자연어 생성 인공지능에게 이를 학습시킴
: : 2) 자연어 생성 인공지능이 생성한 답변들 중, 가장 적절한 답변을 사람이 선택 -> '사람의 선택'을 묘사하는 인공지능 학습
: : 3) 자연어 생성 인공지능과 선택 인공지능이 상호작용을 통해 답변 생성 능력 향상
<br><br>
: : hulic.smilegate.net 등의 데이터셋을 통해 한국어도 human-in-the-loop 학습 가능하다.
<br><br>

- Chain of Thoughts & Program of Thoughts<br><br>
: : 프로그램 언어 형태로 된 Program-of-thought이 더 효과적이다?
<br><br>

- Modular LLM<br><br>
: : Hallucination을 최소화하기 위한 방법
: : 정확성이 요구되는 항목에 대해서는 외부 모듈을 활용하도록 하는 것
: : RAG(Retrieval-Augmented Generation) 방식 - 외부 검색엔진을 준비해놓고, 질문에 대한 답을 구글 검색결과와 함께 물어보면 hallucination 없이 올바른 답변을 내놓음
<br><br>

- FLAN<br><br>
: : 지금은 Instruction-tuning이 prompt-engineering의 핵심이다.
: : 특정 task를 instruction의 형태로 유도하여 수행토록 함
<br><br>

- 한계점(5)<br><br>
: : 생성 결과의 평가 방법이 마땅치 않다.
: : 정성적인 방법 뿐, 정량적인 방법이 아직 마땅치 않다. (->음악 생성에서도 마찬가지이지 않을까?)
<br><br>

- LLM Benchmark<br><br>
: : 1) MMLU - 수학, 과학, 상식, 인문 등 총 57개의 다양한 문제 유형을 다루는 데이터셋
: : 2) BIG-bench - 다양한 분야의 204개 task를 포함하는 평가 데이터셋
: : 3) HELM (Holistic Evaluation of Language Models) - 다양한 기준과 상황에서 LLM을 평가할 수 있는 데이터셋
: : 우리나라) OpenKo-LLM이라는 리더보드가 있음. 그런데 한국어 특화된 모델이 영어 모델보다 성능이 떨어지거나, 1/10 사이즈의 모델이 더 큰 모델보다 성능이 더 좋기도 함. 꼭 리더보드가 성능을 정확히 반영한다고 말할 수 있을까?
