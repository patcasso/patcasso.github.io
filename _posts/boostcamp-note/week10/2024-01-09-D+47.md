---
layout: single
title:  "Day 47 학습정리"
categories: boostcamp-note-week10
sidebar:
  nav: "docs"
---

24/01/09 (화) 학습 내용

<h1>KLUE</h1>

<h2>(11강) 최신 자연어처리 연구</h2>
1. BERT 이후의 다양한 LM<br><br>
: : <u>BERT의 문제점</u>
: : Token 사이의 관계 학습 불가능
: : Segment간 관계 학습 불가능 
: <br><b>1.1 XLNet</b><br><br>
: : Relative positional encoding 사용
: : 마스크 토큰을 없애고, Permutation language modeling 사용 (모든 순열대로 섞인 문장을 학습)
: <br><b>1.2 RoBERTa</b><br><br>
: : 학습 시간, batch size, training data 증가
: : Next sentence prediction 제거 (너무 쉬운 문제라 성능 하락)
: : Longer sentence 추가
: : Dynamic masking -> 같은 텍스트 데이터에 대해 마스킹을 10번 다르게 적용하여 학습
: <br><b>1.3 BART</b><br><br>
: : Transformer의 Encoder-Decoder 통합 LM
: : Token Masking, Sentence Permutation, Document Rotation, Token Deletion, Text Infilling 등 여러 task들을 통해 학습
: <br><b>1.4 T5</b><br><br>
: : Transformer Encoder-Decoder 통합 LM
: : Masking 기법 사용, 의미를 가진 여러 어절들을 동시에 마스킹함
: <br><b>1.5 Meena</b><br><br>
: : 대화만을 위한 LM
: : 26억개의 파라미터를 가진 end-to-end multi-turn 챗봇
: : 챗봇의 평가를 위한 새로운 Metric인 SSA(Sensibleness and Specificity Average)를 제시
: <br><b>1.6 Controllable LM</b><br><br>
: : <u>Plug and Play Language Model (PPLM)</u>
: : 다음에 등장할 단어 -> 확률 분포를 통해 선택
: : 내가 원하는 단어들의 확률이 최대가 되도록 이전 상태의 vector 수정
: : gradient가 업데이트 되는 것이 아니라, 이전에 만들어진 벡터값을 수정함으로써, 내가 가진 BOW의 단어들이 최대 확률을 가지도록 수정하는 것
: : 확률 분포를 사용하는 것이기 때문에, 중첩도 가능 (ex. 기쁨 + 놀람 + 게임)
: : 특정 카테고리에 대한 감정을 컨트롤해서 생성 가능
: : 확률 분포 조절을 통해 그라데이션 분노 가능
<br><br>
: : 특정 카테고리(ex. 마약)의 어휘를 아예 학습하지 않는다고 해당 부분에서 클린한 모델이 나오는 게 아니다. 모르면 오히려 이에 대해 긍정하거나 동의할 수 있으므로, 언어모델 자체는 언어를 잘 알도록 학습을 한 후, 필터 모델을 넣는 등으로 걸러낸다.
: : Music BoW 같은 특정 Bag-of-Words를 지정해주면 내가 원하는 방향으로 생성하도록 유도할 수 있다.
<br><br>

2. Multi-modal Language Model<br><br>
: : 과연 자언어 to 자연어로 충분할까?
: : 인간은 모든 감각을 이용해 세상을 학습한다.
<br><br>
: <br><b>2.1 할머니세포</b><br><br>
: : 1개의 단일 세포가 어떤 개념에 대해 반응하는 것
: : Halle Berry 뉴런 -> 유독 할리 베리의 모습이 나타나면 뇌세포가 반응하는 것
: : Halle Berry라는 글자에도 반응하고, 이 사람이 연기했던 캣 우먼의 이미지에도 반응을 하고, 초상화, 즉 그림에도 반응한다는 것.
: : 우리의 뇌가 하나의 객체를 인지할 때, 다양한 멀티모달 정보를 바탕으로 해당 객체를 인지한다는 것.
: : 우리의 뇌 역시 시각, 청각, 후각 등 여러 감각을 통해 세상을 이해하고 언어를 이해하는 것이다.
: : Multi-modal에 관한 연구는 반드시 필요하고, 계속해서 발전해 나갈 것이다.
: : 이미지에 대해 배운 내용도 활용하면 더 좋은 연구자가 될 수 있을 것이다.
: <br><b>2.2 LXMERT</b><br><br>
: : Cross-modal reasoning language model (Learning Cross-Modality Encoder Representations from Transformers)
: : 이미지 feature - 자연어 feature가 하나의 모델에 반영됨
: <br><b>2.3 ViLBERT</b><br><br>
: : BERT for vision-and-language
: : 앞에는 이미지 토큰에 대한 임베딩 벡터를 넣고, SEP 토큰 다음에는 자연어에 대한 벡터를 넣는다.
: : 이미지에 대한 토큰과, 자연어에 대한 토큰을 하나로 합쳐 CLS 토큰을 만들어낼 것이다.
: : 이를 통해 멀티 모달 정보를 통해 분류를 수행하는 task를 할 수 있게 된다.
: : MM-IMDB 데이터셋
: <br><b>2.4 Dall-e</b><br><br>
: : 자연어로부터 이미지를 생성해내는 모델
: : 아보카도 모양의 의자, 램프 등 신기한 이미지들 생성
<br><br>
: : 1. VQ-VAE를 통해 이미지의 차원 축소 학습
: : 앞의 256 토큰 동안은 텍스트 토큰이, 이후의 토큰은 이미지 토큰이 들어가도록 함
: : 음악 생성도 이런 식으로 학습해보면 어떨까? (지영)
<br><br>

- 마치며<br><br>
: : 최신 인공지능 기술들은 multi-modal 모델로 점점 발전하고 있음
: : 이런 발전들은, 범용 인공지능으로의 발전으로 점점 다가갈 것이다
: : 발전의 중심은 자연어 처리!
: : 특히, 한국어에 특화된 자연어 처리 연구를 다같이 하면서 한국어 특화된 범용 인공지능으로 다가갈 수 있는 연구를 함께 하면 좋을 것 같다.


<h2>(12강) Encoder-Decoder 구조 심화</h2>

- 1. Encoder-Decoder<br><br>
: : GPT3 이후의 Encoder-Decoder 모델에 대해 더 알아봅시다.
<br><br>
: <br><b>1.1 T0 (2022)</b><br><br>
: : FLAN과 마찬가지로, instruction을 prefix 형태로 정의하여 학습 -> 일반화 성능 더욱 향상
: : 데이터셋을 이용해 instruction을 만듦
: : 36명의 human contributor가 다양한 prompt template 생성
: <br><b>1.2 FLAN-T5 (2022)</b><br><br>
: : FLAN의 방법론과 CoT(Chain-of-Thought)를 결합하여 학습
: : 이러한 방법론들을, 우리의 LLM을 학습시킬 때 활용할 수 있다는 것을 알 수 있다.
: <br><b>1.3 모델의 구조별 특징</b><br><br>
: : Encoder-only 모델(BERT, RoBERTa 등)은 문맥을 이해하는 데 강점을 가지고 있다.
: : 문장 분류, Token tagging, 감정 분류 등에서 활용 가능
: : 특정 task를 위한 fine-tuning이 요구
: : 생성 task에는 부적합
<br><br>
: : Decoder-only 모델(GPT-3 등)은 생성 task에 적합하다.
: : But, 짧은 문맥을 이해하는데 어려움
: : Fine-tuning 없이도 특정 task 수행 가능
: : 생성을 컨트롤하기 어려움
<br><br>
: : Decoder-only 모델 (개체명 인식 task)
: : Encoder-only 모델은 상대적으로 의미모호성 문제를 잘 해결할 수 있다 (ex. 동음이의어 등)
<br><br>
: : Encoder-Decoder 모델
: : 문맥 이해 능력 + 생성 능력
: : Fine-tuning 없이도 다양한 task 적용 가능
: : 생성 task 적용 가능
: : Decoder-only 모델에 비해 용량이 2배
: : 병렬 처리 구현의 어려움
: : Encoder (context 압축) -> Decoder (context 압축 해제) 과정에서의 손실
<br><br>
: : 결국, 우리가 해결하려는 task에 맞게 모델을 잘 선택해서 학습해야 좋은 성능을 낼 수 있다!