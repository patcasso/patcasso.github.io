---
layout: single
title:  "Day 59 학습정리"
categories: boostcamp-note-week12
sidebar:
  nav: "docs"
---

24/01/25 (목) 학습 내용

<h1>Data Centric</h1>

<h2>(7강) Data-Centric NLP 응용 분야</h2>

1. Without Model Modification <br><br>
: <br><b>Subword Tokenization</b><br><br>
: : 주어진 말뭉치(corpus)를 토큰(token)으로 나누는 작업
: : e.g., 단어, 형태소, 서브워드, 문자
: : 주어진 말뭉치를 서브워드로 단위로 나누는 작업
: : OOV(Out-Of-Vocabulary), 희귀 단어, 신조어 등의 문제 해소 가능
: : vocab 사이즈를 무조건 늘릴 수는 없어서 나온 방법론
<br><br>
: : <u>BPE</u>
: : 가장 빈도수가 높은 유니그램 쌍을 하나의 유니그램으로 통합
: : 위 과정을 반복하여 딕셔너리 생성
<br><br>
: : 한국어는 교착어이므로 단어가 형태소(어간, 접사)로 구성됨
: : <u>형태소 기반 서브워드 토큰화</u>가 유리함
<br><br>
: <br><b>Data Augmentation</b><br><br>
: : 원본 데이터를 변환하거나 새로운 데이터를 추가하여 데이터의 수를 늘리는 기법
<br><br>
: : <u>Rule-Based Techniques</u>
<br><br>
: : Easy Data Augmentation(EDA)
: : SR (Synonym Replacement)
: : RI (Random Insertion)
: : RS (Random Swap)
: : RD (Random Deletion)
<br><br>
: : Unsupervised Data Augmentation(UDA)
<br><br>
: : Example Interpolation Techniques
: : MixUp을 적용하여 둘 이상의 실제 예시로부터 입력값과 레이블을 보간하는 기법
: : Mixed Sample Data Augmentation (MSDA)
<br><br>
: : <u>Model-Based Techniques</u>
: : Back-Translation
: : Fine-Tuning GPT for Paraphrasing
: : GPT-3 등 대규모 생성 모델을 미세조정하여 문장을 바꿔 쓰는 기법
<br><br>
: <br><b>Data Filtering</b><br><br>
: : Data Filtering vs. Data Cleaning
: : 데이터 필터링은 데이터 제거를 통해 실제 데이터 양이 줄어듦
: : 데이터 클리닝은 데이터 전처리와 비슷한 개념 (불용어 처리, Stemming, Lemmatization)
<br><br>
: : <u>병렬 말뭉치(Parallel Corpus)</u>
: : 두 개 언어 이상의 번역된 문서를 모은 말뭉치
: : Wikipedia, OPUS, AI Hub의 여러 병렬 말뭉치
<br><br>
: : <u>병렬 말뭉치 필터링 (Parallel Corpus Filtering)</u>
: : 언어 감지 필터 (문장쌍이 원하는 언어인지 확인 - 언어 자동 감지)
: : 수용 가능성 필터
: : 도메인 필터
<br><br>
: <br><b>Synthetic Data</b><br><br>
: : 실제 크롤링, 크라우드소싱 등으로 수집한 데이터가 아닌, 통계적, 전산학적 기법으로 생성한 데이터
: : GPT-3 등 생성 모델 기반 합성 데이터가 각광받고 있음
: : GPT-3에게 임의의 데이터에 대해 레이블링 하도록 하는 것
: : 앞으로 미래 AI 기술의 핵심이 될 것이다!
: <br><b>Training Strategies</b><br><br>
: : 커리큘럼 학습(Curriculum learning) - 쉬운 내용부터 어려운 내용으로 단계적으로 모델을 학습하는 ML 기법
<br><br>

2. Data Measurement<br><br>
: <br><b>Inter-Annotator Agreement (IAA)</b><br><br>
: : 2명 이상의 어노테이터가 생성한 레이블이 얼마나 일관성 있는지에 관한 지표
: : 데이터 품질과 관련 있음
: : 주요 Metric - Cohen's Kappa, Fleiss' Kappa, Krippendorff's Alpha
<br><br>

3. HCI (Human Computer Interaction)<br><br>
: : 좋은 데이터란? -> 일관성 있게 라벨링된 데이터, 중요한 케이스를 포함한 데이터, 예상치 못한 케이스까지 포함한 데이터, 적절한 사이즈의 데이터
: : Data Cascade - AI/ML 분야에서 데이터 품질의 중요성을 과소평가 하고 있음
: : 데이터 단계에서 잘못되면 나중에 나비효과처럼 안 좋은 효과가 난다는 것
: : 그래서 데이터를 만들 때 Human Factor에 대한 부분을 보려는 것이 HCI에 대한 연구임
<br><br>
: <br><b>What is Good Data</b><br><br>
: : 전처리, 클리닝, 라벨링 단계가 있었는가?
: : Raw Data를 별도로 저장했는가?
: : 전처리, 클리닝, 라벨링 단계에서 사용한 SW가 있다면 공개했는가?
<br><br>
: : <u>이외에 좋은 데이터를 위한 체크리스트</u>
: : Metadata가 얼마나 informative 한가?
: : 작업자에게 정당한 보상을 하되, 불필요한 비용이 지불되지 않는 데이터인가?
: : Versioning 체계가 잘 이루어졌는가?
: : 데이터 저장 폴더 구조가 직관적이고 clean 한가?
: : Model based Data-Centric AI가 중요하다
: : Human-in-the-loop Cycle이 중요하다.

<h2>(8강) Recent Work in Data-Centric NLP</h2>
1. Recent Work in Data Augmentation<br><br>
: : Iterative Back-Translation -> Re-Back-Translation
: : Back Translation을 여러 차례 반복해서 하는 것
: : EDA는 BERT, RoBERTa, XLL 등 대규모 사전학습 모델에서는 더 이상의 효과가 없다는 것을 증명한 논문도 있음
: : ChatGPT를 이용한 데이터 증강 시스템 논문도 있음
<br><br>

2. Recent Work in Data Filtering<br><br>
: : 병렬 말뭉치 필터링 (parellel corpus filtering) - Rule Based, Statistic Based, NMT Based
: : 병렬 말뭉치 필터링에서 Rule-based로 80%는 걸러진다고 한다. Rule-based도 무시하지 말고 중요시해야!
: : Low Resource 언어 연구에 대한 중요성도 부각되고 있다
<br><br>

3. Recent Work in Synthetic Data<br><br>
: : PromptBase - 프롬프트 사고 파는 웹 플랫폼
: : 언어 모델에게 입력으로 주는 템플릿 글을 Prompt라고 함
: : "Pre-train, Prompt, and Predict" 논문
: : 최근 Prompt 기반 연구가 급증하고 있음
: : "Prompt Programming for LLMs"
: : Metaprompt = Prompt를 생성하기 위한 Prompt
: : BTS : Back TranScription for Speech-to-text Post-Processor using Text-to-Speech-to-Text
: : 음성인식  후처리기 연구를 위한 합성 데이터
: : 기존에는 후처리를 모두 Rule-based로 했는데, 개별 연구를 모두 하기가 비효율적이다.
: : 기계번역의 Seq2Seq 모델처럼, 학습하는 패러다임이 등장함
: : 근데 이걸 전사자가 직접 다 하게 되면 비용이 많기 때문에, 네이버 뉴스 데이터를 활용하면 좋겠다고 생각함.
: : 해당 문장들을 TTS로 바꿔서, 이 TTS를 다시 STT로 돌린다. 
: : 그럼 이런 음성인식된 STT 결과를 입력으로 쓰고, 원래 문장을 다시 라벨로 쓰면 병렬 말뭉치가 구성되는 것이다.
<br><br>

4. Recent Work in Data Measurement<br><br>
: : Measuring Annotator Agreement Generally across Complex Structured, Multi-object, and
Free-text Annotation Tasks
: : Krippendorff's Alpha는 복잡한 태스크에는 적용이 어려움
<br><br>
: : "Everyone's Voice Matters: Quantifying Annotation Disagreement Using Demographic Information"
: : Data Annotator의 인구통계학적 정보를 함께 입력으로 주고, 일치 정도를 예측하게 하는 것
: <br><b>Cross Lingual Transfer Learning</b><br><br>
: : 한국어 BERT를 만든다는 것이 쉽지 않음
: : 인코딩 Layer는 학습이 너무 잘 weight일 것이므로, 
: : 영어 BERT의 Input Layer는 Initialize하고, 인코딩 Layer는 Freeze하되, Adapter Layer 를 추가
: : 나중에는 인코딩 Layer도 unfreeze하고 Post training 진행
