---
layout: single
title:  "D+3 학습일지"
categories: boostcamp-note-week1
sidebar:
  nav: "docs"
---

# 23/11/08 (수) 학습 내용

<h2>AI Math</h2>

- <b>(5강) 딥러닝 학습방법 이해하기</b><br><br>
: : 데이터를 모아놓은 행렬 X와, X를 다른 차원으로 보내주는 가중치 행렬 W를 통해 선형 모델을 표현할 수 있다.
: : 각 행들이 모두 같은 값을 가진 b행렬
: : 출력벡터의 차원은 d에서 p로 바뀌게 된다. (O<sub>(n x p)</sub>)
: : 모델에서 그리는 화살표는 선형결합에서의 가중치 W<sub>ij</sub>들로 이해한다
: <br><b>소프트맥스 연산</b><br><br>
: : 분류 문제에 씌우는 함수
: : 지수함수를 통해 계산한다.
: : 추론만 할 때는 One hot vector만 있으면 된다. (학습을 하는 경우에 softmax 사용)
: <br><b>신경망 수식으로 분해해보기</b><br><br>
: : 신경망은 선형모델과 활성함수(activation function)를 합성한 함수이다.
: : 활성함수는 비선형 함수로서, 각각의 원소에 적용된다. (소프트맥스와는 달리, 모든 주소값이 아닌 해당 주소값만 갖고 계산한다.)
: : 이렇게 변형시킨 벡터를 잠재벡터(Hidden vector)라 부르고, 이를 뉴런 이라고 부른다.
: : 뉴런으로 이루어진 모델을 바로 Neural Network라고 하는 것.
: : sigmoid나 tanh는 그렇다 해도,
: : ReLU가 왜 비선형일까? -> 전형적인 비선형함수로, 활성함수로서의 좋은 특징을 많이 가지고 있어 오늘날 많이 쓰이고 있다.
: : 2층신경망 : 입력 -> 선형변환(W<sup>(1)</sup>) -> 활성함수 -> 선형변환(W<sup>(2)</sup>)
: : 이를 반복적으로 적용하면 Multi-layer Perceptron이라 하고, 이것이 오늘날 딥러닝에서 사용하는 가장 기본적인 모델이다.
: : MLP의 패러미터는 L개의 가중치 행렬 W<sup>(L)</sup>, ... , W<sup>(1)</sup>과 가중치 행렬 b<sup>(L)</sup>, ... , B<sup>(1)</sup>로 이루어져있다.
: : L개의 순차적 신경망 계산을 순전파(forward propagation)이라고 한다. (학습이 아니라 출력물을 내뱉는 과정)
: : 층이 깊어질수록 적은 패러미터(적은 뉴런 수)로 복잡한 함수를 근사할 수 있다. (그렇다고 해서 최적화가 더 쉽다고 할 수는 없다.)
: <br><b>딥러닝 학습원리: 역전파 알고리즘</b><br><br>
: : 딥러닝은 Backpropagation 알고리즘을 이용해 각 층에 사용된 패러미터{W<sup>(l)</sup>, b<sup>(l)</sup>}<sub>l=1</sub><sup>L</sup>를 학습한다.
: : 한 번에 계산은 안되고, 역순으로 순차적으로 미분을 계산하게 된다.
: : 합성함수의 미분법인 연쇄법칙을 통해 층별로 미분값을 전달하게 된다.

- <b>(6강) 확률론 맛보기</b><br><br>
: : 왜 필요한가? - 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있기 때문
: : 손실함수(loss function)은 데이터 공간을 통계적으로 해석해서 유도하게 된다.
: : 회귀 분석에서 손실함수로 사용되는 L2-노름은 <u>예측오차의 분산을 가장 최소화하는 방향으로 학습</u>하도록 유도
: : 분류 문제에서 사용하는 교차엔트로피(cross-entropy)는 <u>모델 예측의 불확실성을 최소화하는 방향으로 학습</u>하도록 유도
: : <u>분산 및 불확실성을 최소화</u>하기 위해서는 측정 방법을 알아야 한다.
: <br><b>확률분포는 데이터의 초상화</b><br><br>
: : 데이터공간을 X x Y 라 표기하고, D는 데이터공간에서 데이터를 추출하는 분포
: : 데이터는 확률변수로 (x, y) ~ D라 표기
: : 확률변수는 확률분포 D에 따라 이산형(discrete)와 연속형(continuous) 확률변수로 구분하게 됩니다. (데이터 공간에 따라 결정되는 것이 아니다. 실수 데이터 공간에서 정의되었다고 해도 무조건 연속확률변수라고 할 수는 없다.)<br><br>
: : 이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링
: : P(X ∈ A) = Σ<sub>x∈A</sub>P(X = x)
: : 이러한 함수를 확률질량함수라고 함<br><br>
: : 연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통해 모델링한다.
: : P(X ∈ A) = ∫<sub>A</sub>P(**x**)d**x**
: : 이러한 함수를 밀도함수라고 함(누적확률분포의 변화율을 모델링)<br><br>
: : 결합분포 P(**x**, y)는 D를 모델링한다.
: : P(**x**)는 입력 **x**에 대한 주변확률분포로 y에 대한 정보를 주진 않는다.
: <br><b>조건부확률과 기계학습</b><br><br>
: : 조건부확률분포 P(**x**\|y)는 데이터 공간에서 입력 **x**와 출력 y 사이의 관계를 모델링한다.
: : 우리가 관심을 갖고 있는 관점에 따라 '데이터의 초상화'를 그릴 수 있고, 따라서 데이터를 해석하는 데 중요한 도구로 사용될 수 있다.<br><br>
: : 조건부확률 P(y\|**x**)는 입력변수 **x**에 대해 정답이 y일 확률
: : 분류 문제에서 softmax(**W**ϕ + **b**)은 데이터 **x**로부터 추출된 특징 패턴 ϕ(**x**)과 가중치행렬 **W**을 통해 조건부확률 P(y\|**x**)을 계산한다.
: : 회귀 문제의 경우 분류와 달리 연속확률변수를 보통 다루기 때문에, 확률로 해석하긴 어렵고 밀도함수로 해석해야한다.
: : 이 경우, 조건부기대값 𝔼[y\|**x**]을 추정한다.
: : 조건부기대값은 𝔼‖⃦y-f(x)‖<sub>2</sub>을 최소화하는 함수 f(x)와 일치한다.
: : 예측의 오차의 분산을 최소화하는 적절한 통계치로 사용 가능하다.
: <br><b>기대값이 뭔가요?</b><br><br>
: : 확률분포가 주어지면 데이터를 분석하는 데 사용 가능한 여러 종류의 <u>통계적 범함수(statistical functional)</u>를 계산할 수 있다.
: : <u>기대값(expectation)은 데이터를 대표하는 통계량</u>이면서 동시에 확률분포를 통해 다른 통계적 범함수를 계산하는데 사용된다.
: : 단순히 평균만 구하는 것이 아니라, 목적으로 하는 주어진 함수가 있을 때, 해당 함수의 기대값을 계산하는 것을 통해 데이터를 해석할 때 여러 방면으로 사용할 수 있다.<br><br>
: : 연속확률분포의 경우엔 적분을 사용
: : 이산확률분포의 경우엔 급수를 사용
: : <u>분산, 첨도, 공분산</u> 등 여러 통계랑을 계산할 수 있다.
: : 𝕍(x) = 𝔼<sub>x~P(x)</sub>[(x - 𝔼[x])<sup>2</sup>]
: : Skewness(x) = 𝔼\[(x - 𝔼\[x] / 𝕍(x)<sup>1/2</sup>)
<sup>3</sup>]
: : Cov(**x**<sub>1</sub>, **x**<sub>2</sub>) = 𝔼<sub>x1,x2~<i>P</i>(x1,x2)</sub>\[(**x**<sub>1</sub>- 𝔼[**x**<sub>1</sub>])(**x**<sub>2</sub> -  𝔼[**x**<sub>2</sub>])]<br><br>
: : 딥러닝은 다층신경망을 사용하여 데이터로부터 특징패턴 ϕ을 추출한다.
: <br><b>몬테카를로 샘플링</b><br><br>
: : 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 대부분
: : 확률분포를 모를 때 <u>데이터를 이용하여 기대값을 계산하려면 몬테카를로(Monte Carlo) 샘플링 방법</u>을 사용해야 한다.
: : 몬테카를로는 이산형이든 연속형이든 상관없이 한다.
: : 몬테카를로 샘플링은 독립추출만 보장된다면 <u>대수의 법칙(law of large number)</u>에 의해 수렴성을 보장한다.
: - 몬테카를로 예제: 적분 계산하기<br><br>
함수 f(x) = e<sup>-x<sup>2</sup></sup>의 [-1,1]상에서 적분값을 어떻게 구할까?<br>
<i><u>균등분포란 무엇?</u></i><br><br>

- <b>(7강) 통계학 맛보기</b><br><br>
: <b>모수가 뭐예요?</b><br><br>
: : 데이터가 특정 확률분포를 따른다고 선험적으로(a priori) 가정한 후 그 분포를 결정하는 모수(parameter)를 추정하는 방법을 <u>모수적(parametric) 방법론</u>이라고 한다.
: : 특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌면 <u>비모수(nonparametric) 방법론</u>이라 부른다. (기계학습의 많은 방법론은 비모수 방법론에 속한다)
: <br><b>확률분포 가정하기: 예제</b><br><br>
: : 데이터가 2개의 값(0 / 1)만 가지는 경우 -> 베르누이분포
: : 데이터가 n개의 이산적인 값을 가지면 -> 카테고리분포
: : 데이터가 \[0,1] 사이에서 값을 가지는 경우 -> 베타분포
: : 데이터가 0 이상의 값을 가지는 경우 -> 감마분포, 로그정규분포 등
: : 데이터가 ℝ 전체에서 값을 가지는 경우 -> 정규분포, 라플라스분포 등<br>
: : 기계적으로 확률분포 가정 (x) (데이터 생성 원리 먼저 고려하기)
: <br><b>데이터로 모수를 추정해보자!</b><br><br>
: : 데이터의 확률분포를 가정하면 모수 추정 가능
: : 정규분포의 모수는 평균 μ(뮤)와 분산 𝜎<sup>2</sup>(sigma)로, 이를 추정하는 통계랑은,
: : 표본평균 -> 주어진 데이터들의 산술평균 (원래 데이터 평균과 기대값 평균이 일치)
: : 표본분산(S<sup>2</sup>) -> 주어진 데이터에서 표본평균을 뺀 후 제곱을 하고, 이에 산술평균을 취하는데, 1/N이 아니라 1/(N-1)로 표본분산을 정의한다.
: : 이렇게 하면 표본분산의 기대값이 원래 모집단의 분산 𝜎<sup>2</sup>과 일치하게 되기 때문이다.
: : 통계량의 확률분포를 표집분포(sampling distribution)라 부르며, 특히 표본평균과 표집분포는 N이 커질수록 정규분포 𝓝(μ, 𝜎<sup>2</sup>/𝘕)를 따른다.
: : 이를 중심극한정리(Central Limit Theorem)이라 부르며, 모집단의 분포가 정규분포를 따르지 않아도 표본평균의 포집분포는 정규분포를 따른다는 사실을 기억하자.
: <br><b>최대가능도 추정법 (심화 과제 2 관련)</b><br><br>
: : 표본평균, 표본분산은 중요한 통계랑이지만, 확률분포마다 사용 모수가 다르므로 적절한 통계량이 달라진다.
: : 이론적으로 가장 가능성 높은 모수를 추정하는 방법 중 하나는 <u>최대가능도 추정법(maximum likelihood estimation, MLE)</u>이다.
: : 가능도 함수는 L(𝜃;**x**)로 정의된다. (𝜃 : 모수)
: : 가능도함수는 주어진 데이터 x에 대해 모수 𝜃를 변수로 둔 함수
: : 즉, 데이터가 주어져있는 상황에서 𝜃를 변형시킴에 따라 값이 바뀌는 함수로 이해하면 된다.
: : 가능도함수는 모수 𝜃를 따르는 분포가 데이터 **x**를 관찰할 가능성 (확률로 해석 x. 𝜃에 대해 적분했을때 1이 되거나 하는 것이 아님)
: : 𝜃<sub>MLE</sub> = argmax<sub>𝜃</sub> L(𝜃;**x**) = argmax<sub>𝜃</sub> P(**x**\|𝜃)
: : 데이터 집합 **X**의 각 행벡터가 독립적으로 추출되었을 경우 로그가능도를 최적화한다.
: : L(𝜃;**x**) = <sub>i=1</sub>∏<sub>n</sub>P(**x**<sub>i</sub>\|𝜃) => log L(𝜃;**x**) = <sub>i=1</sub>∑<sub>n</sub>P(**x**<sub>i</sub>\|𝜃)
: <br><b>왜 로그가능도를 사용하나요?</b><br><br>
: : 데이터의 숫자가 수억 단위가 된다면, 컴퓨터의 정확도로는 가능도를 계산하는 것이 불가능하다.
: : 가령 0~1 사이의 확률값을 수억 번 곱해준다면, 수학적으로는 낮은 자리수까지 계산 가능하지만, 컴퓨터는 그렇게 낮은 자리수까지는 연산 오차때문에 계산이 불가능하다.
: : 로그가능도의 덧셈으로 바꾸면, 컴퓨터로도 연산이 가능해진다.
: : 연산량이 O(n<sup>2</sup>) -> O(n) 으로 줄어든다.
: : 대개의 손실함수의 경우 경사하강법을 사용하므로, <u>음의 로그가능도(negative log-likelihood)를 최적화</u>하게 된다.
: <br><b>최대가능도 추정법 예제: 정규분포</b><br><br>
: : 정규분포를 따르는 확률변수 X로부터 독립적인 표본 {x1, .. ,xn} 을 얻었을 때, 최대가능도 추정법을 이용해 모수를 추정하면?
: : Likelihood 함수를 최적화하는 𝜃를 찾는 것
: : 평균, 분산 두 파라미터로 정규분포를 표현하므로 argmax P(X\|μ, 𝝈<sup>2</sup>으로 표현 가능)
: : 정규분포의 확률밀도함수에 로그를 씌운 것으로 계산해 볼 수 있는데, 
: : <a href="https://m.blog.naver.com/iotsensor/222181186685">정규분포(normal distribution, Gaussian distribution)의 확률밀도함수란?</a>
: : 이는 두 가지 항의 곱셈으로 이루어져있는데, 하나는 분산(𝜎)만 들어있는 항과 지수함수 텀이 있는 항으로 이루어졌다.
: : 이 두 항에 로그를 씌움으로써 덧셈으로 쪼갤 수 있다.
: : 여기서 모수(평균과 분산)에 대해 수식을 미분하여 최적화 해볼수가 있는데,
: (<span style="color:red">평균(μ)에 대해 미분하는 식은 이해가 되는데 분산(𝜎)에 대해 미분하는 식이 잘 이해가 안 된다. 확인하고 넘어가기!</span>)
: : 미분을 사용해 최대가능도를 구하고 싶으면, 미분을 했을 때 각각의 파라미터에 대해 0이 되는 점을 찾아주면 되는데, 0 이 되는 μ와 𝜎를 구하게 되면 로그 가능도를 최대화해주는 모수를 찾게 된다.
: : 이 값들을 구했을 때, 
: : μ에 대한 최대 가능도 추정법으로 계산한 모수는 x<sub>i</sub> 데이터들이 주어져있는 상황에서의 산술평균으로 도출되고,
: : 𝜎에 대한 최대가능도 추정법으로 계산한 모수는 (x<sub>i</sub>-μ)<sup>2</sup>들을 산술평균으로 계산한 값이다.
: : 분모가 n-1이 아니라 1/n인 것을 알 수 있는데, MLE는 불편추정량을 보장하진 않는다.
: <br><b>최대가능도 추정법 예제: 카테고리 분포 (이산확률분포에 해당)</b><br><br>
: : 베르누이 분포의 다차원(d차원)으로 확장한 개념으로 보면 됨.
: : d개의 각 차원에서 하나의 값을 선택하게 되는 확률 변수.
: : 정규분포에서 사용된 모수는 평균과 분산이라는 통계량이었지만, 카테고리 분포에서의 모수의 p1 ~ pd는, 1에서 d차원까지 각각의 차원에서 값이 1 또는 0이 될 확률을 의미하는 모수이다.
: : p1 ~ pd까지의 모수는 다 더했을 때 1이 되어야 하는 성질이 있다.
: (<sub>k=1</sub>Σ<sub>d</sub> p<sub>k</sub> = 1)
: : 카테고리 분포의 Likelihood 함수는 아래와 같은 방식으로, 모수 p<sub>k</sub>에 해당하는 값에 x<sub>i,k</sub>, 즉, 주어진 데이터 x<sub>i</sub>의 7번째 차원에 해당하는 값을 승수를 취해주는 형태로 계산해주게 된다.<br><br>
: : 𝜃<sub>MLE</sub> = argmax<sub>p<sub>1</sub>,...,p<sub>d</sub></sub> logP(**x**<sub>i</sub> \| 𝜃) = argmax<sub>p<sub>1</sub>,...,p<sub>d</sub></sub> 
log(<sub>i=1</sub>∏<sub>n</sub> <sub>k=1</sub>∏<sub>d</sub> p<sub>k</sub><sup>x<sub>i,k</sub></sup>)<br><br>
: : 만약 x<sub>i,k</sub>가 0이 되면, p<sub>k</sub><sup>x<sub>i,k</sub></sup>은 p<sub>k</sub>의 0승이 되니까 그냥 1이 될 것이고, 만약 p<sub>k</sub><sup>x<sub>i,k</sub></sup>에서 x<sub>i,k</sub>가 1이면, 이 값은 p<sub>k</sub> 라는 값을 가지게 되기 때문에,
: : 조심해야 할 것은,  x<sub>i,k</sub>가 0과 1 두 가지 값만을 가지기 때문에 Likelihood 함수를 계산할 때, p<sub>k</sub>의 값이 1 혹은 p<sub>k</sub>로 사용되는 형태로 함수가 정의되게 되고, 이를 이용해 최대우도(최대가능도)를 추정하게 되는 것이다.
: : 이 Likelihood 함수에 로그를 취하게 되면, Σ 으로 표현할 수 있게 된다.
: : 주의할 점은, 우리가 추정한 모수 p<sub>1</sub> ~ p<sub>d</sub>는 주어진 카테고리 분포의 질량함수에 해당하는 모수이기 때문에, 다 더했을 때 반드시 1이 되어야 한다는 제약식이 있고,
: : 제약식이 주어진 상황에서 가능도를 최적화 시켜야 한다는 사실을 기억할 것!
: : 로그를 씌우면 곱셈을 덧셈으로 바꿀 수 있다. (pdf 수식 참조)
: : x<sub>i,k</sub>는 전부 0 또는 1이므로, 주어진 각 데이터들에 대해 k값이 1인 개수를 세는 n<sub>k</sub>로 대체해서 표현할 수 있게 된다.
: : 따라서 주어진 가능도 함수에 로그를 씌운 것을 다음과 같이 표현할 수 있다.<br><br>
: : <sub>k=1</sub>Σ<sub>d</sub> n<sub>k</sub> · log p<sub>k</sub>
: : 제약식을 만족하면서 목적식을 최대화하는 것이 우리가 구하는 MLE이다.
: : 목적식이 주어진 상황이면, 라그랑주 승수에 해당하는 λ를 곱해준 식을 목적식에 더해서 새로운 목적식을 만들어줄 수 있다.
: : 주어진 라그랑주 목적식을 각각의 모수 p, k 그리고 λ에 대해서도 미분을 해준다.
: : (식 전개 과정 PDF 참조)
: : 따라서 카테고리 분포에서의 MLE는 각각의 차원에 해당하는 데이터의 카운터 수, 즉 경우의 수를 세어서 비율을 구하는 것으로 MLE를 달성하는 모수를 추정하는 것이 가능하다 라는 직관적인 결론을 얻을 수 있게 된다.
: <br><b>딥러닝에서 최대가능도 추정법</b><br><br>
: : MLP의 가중치를 𝜃 = (W<sup>(1)</sup>, ... ,W<sup>(L)</sup>)라 표기했을 때, 소프트맥스 확률 벡터가 카테고리 분포의 모수를 모델링하는데 사용할 수 있다.
: : 원핫벡터인 정답레이블 **y**를 관찰데이터로 이용해, 확률분포인 소프트맥스 벡터의 로그가능도를 최적할 수 있다.
: <br><b>확률분포의 거리를 구해보자</b><br><br>
: : 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와, 데이터에서 관찰되는 확률분포의 거리를 통해 유도한다.
: : 데이터공간에 두 개의 확률분포 P(**x**), Q(**x**)가 있을 경우, 두 확률분포 사이의 거리를 계산할 때, 이 강의에서는 쿨백-라이블러 발산 함수를 설명한다.
: <br><b>쿨백-라이블러 발산</b><br><br>
: : 쿨백 라이블러 발산 함수는 두 개의 엔트로피 함수로 표현할 수 있다. (크로스 엔트로피, 엔트로피)
: : 분류 문제에서 정답레이블을 P, 모델 예측을 Q라 두면 <u>최대가능도 추정법은 두 확률분포 사이의 거리, 즉 쿨백-라이블러 발산을 최소화</u>하는 것과 같다.


<h2>과제 수행 내용</h2>
- <b>심화 과제1</b><br><br>
: : <a href="https://colab.research.google.com/drive/1u5pMFAVcm3Weu-Lax4g5W1NVkmEBCG1U#scrollTo=_rPsh3Gm07LV">심화 과제 1 링크</a>
: : Google Colab내에 주석 형태로 이해가 잘 가지 않는 부분들을 comment 해놓았고, peer session에서 팀원들과 토론을 통해 해결할 예정

- **심화 과제 2**<br><br>
: : <a href="https://colab.research.google.com/drive/1PmGCTW4C4bbjMWYTAcJclsYVfa3pcYLk#scrollTo=4tNgRvH3FQCo">심화 과제 2 링크</a>
: : 7강 강의 참고하여 풀어볼 것



<h2>Peer Session</h2>

- 토론 내용
: : AI Math - 5~6강 및 심화과제 1 토론
: : 금요일 멘토링 관련 의논
: : 노션 팀 페이지 활용 방안 공유
