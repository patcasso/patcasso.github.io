---
layout: single
title:  "D+3 학습일지"
categories: boostcamp-note
sidebar:
  nav: "docs"
---

# 23/11/07 학습 내용

<h2>AI Math</h2>

- <b>(5강) 딥러닝 학습방법 이해하기</b><br><br>
: : 데이터를 모아놓은 행렬 X와, X를 다른 차원으로 보내주는 가중치 행렬 W를 통해 선형 모델을 표현할 수 있다.
: : 각 행들이 모두 같은 값을 가진 b행렬
: : 출력벡터의 차원은 d에서 p로 바뀌게 된다. (O<sub>(n x p)</sub>)
: : 모델에서 그리는 화살표는 선형결합에서의 가중치 W<sub>ij</sub>들로 이해한다
: <br><b>소프트맥스 연산</b><br><br>
: : 분류 문제에 씌우는 함수
: : 지수함수를 통해 계산한다.
: : 추론만 할 때는 One hot vector만 있으면 된다. (학습을 하는 경우에 softmax 사용)
: <br><b>신경망 수식으로 분해해보기</b><br><br>
: : 신경망은 선형모델과 활성함수(activation function)를 합성한 함수이다.
: : 활성함수는 비선형 함수로서, 각각의 원소에 적용된다. (소프트맥스와는 달리, 모든 주소값이 아닌 해당 주소값만 갖고 계산한다.)
: : 이렇게 변형시킨 벡터를 잠재벡터(Hidden vector)라 부르고, 이를 뉴런 이라고 부른다.
: : 뉴런으로 이루어진 모델을 바로 Neural Network라고 하는 것.
: : sigmoid나 tanh는 그렇다 해도,
: : ReLU가 왜 비선형일까? -> 전형적인 비선형함수로, 활성함수로서의 좋은 특징을 많이 가지고 있어 오늘날 많이 쓰이고 있다.
: : 2층신경망 : 입력 -> 선형변환(W<sup>(1)</sup>) -> 활성함수 -> 선형변환(W<sup>(2)</sup>)
: : 이를 반복적으로 적용하면 Multi-layer Perceptron이라 하고, 이것이 오늘날 딥러닝에서 사용하는 가장 기본적인 모델이다.
: : MLP의 패러미터는 L개의 가중치 행렬 W<sup>(L)</sup>, ... , W<sup>(1)</sup>과 가중치 행렬 b<sup>(L)</sup>, ... , B<sup>(1)</sup>로 이루어져있다.
: : L개의 순차적 신경망 계산을 순전파(forward propagation)이라고 한다. (학습이 아니라 출력물을 내뱉는 과정)
: : 층이 깊어질수록 적은 패러미터(적은 뉴런 수)로 복잡한 함수를 근사할 수 있다. (그렇다고 해서 최적화가 더 쉽다고 할 수는 없다.)
: <br><b>딥러닝 학습원리: 역전파 알고리즘</b><br><br>
: : 딥러닝은 Backpropagation 알고리즘을 이용해 각 층에 사용된 패러미터{W<sup>(l)</sup>, b<sup>(l)</sup>}<sub>l=1</sub><sup>L</sup>를 학습한다.
: : 한 번에 계산은 안되고, 역순으로 순차적으로 미분을 계산하게 된다.
: : 합성함수의 미분법인 연쇄법칙을 통해 층별로 미분값을 전달하게 된다.

- <b>(6강) 확률론 맛보기</b><br><br>
: : 왜 필요한가? - 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있기 때문
: : 손실함수(loss function)은 데이터 공간을 통계적으로 해석해서 유도하게 된다.
: : 회귀 분석에서 손실함수로 사용되는 L2-노름은 <u>예측오차의 분산을 가장 최소화하는 방향으로 학습</u>하도록 유도
: : 분류 문제에서 사용하는 교차엔트로피(cross-entropy)는 <u>모델 예측의 불확실성을 최소화하는 방향으로 학습</u>하도록 유도
: : <u>분산 및 불확실성을 최소화</u>하기 위해서는 측정 방법을 알아야 한다.
: <br><b>확률분포는 데이터의 초상화</b><br><br>
: : 데이터공간을 X x Y 라 표기하고, D는 데이터공간에서 데이터를 추출하는 분포
: : 데이터는 확률변수로 (x, y) ~ D라 표기
: : 확률변수는 확률분포 D에 따라 이산형(discrete)와 연속형(continuous) 확률변수로 구분하게 됩니다. (데이터 공간에 따라 결정되는 것이 아니다. 실수 데이터 공간에서 정의되었다고 해도 무조건 연속확률변수라고 할 수는 없다.)<br><br>
: : 이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링
: : P(X ∈ A) = Σ<sub>x∈A</sub>P(X = x)
: : 이러한 함수를 확률질량함수라고 함<br><br>
: : 연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통해 모델링한다.
: : P(X ∈ A) = ∫<sub>A</sub>P(**x**)d**x**
: : 이러한 함수를 밀도함수라고 함(누적확률분포의 변화율을 모델링)<br><br>
: : 결합분포 P(**x**, y)는 D를 모델링한다.
: : P(**x**)는 입력 **x**에 대한 주변확률분포로 y에 대한 정보를 주진 않는다.
: <br><b>조건부확률과 기계학습</b><br><br>
: : 조건부확률분포 P(**x**\|y)는 데이터 공간에서 입력 **x**와 출력 y 사이의 관계를 모델링한다.
: : 우리가 관심을 갖고 있는 관점에 따라 '데이터의 초상화'를 그릴 수 있고, 따라서 데이터를 해석하는 데 중요한 도구로 사용될 수 있다.<br><br>
: : 조건부확률 P(y\|**x**)는 입력변수 **x**에 대해 정답이 y일 확률
: : 분류 문제에서 softmax(**W**ϕ + **b**)은 데이터 **x**로부터 추출된 특징 패턴 ϕ(**x**)과 가중치행렬 **W**을 통해 조건부확률 P(y\|**x**)을 계산한다.
: : 회귀 문제의 경우 분류와 달리 연속확률변수를 보통 다루기 때문에, 확률로 해석하긴 어렵고 밀도함수로 해석해야한다.
: : 이 경우, 조건부기대값 𝔼[y\|**x**]을 추정한다.
: : 조건부기대값은 𝔼‖⃦y-f(x)‖<sub>2</sub>을 최소화하는 함수 f(x)와 일치한다.
: : 예측의 오차의 분산을 최소화하는 적절한 통계치로 사용 가능하다.
: <br><b>기대값이 뭔가요?</b><br><br>
: : 확률분포가 주어지면 데이터를 분석하는 데 사용 가능한 여러 종류의 <u>통계적 범함수(statistical functional)</u>를 계산할 수 있다.
: : <u>기대값(expectation)은 데이터를 대표하는 통계량</u>이면서 동시에 확률분포를 통해 다른 통계적 범함수를 계산하는데 사용된다.
: : 단순히 평균만 구하는 것이 아니라, 목적으로 하는 주어진 함수가 있을 때, 해당 함수의 기대값을 계산하는 것을 통해 데이터를 해석할 때 여러 방면으로 사용할 수 있다.<br><br>
: : 연속확률분포의 경우엔 적분을 사용
: : 이산확률분포의 경우엔 급수를 사용
: : <u>분산, 첨도, 공분산</u> 등 여러 통계랑을 계산할 수 있다.
: : 𝕍(x) = 𝔼<sub>x~P(x)</sub>[(x - 𝔼[x])<sup>2</sup>]
: : Skewness(x) = 𝔼\[(x - 𝔼\[x] / 𝕍(x)<sup>1/2</sup>)
<sup>3</sup>]
: : Cov(**x**<sub>1</sub>, **x**<sub>2</sub>) = 𝔼<sub>x1,x2~<i>P</i>(x1,x2)</sub>\[(**x**<sub>1</sub>- 𝔼[**x**<sub>1</sub>])(**x**<sub>2</sub> -  𝔼[**x**<sub>2</sub>])]<br><br>
: : 딥러닝은 다층신경망을 사용하여 데이터로부터 특징패턴 ϕ을 추출한다.
: <br><b>몬테카를로 샘플링</b><br><br>
: : 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 대부분
: : 확률분포를 모를 때 <u>데이터를 이용하여 기대값을 계산하려면 몬테카를로(Monte Carlo) 샘플링 방법</u>을 사용해야 한다.
: : 몬테카를로는 이산형이든 연속형이든 상관없이 한다.
: : 몬테카를로 샘플링은 독립추출만 보장된다면 <u>대수의 법칙(law of large number)</u>에 의해 수렴성을 보장한다.
: - 몬테카를로 예제: 적분 계산하기<br><br>
함수 f(x) = e<sup>-x<sup>2</sup></sup>의 [-1,1]상에서 적분값을 어떻게 구할까?<br>
<i><u>균등분포란 무엇?</u></i><br><br>

- <b>(7강) 통계학 맛보기</b><br><br>
: <b>모수가 뭐예요?</b><br><br>
: : 데이터가 특정 확률분포를 따른다고 선험적으로(a priori) 가정한 후 그 분포를 결정하는 모수(parameter)를 추정하는 방법을 <u>모수적(parametric) 방법론</u>이라고 한다.
: : 특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌면 <u>비모수(nonparametric) 방법론</u>이라 부른다. (기계학습의 많은 방법론은 비모수 방법론에 속한다)
: <br><b>확률분포 가정하기: 예제</b><br><br>
: : 데이터가 2개의 값(0 / 1)만 가지는 경우 -> 베르누이분포
: : 데이터가 n개의 이산적인 값을 가지면 -> 카테고리분포
: : 데이터가 \[0,1] 사이에서 값을 가지는 경우 -> 베타분포
: : 데이터가 0 이상의 값을 가지는 경우 -> 감마분포, 로그정규분포 등
: : 데이터가 ℝ 전체에서 값을 가지는 경우 -> 정규분포, 라플라스분포 등<br>
: : 기계적으로 확률분포 가정 (x) (데이터 생성 원리 먼저 고려하기)
: <br><b>데이터로 모수를 추정해보자!</b><br><br>
: : 데이터의 확률분포를 가정하면 모수 추정 가능
: : 정규분포의 모수는 평균 μ(뮤)와 분산 𝜎<sup>2</sup>(sigma)로, 이를 추정하는 통계랑은,
: : 표본평균 -> 주어진 데이터들의 산술평균 (원래 데이터 평균과 기대값 평균이 일치)
: : 표본분산(S<sup>2</sup>) -> 주어진 데이터에서 표본평균을 뺀 후 제곱을 하고, 이에 산술평균을 취하는데, 1/N이 아니라 1/(N-1)로 표본분산을 정의한다.
: : 이렇게 하면 표본분산의 기대값이 원래 모집단의 분산 𝜎<sup>2</sup>과 일치하게 되기 때문이다.
: : 통계량의 확률분포를 표집분포(sampling distribution)라 부르며, 특히 표본평균과 표집분포는 N이 커질수록 정규분포 𝓝(μ, 𝜎<sup>2</sup>/𝘕)를 따른다.
: : 이를 중심극한정리(Central Limit Theorem)이라 부르며, 모집단의 분포가 정규분포를 따르지 않아도 표본평균의 포집분포는 정규분포를 따른다는 사실을 기억하자.
: <br><b>최대가능도 추정법 (심화 과제 2)</b><br><br>

<h2>과제 수행 내용</h2>
- <b>심화 과제1</b><br><br>
: : <a href="https://colab.research.google.com/drive/1u5pMFAVcm3Weu-Lax4g5W1NVkmEBCG1U#scrollTo=_rPsh3Gm07LV">심화 과제 1 링크</a>
: : Google Colab내에 주석 형태로 이해가 잘 가지 않는 부분들을 comment 해놓았고, peer session에서 팀원들과 토론을 통해 해결할 예정




<h2>Peer Session</h2>

- AI Math - 5~6강 및 심화과제 1 토론
: :  토론 내용


