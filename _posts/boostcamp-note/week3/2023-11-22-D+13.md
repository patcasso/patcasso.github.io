---
layout: single
title:  "Day 13 학습정리"
categories: boostcamp-note-week3
sidebar:
  nav: "docs"
---

# 23/11/22 (수) 학습 내용

<h2>DL Basic</h2>

- (8강) Transformer<br><br>
: <br><b>강의 키워드</b><br><br>
: - Transformer
: - Multi Head Attention
: - Positional Encoding
: <br><b>Transformer</b><br><br>
: : 2017년, \<Attention Is All You Need\>
: : Transformer is the first sequence transduction model based entirely on attention.
: : RNN은 재귀적으로 작동했는데, Transformer는 재귀적인 구조가 없고, attention이라는 구조를 활용했다.
: : 원래는 NMT(Neural Machine Translation)에서 사용하기 위해 개발되었다.
: : GPT-3도 Self-attention, Transformer 원리를 이용한 것
<br><br>
: : Seq2Seq model (ex. 주어진 sequence 프랑스어 -> 아웃풋 sequence 영어)
: : 입력 seq과 출력 seq의 길이는 다를 수 있다.
: : 인코딩 할때는 N개의 단어를 한 번에 처리할 수 있다.
: : 동일한 구조를 갖지만, 네트워크 파라미터가 다르게 학습되는 Encoder와 Decoder가 Stack되어 있는 것.
<br><br>
: <br><b>1) N개의 단어가 어떻게 인코더에서 한 번에 처리되는가?</b><br><br>
: : 인코더에 벡터가 한 번에 100, 200개 등 한 번에 들어가는 것
: : Self-Attention이라는 구조와, Feed Forward Neural Network라는 구조를 한 번씩 거치는 게 하나의 인코더
: : 그 출력으로 나오는 N개의 출력값이 두 번째 레이어 인코더로 들어가는 식
: : The <u>Self-Attention</u> in both encoder and decoder is the cornerstone of Transformer.
: : Feed Forward NN은 그냥 MLP와 동일하다.
<br><br>
: : NMT 문제를 푼다고 가정하면, (PDF 참조)
: : N개의 단어가 들어가면 N개의 벡터로 표현하게 된다.
: : Self-Attention이 하는 일은, N 개의 단어가 들어가면 N 개의 벡터를 찾아준다.
: : Self-Attention은 $x_1$이 $z_1$으로 넘어갈 때, 단순히 $x_1$의 정보만 활용하는 것이 아니라, $x_2, x_3$ 정보를 같이 활용한다.
: : 즉, $n$개의 단어가 주어지고 $n$개의 벡터를 찾는데, $x_i$를 $z_i$로 바꿀 때, 나머지 $n-1$개의 $x$ 벡터를 같이 고려하는 것이 바로 Self-Attention의 가장 큰 특징이다.
: : Self-attention 구조는 Dependency가 있고, Feed Forward NN은 서로 단어들에 대해서 independent하다.
<br><br>
: : 예를 들어, 'Thinking Machines'가 주어졌을 때, 
: : 세 가지 벡터를 만들어내게 된다. (Query, Key, Value)
: : <span style="color:red">Q. Query, Key, Value 벡터는 Neural Network를 통해 만든다는데, 구체적으로 어떤 뉴럴 네트워크인가? => (Multi Layer Perceptron 이라고 하고, 인코딩된 단어마다 다 share 된다고 한다)</span>
: : NN가 세 개 있다고 보면 된다.
: : 이 벡터들은 하나의 입력이 주어졌을 때, 하나마다 세 개의 벡터를 만들게 된다.
: : 이 세 개의 벡터를 통해 $x_1$이라는 첫번째 단어에 대한 Embedding 벡터를 새로운 벡터로 바꿔줄 것이다.
<br><br>
: : Score벡터를 만드는데, i번째 단어에 대한 score 벡터를 계산할 때, 내가 인코딩을 하고자 하는 벡터의 쿼리벡터와, 나머지 모든 n개의 단어에 대한 key 벡터를 구하고, 그 두 개를 내적하는 것.
: : 즉, i번째 단어가 나머지 n개의 단어와 얼마나 유사도가 있는지, 관계가 있는지를 정하게 된다.
: : 내가 인코딩하고자 하는 벡터의 쿼리 벡터와, 자기 자신을 포함한 나머지 벡터들의 키 벡터를 전부 구한 후, 그 둘 사이의 내적을 한다.
: : i번째 단어와 나머지 단어 사이에 얼마나 interaction 해야하는지를 알아서 학습하도록 하는 것. 이것이 바로 "Attention"!
<br><br>
: : Score 벡터가 나오면 한번 normalize 해준다. (Key 벡터의 제곱근으로 나눠준다)
: : Score value가 어떤 범위 안에 들어가도록 해주기 위해서
: : 이후에 normalize 취한 값을 softmax를 취해주는 것이다. 이것이 "Attention rate"
: : 최종적으로 직접 사용할 값(해당 단어에 대한 인코딩된 벡터)은, 각각의 단어에서 나오는 Value 벡터들의 Weighted Sum이 되는 것이다.
<br><br>
: : !주의해야 할 점은, Query 벡터와 Key 벡터는 항상 차원이 같아야 한다. (내적해야 하므로)
: : Value 벡터는 차원이 달라도 된다. (Weighted Sum을 하기만 하면 되니까)
: : 인코딩된 벡터의 차원은 Value 벡터의 차원과 같게 된다. (Multi-head attention 에서는 달라진다)
<br><br>
: : 이게 잘 되는 이유? 이미지 하나를 CNN이나 MLP로 dimension을 바꾸면, 입력이 고정되어 있으면 출력이 고정된다. 왜냐하면, Conv. filter나 Weight가 고정되어 있기 때문이다.
: : 반면 Transformer는 하나의 인풋이 고정되어 있다고 해도, 인코딩 하려는 단어와 옆에 있는 단어들에 따라 인코딩 된 값이 달라지기 때문이다. 따라서 MLP보다 조금 더 Flexible 하다. 입력이 고정되더라도, 내 옆에 주어진 다른 입력들에 따라 출력이 달라질 여지가 있는 것이다.
: : 바꿔 말하면, 더 많은 Computation이 필요하게 된다. ($N^2$의 Attention map을 만들어야 한다.)
: : Length가 길어질수록 처리할 수 있는 한계가 있다. 그것이 어떻게 보면 Transformer의 한계인 것.
<br><br>
: : N개의 Attention을 반복하면, N개의 인코딩된 벡터가 나오게 된다.
: : 임베딩된 dimension과, 인코딩되어서 Self-attention으로 나오는 벡터가 항상 같은 차원이어야 한다.
: : N개의 Output이 나오니까, 또 다른 학습 가능한 선형 변환 매트릭스를 곱하여 차원을 맞춰주게 된다. (블로그 참조하여 더 정확히 이해하기)
<br><br>
: : 그러나 실제로는,
: : 원래 주어진 Embedding Dimension이 100이고, 10개의 Head를 사용한다고 하면, 100 Dimension을 10개로 나눈다. 실제로 Q, K, V 벡터를 만드는 것은 10 dim만 갖고 돌아가게 된다. (실습 코드 참조)
<br><br>
: : Positional Encoding을 추가한다(Bias 라고 보면 된다)
: : 필요한 이유는, sequential한 정보가 포함되어 있지 않기 때문이다. 
: : ABCD, BACD, DCBA 등의 순서로 다르게 넣어도 각각의 단어가 인코딩되는 값은 똑같다. (인코딩 과정이 순서에 independent하므로)
: : 주어진 입력에 그냥 어떤 값을 더하는 것임.
: : Positional encoding이 만들어지는 방법은 Predefined 방법에 따른 것
<br><br>
: <br><b>2) 인코더와 디코더 사이에 어떤 정보를 주고받는지?</b><br><br>
: <br><b>3) 디코더가 어떻게 Generation 을 할 수 있는지?</b><br><br>
: : Encoder 는 주어진 단어를 표현하는 거였고, Decoder는 그걸로 뭔가를 생성하는 것
: : 인코더에서 디코더에 어떤 정보가 전해지는가? -> Key와 Value를 보낸다.
: : (Transformer transfers key (K) and value (V) of the topmost encoder to the decoder)
: : Decoder에 들어가는 단어들로 만들어진 Query 벡터와, Encoder로 얻어지는 K, V 벡터들을 가지고 최종값이 나오게 된다.
: : 최종 출력은 autoregressive 하게(하나의 단어씩) 만들어진다.
: : 마지막에 eos(End of sentence) 라고 하는 스페셜 토큰이 나오면 끝나게 된다.
: : 디코더에서 masking 을 하게 되는데, 이전 단어들만 dependent하고 뒤에 있는 단어들에 dependent 하지 않도록 하는 방법.
: : Encoder-Decoder Attention은 Encoder와 Decoder 사이의 interaction인데, 작동 방식은 Multi-headed self-attention과 동일한데, 다만 이전까지 generate 한 단어들만 갖고 Query를 만들고, Key, Value는 Input raw sequence에서 나오는 인코딩 벡터를 활용하게 된다.
<br><br>
: : The final layer converts the stack of decoder outputs to the distribution over words.
: : 단어들의 분포를 만들어서, 그 중의 단어 하나를 매번 샘플링하는 방식으로 돌아가게 된다.
: : 해당 transformer는 NMT에만 해당이 되었었는데...
<br><br>
: <br><b>Vision Transformer</b><br><br>
: : 이미지를 특정 영역으로 나누고, 각 영역의 sub-patch들을 linear layer를 통과해 입력처럼 넣어주게 되고, positional embedding이 들어가게 된다.
: <br><b>DALL-E</b><br><br>
: : 엄청나다!
: : Transformer에 있는 decoder만 활용을 했다고 한다. (GPT-3를 활용했다고 함)
: : 실습에서는 MHA를 파이토치로 구현하는 법을 알아본다.


- (9-1강) Generative Models Part 1<br><br>
: <br><b>강의 키워드</b><br><br>
: - Basic Probability Theory
: - Autoregressive Models
: <br><b>소개</b><br><br>
: : Generative model을 학습한다는 것은 무슨 의미인가?
: <br><b>Learning a Generative Model</b><br><br>
: : Generation: p(x)의 분포를 배운다.
: : Density estimation: p(x) should be high if x looks like a dog, and low otherwise
: <br><b>Basic Discrete Distributions</b><br><br>
: : Bernoulli distribution: (biased) coin flip
: : D = {Heads, Tails}
: : Specify $P(X = $Heads$) = p$. Then $P(X = $Tails) $= 1-p$
: : Write: $X \sim Ber(p)$
: : 나올 수 있는 경우의 수는 두 개이고, 이 확률분포를 표현할 수 있는 파라미터의 수는 한 개(p)이다.
<br><br>
: : Categorical distribution: (biased) m-sided dice
: : D = {1, ... , m}
Specify $P(Y=i) = p_i$ such that $ \sum_{i=1}^{m}p_i = 1$
: : Write: $Y \sim Cat(p1,...p_m)$
: : (m-1)개의 파라미터가 필요하다 (다섯 개의 면이 나올 확률만 정의하면 나머지는 1에서 나머지의 합을 빼면 된다)
<br><br>
: : <u>Example</u>
: : $(r,g,b) \sim p(R,G,B)$
: : Number of cases? = 256 x 256 x 256 (한 개의 픽셀)
: : How many parameters do wee need to specify? 256 x 256 x 256 -1 (역시 한 개의 픽셀)
<br><br>
: <br><b>Independence</b><br><br>
: : 파라미터 수를 줄이기 위한 가장 기본이 되는 개념 중 하나.
: : $X_1, ... X_n$ of $n$ binary pixels(binary image)
: : Number of cases? $2^n$
: : How many parameters? $2^n - 1$
: : 픽셀이 28*28만 되어도 거의 구현이 불가능한 거대한 숫자가 된다. (데이터 수가 파라미터 수보다 적으면 학습이 잘 안 된다.)
<br><br>
: : What if $X_1, ... ,X_n$ are **independent**, then
: : $P(X_1, ..., X_n) = P(X_1)P(X_2)P(X_3)...P(X_n)$
: : 경우의 수는 그대로 $2^n$이지만,
: : 이 분포를 표현할 수 있는 파라미터의 수는 n이면 된다.
: : 이게 무슨 말일까? (Exponential reduction)
: : 이런 independence assumption은 굉장히 많은 기계학습 분야에서 사용해왔다.
: : 다만 파라미터 수를 굉장히 줄여주기는 하지만, 유의미한 분포는 얻어낼 수 없다.
<br><br>
: <br><b>Conditional Independence</b><br><br>
: : 중간 어딘가에 있는, 모두 독립과 모두 종속의 사이.
: : 세 가지 중요한 규칙. ($,$ 는 $\cap$을 의미)
: - 1) Chain rule (독립 여부 상관 없이 항상 만족)
<br> $p(x_1,...,x_n) = p(x_1)p(x_2 \vert x_1)p(x_3 \vert x_1,x_2)...p(x_n \vert x_1, ..., x_{n-1})$
: - 2) Bayes' rule
<br> $p(x \vert y) =  \frac{p(x,y)}{p(y)} =  \frac{p(y\vert x)p(x)}{p(y)}$
: - 3) Conditional independence
<br> If $ x \perp y \vert z,$ then $p(x\vert y,z) = p(x \vert z)$ 
<br>: if (z라는 확률변수가 주어졌을 때, x와 y가 independent 하면)
<br>: then ($y$와 $z$가 모두 주어졌을 때 $x$에 대한 conditional distribution이, $y$를 떼어낼 수 있게 되는 것)
<br><br>
: : Using the chain rule
: : $P(X_1,...,X_n) = p(X_1)p(X_2 \vert X_1)p(X_3 \vert X_1,X_2)...p(X_n \vert X_1, ..., X_{n-1})$
: : How many parameters?
: : $P(X_1)$: 1 parameter
: : $P(X_2 \ vert X_1) = 2$
: : $P(X_3 \ vert X_1, X_2) = 4$
: : ...
: : 그러므로 $2^n - 1$로 이전과 같다. 
<br><br>
: : Now, suppose $X_{i+1} \perp X_1, ..., X_{i-1} \vert X_i$(Markov assumption), then
: : $p(x1, ..., xn) = p(x_1)p(x2 \vert x_1)p(x_3 \vert x_2)...p(x_n \vert x_{n-1})$
: : 즉, 직전에 있는 값에만 dependent 한 것
: : Parameter 개수는 $2n-1$이 된다. (1+2+2+2...+2(n번째까지))
: : Fully independence보다는 크고, completely dependent 보다는 작게 된다 (더 실용적임)
: : 이를 잘 활용하는 모델이 Autoregressive, AR model 이다.
<br><br>
: <br><b>Autoregressive Models</b><br><br>
: : Suppose we have 28 x 28 binary pixel.
: : $P(X_{1:784}) = P(X_1)P(X_2 \vert X_1)P(X_3 \vert X_2)...$
: : This is called autoregressive model
: : We need an <u>ordering</u>(e.g., raster scan order) of all r.v.s.
<br><br>
: <br><b>NADE: Neural Autoregressive Density Estimator</b><br><br>
: : Explicit model that can compute the density of the given inputs.
<br><br>
: <br><b>Summary of AR models</b><br><br>
: : Easy to <u>sample</u> from
: : Easy to <u>compute probability</u>
: : Easy to be extended to <u>continuous</u> variables (이산 말고 연속 확률변수에도 적용 가능)

- (9-2강) Generative Models Part 2<br><br>
: <br><b>강의 키워드</b><br><br>
: - Maximum Likelihood Learning
: - Latent variable models
: - Generative Adversarial Networks
: - Diffusion Models
: <br><b>Maximum Likelihood Learning</b><br><br>
: : How can we evaluate the goodness of the approximation?
: : (분포 사이의 거리가 가까워지는 기준)
<br><br>
: : <u>KL-divergence</u> leads to VAE
: : 근사적으로 두 확률분포 사이의 거리(?)를 나타내는 것 (not symmetric)
: : <u>Jensen-Shannon divergence</u> leads to GAN (KL Divergence의 symmetric version)
: : <u>Wasserstein distance</u> leads to WAE or AAE
: <br><b>Latent Variable Models (VAE)</b><br><br>
: : Is an autoencoder a generative model? No! It's just a model
: : objective is simple. Maximize $p \theta (\mathbb{x})$
<br><br>
: <br><b>Generative Adversarial Network</b><br><br>
: : Discriminator, Generator 두 개의 네트워크가 서로가 서로를 속이려고 한다
: <br><b>Diffusion Models</b><br><br>
: : Diffusion models generate images from noise.(Wow!)
: : VAE, GAN도 노이즈로부터 이미지를 만들어내는 것에서는 비슷하지만, 큰 차이점은
: : Diffusion model은 조금씩 이미지를 변경해가면서 생성하게 된다. (예: original diffusion : 1000번의 과정을 거쳐 이미지를 생성해낸다.)
<br><br>
: : Diffusion process
: : Forward process에서는 이미지에 노이즈를 집어넣어서, 이미지를 점점 노이즈화 시키는 것.
: : Reverse process에서는 노이즈를 없애고 clean image로 바꾸는 과정을 학습하는 것이다.
: : 즉, 굉장히 많은 step을 거쳐 noise 벡터를 original image로 refinement 해나가는 과정이다.

<h2>Peer Session</h2>
- 1) 주제 1<br><br>
: : 
: : 
- 2) 주제 2<br><br>
: : 
: : 