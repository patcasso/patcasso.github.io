---
layout: single
title:  "Day 11 í•™ìŠµì •ë¦¬"
categories: boostcamp-note-week3
sidebar:
  nav: "docs"
---

# 23/11/20 (ì›”) í•™ìŠµ ë‚´ìš©

<h2>DL Basic</h2>

- (1ê°•) Historical Review<br><br>
: <br><b>ê°•ì˜ í‚¤ì›Œë“œ</b><br><br>
: - ë”¥ëŸ¬ë‹ì˜ ì—­ì‚¬ 
: <br><b>\<ì¸íŠ¸ë¡œ\></b><br><br>
: : <u>ì¢‹ì€ ë”¥ëŸ¬ë„ˆì˜ 3ìš”ì†Œ</u>
<br><br>
: : Implementation Skills (TensorFlow, PyTorch ë“±)
: : Math Skills (Linear Algebra, Probability)
: : Knowing (a lot of recent) Papers<br><br>
: : <u>ë”¥ëŸ¬ë‹ì˜ í•„ìˆ˜ 4ìš”ì†Œ</u>
<br><br>
: : **Data** that the model can learn from
: : Data type depend on the type of the problem to solve
: : Classification, Semantic Segmantation(ì´ë¯¸ì§€ì˜ í”½ì…€ë³„ë¡œ ë¶„ë¥˜), Detection(ì´ë¯¸ì§€ ì•ˆ ë¬¼ì²´ì— ëŒ€í•œ bounding box ì°¾ê¸°), Pose Estimation, Visual QnA
<br><br>
: : **Model** how to transform the data
: : AlexNet, GoogLeNet, ResNet, DenseNet, LSTM, Deep AutoEncoders, GAN
<br><br>
: : **Loss** function that quantifies the badness of the model
: : Regression Task - MSE<br><br>
: $ \frac{1}{N}\sum_{i=1}^{N}\sum_{d=1}^{D}(y_i^{(d)} - \hat{y}_i^{(d)})^2$<br><br>
: : Classification Task - CE 
: $ - \frac{1}{N}\sum_{i=1}^{N}\sum_{d=1}^{D} y_i^{(d)}\log\hat{y}_i^{(d)}$<br><br>
: : Probabilistic Task - MLE
: $ \frac{1}{N}\sum_{i=1}^{N}\sum_{d=1}^{D} \log ğ’© \hspace{1mm} (y_i^{(d)};\hat{y}_i^{(d)},1)$<br><br>
<br><br>
: : **Algorithm** to adjust the parameters to minimize the loss
: : SGD, Momentum, NAG, Adagrad, Adadelta, Rmsprop
: : ì •ê·œí™” - Dropout, Early stopping, k-fold validation, Weight decay, Batch normalization, MixUp, Ensemble, Bayesian Optimization
<br><br>
: <br><b>\<Historical Review\></b><br><br>
: : 2012 - AlexNet ì´ë¼ëŠ” ë…¼ë¬¸ ì¶œì‹œ. CNN. ë”¥ëŸ¬ë‹ì„ ì´ìš©í•´ ì²˜ìŒìœ¼ë¡œ 1ìœ„ë¥¼ í•¨. (Paradigm Shift)
: : 2013 - DQN. DeepMindê°€ RLì„ ì´ìš©í•´ Atarië¥¼ ê¹¨ëŠ” ëª¨ë¸
: : 2014 - Encoder / Decoder, Adam
: <span style="color:gray">(Adamì€ Hyperparameterë¥¼ ë§ì´ íŠœë‹í•˜ì§€ ì•Šì•„ë„ ì›¬ë§Œí•˜ë©´ ì˜ ë¼ì„œ, resource ë¶€ì¡±í•œ ì‚¬ëŒë“¤ì´ ì•ˆì „í•˜ê²Œ ì±„íƒí•˜ê¸°ì— ì¢‹ì€ ëª¨ë¸ì´ë‹¤.)</span>
: : 2015 - GAN, ResNet
: <span style="color:gray">(GAN - Generative Adversarial Network)</span>
: <span style="color:gray">(Resnet - Residual Networks : ì´ ì—°êµ¬ ë•Œë¬¸ì— ë”¥ëŸ¬ë‹ì˜ deep ëŸ¬ë‹ì´ ê°€ëŠ¥í•´ì¡Œë‹¤. ì´ì „ê¹Œì§€ëŠ” ë„ˆë¬´ ê¹Šê²Œ layerë¥¼ ìŒ“ìœ¼ë©´ ì˜ ì•ˆ ëœë‹¤ëŠ” ì¸ì‹ì´ ìˆì—ˆëŠ”ë°, ì´í›„ì—ëŠ” 100ë‹¨ ì •ë„ê¹Œì§€ ìŒ“ì•„ë„ test dataì—ì„œì˜ ì„±ëŠ¥ì´ ê°œì„ ë  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.)</span>
: : 2017 - Transformer
: <span style="color:gray">(Attention is all you need ë…¼ë¬¸. ì´ì œëŠ” Vision ë“± ë‹¤ë¥¸ ë¶„ì•¼ë„ ë§ì´ ëŒ€ì²´í•´ë‚˜ê°€ê³  ìˆë‹¤. Multi Head Attention êµ¬ì¡°ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.)</span>
: : 2018 - Bert (Fine-tuned NLP models)
: <span style="color:gray">Bidirectional Encoder Representations from Transformers</span>
: : 2019 - Big Language Models(GPT-X)
: <span style="color:gray">(êµ‰ì¥íˆ ë§ì€ parameterë¡œ ë˜ì–´ ìˆìŒ. GPT-3ëŠ” 1750ì–µê°œì˜ íŒŒë¼ë¯¸í„°)</span>
: : 2020 - Self Supervised Learning
: <span style="color:gray">(SimCLR :  a simple framework for contrastive learning of visual representations. ì£¼ì–´ì§„ í•™ìŠµ ë°ì´í„° ì™¸ì— labelì„ ëª¨ë¥´ëŠ” unsupervised learningì„ ìˆ˜í–‰í•˜ê² ë‹¤ëŠ” ê²ƒ)</span>
<br><br>

- <b>(2ê°•) Neural Networks & Multi-Layer Perceptron</b><br><br>
: <br><b>ê°•ì˜ í‚¤ì›Œë“œ</b><br><br>
: - Multi-Layer Perceptron
: - Hidden Layers
: - Activation Functions
: <br><b>ì¸íŠ¸ë¡œ</b><br><br>
: : ë”¥ëŸ¬ë‹ì€ ê¼­ ì‚¬ëŒì˜ ë‡Œë¥¼ ëª¨ë°©í•œ ê²ƒì¼ê¹Œ?
: : ex) Backpropagationì´ ì¸ê°„ì˜ ë‡Œì—ì„œëŠ” ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤
: : cf) ë¹„í–‰ê¸°ê°€ ê¼­ ìƒˆë‚˜ ë°•ì¥ì˜ ëª¨ì–‘ì„ ëª¨ë°©í•´ì„œ ë‚ ì§€ëŠ” ì•ŠëŠ”ë‹¤.
: : Neural networks are function approximators that stack affine transformations followed by nonlinear transformations.
: <br><b>Linear Neural Networks</b><br><br>
: Data : $D = {(x_i,y_i)}$ (i=1 ~ i=N)
: Model : $\hat{y} = wx + b$
: Loss : $loss =  \frac{1}{N} \sum_{i=1}^{N}(y_i - \hat{y}_i)^2$ (MSE)<br><br>
: $ \frac{\partial loss}{\partial w} = \frac{\partial}{\partial w} \frac{1}{N} \sum_{i=1}^{N}(y_i - \hat{y}_i)^2$
: &emsp;&emsp;&emsp;$= \frac{\partial}{\partial w} \frac{1}{N} \sum_{i=1}^{N}(y_i - wx - b)^2$
: &emsp;&emsp;&emsp;$= - \frac{1}{N} \sum_{i=1}^{N}2(y_i - wx - b)x_i$
<br><br>
: <a href="http://taewan.kim/post/cost_function_derivation/">MSE í¸ë¯¸ë¶„ ê³¼ì • ì„¤ëª…</a>
<br><br>
: $ \frac{\partial loss}{\partial b} = \frac{\partial}{\partial b} \frac{1}{N} \sum_{i=1}^{N}(y_i - \hat{y}_i)^2$
: &emsp;&emsp;&emsp;$= \frac{\partial}{\partial b} \frac{1}{N} \sum_{i=1}^{N}(y_i - wx - b)^2$
: &emsp;&emsp;&emsp;$= - \frac{1}{N} \sum_{i=1}^{N}2(y_i - wx - b)$
<br><br>
: $w \leftarrow w - \eta \frac{\partial loss}{\partial w}$
: $b \leftarrow b - \eta \frac{\partial loss}{\partial b}$
<br><br>
: : Gradient Descent - ì†ì‹¤í•¨ìˆ˜ë¥¼ ê°ê° W, bì— ëŒ€í•´ í¸ë¯¸ë¶„ì„ êµ¬í•˜ê³ , ì´ ê°’ì— learning rateë¥¼ ê³±í•œ ê°’ì„ í˜„ì¬ì˜ W, bì—ì„œ ëºŒìœ¼ë¡œì¨ W, b ê°’ì„ ì—…ë°ì´íŠ¸ í•´ì£¼ëŠ” ê²ƒ
: <br><b>Linear Transformation</b><br><br>
: : We can handle multi dimensional input and output
: : $\mathbb{y} = W^T + \mathbb{b}$
: : mì°¨ì›ì—ì„œ nì°¨ì›ìœ¼ë¡œ ê°€ëŠ” ì„ í˜• ë³€í™˜
: <br><b>Beyond Linear Neural Network</b><br><br>
: : ì„ í˜• ê²°í•©ë§Œì„ ì•„ë¬´ë¦¬ ë§ì´ ë°˜ë³µí•´ë„ ê²°êµ­ í•œ ê°œì˜ í–‰ë ¬ ê³±ê³¼ ë‹¤ë¥¼ ë°”ê°€ ì—†ìœ¼ë¯€ë¡œ, í™œì„± í•¨ìˆ˜ë¥¼ í†µí•œ Nonlinear transformationì„ ê±°ì³ì•¼ ë¹„ì„ í˜•ì„±ì„ ì§€ë‹ ìˆ˜ ìˆë‹¤.
<br><br>
: : <u>Activation functions</u>
: : ReLU (Rectified Linear Unit)
: : Sigmoid
: : Hyperbolilc Tangent
: <br><b>Multi-Layer Perceptron</b><br><br>
: : <u>Loss function?</u>
: : Regression Task - MSE
: : Classification Task - CE
: : Probabilistic Task - MLE
<br><br>

- (3ê°•) Optimization
: <br><b>ê°•ì˜ í‚¤ì›Œë“œ</b><br><br>
: - Gradient Descent
: - Learning Rate
: - Optimization Algorithms
: <br><b>ì¸íŠ¸ë¡œ</b><br><br>
: : Gradient Descent - í•­ìƒ local minimaë¡œ ê°€ê²Œ ë˜ì–´ ìˆìŒ
: <br><b>\<Important Concepts in Optimization\></b><br>
: <br><b>Generalization</b><br><br>
: : ë§ì€ ê²½ìš°, ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ëŠ” ê²ƒì´ ìš°ë¦¬ì˜ ëª©ì 
: : ì¼ë°˜ì ìœ¼ë¡œ Training Errorì™€ Test Errorì˜ ì°¨ì´ ì •ë„ë¥¼ ë§í•œë‹¤.
: <br><b>Underfitting vs. Overfitting</b><br><br>
: : í•™ìŠµ ë°ì´í„°ëŠ” ì˜ ë˜ëŠ”ë°, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì•ˆ ë˜ëŠ” ê²ƒ => Overfitting
: : ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•˜ê±°ë‚˜ í•˜ì—¬ í•™ìŠµ ë°ì´í„°ë„ ì˜ ì•ˆë¨ => Underfitting
: : ì´ ì‚¬ì´ì˜ Sweet spotì„ ì°¾ëŠ” ê²ƒ
: <br><b>Cross-validation</b><br><br>
: : k-fold validation ì´ë¼ê³  í•˜ê¸°ë„ í•¨
: : k-foldë¥¼ í†µí•´ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ê³ , í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ê³ ì •ì‹œí‚¨ í›„ì—ëŠ” ëª¨ë“  ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ train í•œë‹¤.
: : Trainingì—ëŠ” ì ˆëŒ€ë¡œ ì–´ë–¤ ì‹ìœ¼ë¡œë“  Test Dataë¥¼ í™œìš©í•´ì„œëŠ” ì•ˆ ëœë‹¤.
: <br><b>Bias and Variance</b><br><br>
: : ì›ì ì—ì„œ ë©€ë”ë¼ë„, í•­ìƒ ê°™ì€ ê³³ì—ë§Œ ì°íˆë©´ ì¢‹ì€ ê²ƒ. (= Low Variance)
: : Bias and Variance Tradeoff
: : Given $$D = \{(x_i,t_i)\}_{i=1}^{N}$$, where $t = f(x) + \epsilon$ and $\epsilon \sim  \mathcal{N}(0,\sigma^2)$
: : costëŠ” bias, variance, noise ì´ë ‡ê²Œ ì„¸ ê°€ì§€ë¡œ ì´ë£¨ì–´ì ¸ ìˆìœ¼ë¯€ë¡œ, biasì™€ varianceë¥¼ ë™ì‹œì— ì¤„ì¼ ìˆ˜ëŠ” ì—†ë‹¤.
: <br><b>Bootstrapping</b><br><br>
: : Boostrapping is any test or metric that uses random sampling with replacement(ë³µì› ì¶”ì¶œ).
: <br><b>Bagging vs. Boosting</b><br><br>
: : <u>Bagging</u> (Bootstrapping aggregating)
: : ì´ë¥¼ ì¼ë°˜ì ìœ¼ë¡œ Ensemble ì´ë¼ê³  ë¶€ë¥´ê¸°ë„ í•œë‹¤.
: : Kaggle ê°™ì€ ëŒ€íšŒë¥¼ í’€ ë•Œ ë§ì´ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ ì•™ìƒë¸”ì¸ë°, ì•™ìƒë¸”ì€ Baggingì— ì†í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.
: : <u>Boosting</u>
: : Focuses on those specific training samples that are hard to classify
: : ì˜ ì•ˆë˜ëŠ” ë°ì´í„°ë“¤ì— ëŒ€í•œ ëª¨ë¸ë§Œ ë”°ë¡œ ë§Œë“¤ê³ , ì´í›„ ì—¬ëŸ¬ weak learner ëª¨ë¸ì„ ë§Œë“¤ì–´ í•˜ë‚˜ì˜ strong learner ëª¨ë¸ë¡œ í•©ì¹˜ëŠ” ê²ƒ
: : Baggingì€ ë³‘ë ¬ì ìœ¼ë¡œ ì—¬ëŸ¬ ëª¨ë¸ì„ ë§Œë“œëŠ” ëŠë‚Œ, Boostingì€ ì—°ì‡„ì ìœ¼ë¡œ ëª¨ë¸ì„ ì—°ê²°ì‹œì¼œ í•˜ë‚˜ì˜ ëª¨ë¸ì„ ë§Œë“¤ì–´ë‚´ëŠ” ëŠë‚Œ
: <br><b>Gradient Descent Methods</b><br><br>
: : 1) SGD (Stochastic gradient descent)
: : ì—„ë°€í•œ ì˜ë¯¸ì˜ SGDëŠ” ë‹¨ í•˜ë‚˜ì˜ ìƒ˜í”Œë§Œ í™œìš©í•˜ëŠ” ê²ƒ
: : 2) **Mini-batch gradient descent**
: : Update with the gradient computed from a subset of data (ëŒ€ë¶€ë¶„ GD ì—ì„œëŠ” Mini-batch í™œìš©)
: : 3) Batch gradient descent
: : Update with the gradient computed from the whole data
: <br><b>Batch-size Matters</b><br><br>
: : Generalizeë¥¼ ìœ„í•´ ì¤‘ìš”í•¨
: : Batch sizeë¥¼ í¬ê²Œ í•˜ë©´ sharp minimizerë¡œ ìˆ˜ë ´í•˜ê³ , ì‘ê²Œ í•˜ë©´ flat minimizer ë¡œ ìˆ˜ë ´í•œë‹¤.
: <br><b>Choosing Optimizer</b><br><br>
: 1) SGD
: : Gradient Descent : $W_{t+1} \leftarrow W_t - \eta g_t$
: : learning rateë¥¼ ì¡ëŠ” ê²ƒì´ ì–´ë µë‹¤. ë„ˆë¬´ í¬ë©´ í•™ìŠµì´ ì•ˆ ë˜ê³ , ì‘ìœ¼ë©´ ì•„ë¬´ë¦¬ ì‹œì¼œë„ í•™ìŠµì´ ì•ˆ ëœë‹¤.
<br><br>
: 2) Momentum (ê´€ì„±)
: : $a_{t+1} \leftarrow \beta a_t + g_t$ ($\beta$ = momentum)
: : $W_{t+1} \leftarrow W_t - \eta a_{t+1}$
<br><br>
: 3) NAG (Nesterov Accelerated Gradient)
: : í˜„ì¬ ì •ë³´ê°€ ìˆìœ¼ë©´ ê·¸ ìª½ìœ¼ë¡œ ê°€ë³´ê³ , ê°„ ê³³ì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°í•œ ê²ƒì„ ê°€ì§€ê³  accumulation í•˜ëŠ” ê²ƒ
: : $a_{t+1} \leftarrow \beta a_t + \triangledown \mathcal{L}(W_t-\eta\beta a_t)$
: : $W_{t+1} \leftarrow W_t - \eta a_{t+1}$
: : ë´‰ìš°ë¦¬ì— ì¡°ê¸ˆ ë” ë¹¨ë¦¬ converge í•˜ê²Œ ë˜ëŠ” íš¨ê³¼ê°€ ìˆìŒ
<br><br>
: 4) Adagrad
: : Adaptive ë°©ë²•ë“¤ì˜ ì‹œì´ˆ
: : Adagrad adapts the learning rate, performing larger updates for infrequent and smaller updates for frequent parameters.
: : $G$ = Sum of gradient squares (ì§€ê¸ˆê¹Œì§€ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì–¼ë§ˆë‚˜ ë§ì´ ë³€í–ˆëŠ”ì§€ë¥¼ ì œê³±í•´ì„œ ë”í•œ ê²ƒ)
: : $\epsilon$ for numerical stability(0ìœ¼ë¡œ ë‚˜ëˆ„ì§€ ì•Šê¸° ìœ„í•´)
: : $W_{t+1} = W_t -  \frac{\eta}{\sqrt{G_t + \epsilon}}g_t$
: : ê°€ì¥ í° ë¬¸ì œëŠ” $G$ê°€ ê³„ì† ì»¤ì§€ë¯€ë¡œ, ê²°êµ­ ë¬´í•œëŒ€ë¡œ ê°€ê²Œ ë˜ë©´ ë¶„ëª¨ê°€ ë¬´í•œëŒ€ì´ë¯€ë¡œ $W$ì˜ ì—…ë°ì´íŠ¸ê°€ ì•ˆ ë˜ê³ , ë’¤ë¡œ ê°€ë©´ ê°ˆ ìˆ˜ë¡ í•™ìŠµì´ ì ì  ë©ˆì¶”ëŠ” í˜„ìƒì´ ìƒê¸´ë‹¤. (ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì´ Adam ê°™ì€ ê²ƒë“¤)
<br><br>
5) Adadelta
: : Adadelta ì˜ $G$ê°€ ê³„ì† ì»¤ì§€ëŠ” ê²ƒì„ ë§‰ê² ë‹¤
: : ìœ„ì˜ ë¬¸ì œë¥¼ ë§‰ê¸° ìœ„í•´ Time windowë¥¼ 100ê°œë¡œ ì¡ëŠ”ë‹¤ê³  í•˜ë©´, íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ë§ì€ ëª¨ë¸ì˜ ê²½ìš° ë„ˆë¬´ ìˆ˜ê°€ ë§ì•„ì§€ê²Œ ëœë‹¤.
: : ê·¸ë˜ì„œ ì´ë¥¼ ë§‰ê¸° ìœ„í•´, exponential moving everageì´ë‹¤. (ì•„ë˜ì˜ ìˆ˜ì‹)
: ($G_t$ = EMA of gradient squares)
: : $G_t = \gamma G_{t+1} + (1-\gamma)g_t^2$
: : $W_{t+1} = W -  \frac{\sqrt{H_{t-1} + \epsilon}}{\sqrt{G_t + \epsilon}}g_t$
: : $H_t = \gamma H_{t-1} + (1-\gamma)(\triangle W_t)^2$
: : There is <u>no learning rate</u> in Adadelta
: : ë§ì´ ì‚¬ìš©ë˜ì§€ëŠ” ì•Šì•˜ë‹¤
<br><br>
6) RMSprop
: : ë§ì´ ì‚¬ìš©ë˜ì—ˆìŒ
: : $G_t = \gamma G_{t-1} + (1-\gamma)g_t^2$
: : $G_t$ = EMA of gradient squares
: : $W_{t+1} = W_t -  \frac{\eta}{G_t + \epsilon}g_t$
<br><br>
7) **Adam**
: : ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ ì˜ ë˜ê³  ë¬´ë‚œí•˜ê²Œ ì‚¬ìš©í•˜ëŠ” ê¸°ë²•ì´ Adam
: : ì»¨ì…‰ì ìœ¼ë¡œëŠ”, Gradient Squaresë¥¼ Exponential Moving Averageë¥¼ ê°€ì ¸ê°ê³¼ ë™ì‹œì—, ì•ì—ì„œ ì²˜ìŒì— ë°°ì› ë˜ momentumì„ ê°™ì´ í™œìš©í•˜ëŠ” ê²ƒ.
: : Adaptive Moment Estimationì´ë‹¤.
: : $m_t$ = Momentum, $v_t$ = EMA of gradient squares
: : í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œëŠ” $\beta_1$(ëª¨ë©˜í…€ì„ ì–¼ë§ˆë‚˜ ìœ ì§€ì‹œí‚¬ì§€ì— ëŒ€í•œ ê²ƒ), $\beta_2$(Gradient Squaresì— ëŒ€í•œ EMA ì •ë³´), $\eta$ (Learning rate), $\epsilon$
: : $m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$
: : $v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2$
: : $W_{t+1} = W_t -  \frac{\eta}{\sqrt{v_t + \epsilon}} \cdot \frac{\sqrt{1- \beta_2^t}}{1-\beta_1^t} \cdot m_t$
: : $\sqrt{1- \beta_2^t}$, $1-\beta_1^t$ ëŠ” ì „ì²´ Gradient Descentê°€ Unbiased Estimatorê°€ ë˜ê¸° ìœ„í•´ ìˆ˜í•™ì ìœ¼ë¡œ ì¦ëª…í•œ ê²ƒ
: <br><b>Regularization</b><br><br>
: : ì„ íƒí•œ ë°©ë²•ë¡ ì´ í•™ìŠµ ë°ì´í„° ë¿ë§Œ ì•„ë‹ˆë¼, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œë„ ì˜ ì‘ë™ë˜ë„ë¡ í•˜ëŠ” ê²ƒì´ ì •ê·œí™”ì˜ ëª©ì ì´ë‹¤.
<br><br>
: 1) <u>Early Stopping</u>
: : Trainingì— í™œìš©í•˜ì§€ ì•Šì€ Validation dataì˜ errorê°€ ì»¤ì§€ê¸° ì „ì— ì¼ì° ë©ˆì¶”ëŠ” ê²ƒ (ì¶”ê°€ì ì¸ validation dataê°€ í•„ìš”!)
<br><br>
: 2) <u>Parameter norm penalty</u>
: : NNì˜ íŒŒë¼ë¯¸í„°ê°€ ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šê²Œ í•˜ëŠ” ê²ƒ
: : total cost = $\mathbb loss(\mathcal D;W) +  \frac{\alpha}{2} \lVert W \rVert_2^2 $
: : It adds smoothness to the function space
<br><br>
: 3) <u>Data augmentation</u>
: : More data are always welcomed
: : ex) ê°•ì•„ì§€ ê³ ì–‘ì´ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ê°ë„ íšŒì „ (Label preserving augmentation)
<br><br>
: 4) <u>Noise robustness</u>
: : Add random noises to inputs or weights
: : ê·¸ëŸ¬ë©´ í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì—ì„œ ë” ì˜ ë  ìˆ˜ ìˆë‹¤
<br><br>
: 5) <u>Label smoothing</u>
: : Similar to data augmentation
: : í•™ìŠµ ë°ì´í„° ë‘ ê°œë¥¼ ë½‘ì•„ì„œ, ë‘ ê°œë¥¼ ì„ì–´ì£¼ëŠ” ê²ƒ.
: : ì™œ ì˜ ë˜ëŠ”ì§€ ì´ìœ ë³´ë‹¤ëŠ”, í™œìš©í•´ë³´ë©´ ì„±ëŠ¥ì´ ë§ì´ ì˜¬ë¼ê°„ë‹¤.
: : ë¶„ë¥˜ ë¬¸ì œë¥¼ í‘¸ëŠ”ë° ë°ì´í„°ì…‹ì´ í•œì •ì ì´ê³  ë” ë§ì€ ë°ì´í„°ë¥¼ ì–»ì„ ë°©ë²•ì´ ì—†ë‹¤ë©´ Mixupì´ë‚˜ CutMix ë“±ì„ í™œìš©í•´ ë³´ëŠ” ê²ƒì´ ì¢‹ë‹¤. (ì½”ë“œ ìì²´ê°€ êµ‰ì¥íˆ ê°„ë‹¨)
<br><br>
: : 6) <u>Dropout</u>
: : Neural Networkì˜ Weightì„ 0ìœ¼ë¡œ ë°”ê¾¸ëŠ” ê²ƒ
: : Dropout ratio=0.5 ë¼ê³  í•˜ë©´, NNì´ Inferenceë¥¼ í•  ë•Œì—ëŠ” NNì˜ ë‰´ëŸ° ì¤‘ 50%ë¥¼ 0ìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” ê²ƒ
<br><br>
: : 7) <u>Batch Normalization</u>
: : ë…¼ë€ì´ ë§ì€ ë…¼ë¬¸! (Internal Covariate Shift)
: : ì–˜ê¸°í•˜ìë©´, BNì„ ì ìš©í•˜ê³ ì í•˜ëŠ” Layerì˜ Statisticsë¥¼ ì •ê·œí™”ì‹œí‚¤ëŠ” ê²ƒ.
: : ê°€ì§€ëŠ” íš¨ê³¼ëŠ”, NN ê°ê°ì˜ Layerê°€ 1000ê°œì˜ parameterë¡œ ëœ Hidden layerë¼ê³  í•˜ë©´, ì´ íŒŒë¼ë¯¸í„°ë“¤ì— ëŒ€í•œ statisticsê°€ mean=0, unit varianceê°€ ë˜ê²Œ ë§Œë“œëŠ” ê²ƒ (ê°ê° íŒŒë¼ë¯¸í„°ë“¤ì— ëŒ€í•´ í‰ê· ì„ ë¹¼ê³  í‘œì¤€í¸ì°¨ë¥¼ ë‚˜ëˆ ì£¼ëŠ” ê²ƒ)
: : ë…¼ë¬¸ì—ì„œëŠ” Internal Covariate Shiftë¥¼ ì¤„ì´ëŠ” íš¨ê³¼ê°€ ìˆë‹¤ê³  í•œë‹¤.
: : $\mu_B =  \frac{1}{m}  \sum_{i=1}^{m}x_i$
: : $\sigma_{B}^{2} = \frac{1}{m} \sum_{i=1}^{m}(x_i - \mu_B)^2$
: : $\hat{x}_i =  \frac{x_i - \mu_B}{\sqrt{\sigma^2_B + \epsilon}}$
: : Layerê°€ ê¹Šê²Œ ìŒ“ì—¬ìˆê²Œ ë˜ë©´, ì„±ëŠ¥ì´ ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ì˜¬ë¼ê°€ê²Œ ëœë‹¤.
: : ì´ê²ƒì˜ ë³€í˜•ë“¤ë¡œëŠ” Layer Norm, Instance Norm, Group Norm ë“±ì´ ìˆë‹¤.
: : ì™œ ì˜ ë˜ëŠ”ì§€ë³´ë‹¤ëŠ”, ê°„ë‹¨í•œ ë¶„ë¥˜ ë¬¸ì œì˜ ê²½ìš° Batch Normalization ì„ í™œìš©í•˜ë©´ ë§ì€ ê²½ìš° ì„±ëŠ¥ì„ ì˜¬ë¦´ ìˆ˜ ìˆë‹¤!
<br><br>
: <br><b>3ê°• - ì‹¤ìŠµ</b><br><br>
: : ADAM > Momentum > SGD ì˜ í¼í¬ë¨¼ìŠ¤ ì°¨ì´ê°€ ë‚˜ì˜¤ëŠ” ì´ìœ 
: : ADAMì€ Momentumê³¼ Adaptive Learning Rateë¥¼ í•©ì¹œ ê²ƒì„.
: : ì¦‰, Momentumê³¼ Adaptive Learning Rate ë‘˜ ë‹¤ ì“°ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ê³ ,
: : ì™œ SGDëŠ” ëê¹Œì§€ ëª» ë§ì¶œê¹Œ?
: : Momentumì€ ì´ì „ì˜ gradientë¥¼ í™œìš©í•´ì„œ ë‹¤ìŒë²ˆì—ë„ ì“°ê² ë‹¤ëŠ” ì°¨ì´ì ë§Œ ìˆì„ ë¿ì¸ë°(adaptive learning rateì´ ì•ˆ ë“¤ì–´ê°€ ìˆìœ¼ë¯€ë¡œ), 
: : Mini-batch trainingì‹œ momentumì´ ì¢‹ì€ ì´ìœ ëŠ”,
: : SGDëŠ” ë„ˆë¬´ ë§ì€ ë°ì´í„°ê°€ ìˆì–´ì•¼ ì „ì²´ ì •ë³´ê°€ ë‹¤ Converge í•  ìˆ˜ ìˆê²Œ ë˜ëŠ”ë°,
: : Momentumì€ ì´ì „ batchì˜ ì •ë³´ë„ ë°˜ì˜í•˜ê¸° ë•Œë¬¸ì— ì¢€ ë” ë§ì€ ë°ì´í„°ë¥¼ ë³´ê²Œ ë˜ëŠ” íš¨ê³¼ê°€ ìˆëŠ” ê²ƒ
: : ì¼ë‹¨ ì²˜ìŒì— Adamì„ ì“°ë©´, ì •í•´ì§„ ì§§ì€ ì‹œê°„ ë‚´ì— ë” ê·¸ëŸ´ì‹¸í•˜ê³  ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ! (ê²°ë¡ : ì¼ë‹¨ Adam ì¨)
: : ëª¨ë¸ì´ ì˜ ì•ˆ ë˜ì—ˆì„ ë•Œ, ëª¨ë¸ì´ ì˜ëª»ë˜ì—ˆì„ ìˆ˜ë„ ìˆì§€ë§Œ Optimizerë¥¼ ë‹¤ë¥¸ ê²ƒì„ ì“°ë©´ ë” ì˜ ë  ìˆ˜ë„ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ê¸°ì–µí•˜ë¼.

<h2>Data Visualization</h2>
- (1-1ê°•) Welcome to Visualization<br><br>
: <br><b>ê°•ì˜ í‚¤ì›Œë“œ</b><br><br>
: - ë°ì´í„° ì‹œê°í™”
: <br><b>ë°ì´í„° ì‹œê°í™”ë€ ë¬´ì—‡ì¼ê¹Œ?</b><br><br>
: : ë°ì´í„°ë¥¼ ê·¸ë˜í”½ ìš”ì†Œë¡œ ë§¤í•‘í•˜ì—¬ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒ
: : ëª©ì  / ë…ì / ë°ì´í„° / ìŠ¤í† ë¦¬ / ë°©ë²• / ë””ìì¸ ë“± ì—¬ì„¯ ê°€ì§€ì˜ ê´€ì ì—ì„œ ì ‘ê·¼
: <br><b>ëª©í‘œ</b><br><br>
: : 1. ëª©ì ì— ë”°ë¼ ì‹œê°í™”ë¥¼ ì„ íƒí•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
: : 2. ì‹œê°í™” ê²°ê³¼ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜ìš©í•  ìˆ˜ ìˆë‹¤.


- (1-2ê°•) ì‹œê°í™”ì˜ ìš”ì†Œ ìƒíƒœ<br><br>
: <br><b>ê°•ì˜ í‚¤ì›Œë“œ</b><br><br>
: - ë°ì´í„°ì…‹, ì •í˜• ë°ì´í„°, ë¹„ì •í˜• ë°ì´í„°, Visual Component
: <br><b>1.1 'ë°ì´í„°' ì‹œê°í™”</b><br><br>
: : ìˆ˜ë§ì€ ë°ì´í„°ì…‹ì´ ìˆë‹¤ (ex. êµ­ê°€ Open API, Kaggle)
: <br><b>1.2 ë°ì´í„°ì…‹ì˜ ì¢…ë¥˜</b><br><br>
: <u>1) ì •í˜• ë°ì´í„°</u>
: : í…Œì´ë¸” í˜•íƒœë¡œ ì œê³µë˜ëŠ” ë°ì´í„°, ì¼ë°˜ì ìœ¼ë¡œ csv, tsv íŒŒì¼ë¡œ ì œê³µ
: : Columnì€ attribute(feature)
: : í†µê³„ì  íŠ¹ì„±ì„ ë§ì´ ì‹œê°í™”í•¨
<br><br>
: <u>2) ì‹œê³„ì—´ë°ì´í„° (Time series)</u>
: : ê¸°ì˜¨, ì£¼ê°€ ë“± ì •í˜•ë°ì´í„°ì™€ ìŒì„±, ë¹„ë””ì˜¤ ê°™ì€ ë¹„ì •í˜• ë°ì´í„°
<br><br>
: <u>3) ì§€ë¦¬/ì§€ë„ ë°ì´í„°</u>
: : ê±°ë¦¬, ê²½ë¡œ, ë¶„í¬ ë“±ë„ ë‹¤ì–‘í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥
<br><br>
: <u>4) ê´€ê³„ ë°ì´í„°</u>
: : ê°ì²´ì™€ ê°ì²´ ê°„ì˜ ê´€ê³„ë¥¼ ì‹œê°í™” (Graph Visualization / Network Visualization)
: : ê°ì²´ëŠ” Nodeë¡œ, ê´€ê³„ëŠ” Linkë¡œ
: : í¬ê¸°, ìƒ‰, ìˆ˜ ë“±ìœ¼ë¡œ ê°ì²´ì™€ ê´€ê³„ì˜ ê°€ì¤‘ì¹˜ë¥¼ í‘œí˜„
5) ê³„ì¸µì  ë°ì´í„°
: : ê´€ê³„ ì¤‘ì—ì„œë„ í¬í•¨ê´€ê³„ê°€ ë¶„ëª…í•œ ë°ì´í„°
: : Tree, Treempa, Sunburst ë“±ì´ ëŒ€í‘œì 
<br><br>
: <br><b>ëŒ€í‘œì ìœ¼ë¡œëŠ” 4ê°€ì§€ë¡œ ë¶„ë¥˜</b><br><br>
: <u>ìˆ˜ì¹˜í˜•(numerical)</u>
: - ì—°ì†í˜•(continuous) : ê¸¸ì´, ë¬´ê²Œ, ì˜¨ë„ ë“±
: - ì´ì‚°í˜•(discrete) : ì£¼ì‚¬ìœ„ ëˆˆê¸ˆ, ì‚¬ëŒ ìˆ˜ ë“±
: <u>ë²”ì£¼í˜•(categorical)</u>
: - ëª…ëª©í˜•(nominal) : í˜ˆì•¡í˜•, ì¢…êµ ë“± (ìˆœì„œê°€ ì¤‘ìš”í•˜ì§€ ì•Šì€ ê²ƒ)
: - ìˆœì„œí˜•(ordinal) : í•™ë…„, ë³„ì , ë“±ê¸‰ ë“±
<br><br>
: <br><b>\<2. ì‹œê°í™” ì´í•´í•˜ê¸°\></b><br><br>
: <br><b>2.1 ë§ˆí¬ì™€ ì±„ë„</b><br><br>
: <u>mark</u> : ì , ì„ , ë©´ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„°ì˜ ì‹œê°í™”
: <u>channel</u> : ê° ë§ˆí¬ë¥¼ ë³€ê²½í•  ìˆ˜ ìˆëŠ” ìš”ì†Œë“¤
<br><br>
: <br><b>2.2 ì „ì£¼ì˜ì  ì†ì„±(Pre-attentive Attribute)</b><br><br>
: : Orientation(ê¸°ìš¸ê¸°), Length, Width, Size, Shape, Curvature, Added Marks, Enclosure, Contrast, Colour, Potion, Spatial Grouping
: : ë™ì‹œì— ì‚¬ìš©í•˜ë©´ ì¸ì§€í•˜ê¸° ì–´ë ¤ì›€
: : Visual pop-outì´ ì˜ ì¼ì–´ë‚˜ì•¼ í•¨

- (1-3ê°•) Pythonê³¼ Matplotlib<br><br>
: : ì‹¤ìŠµ ì½”ë“œ ì°¸ì¡°

<h2>Peer Session</h2>
- 1) gather í•¨ìˆ˜<br><br>
- 2) optimzer.zero_grad() ë¥¼ í•´ì£¼ëŠ” ì´ìœ ?<br><br>

