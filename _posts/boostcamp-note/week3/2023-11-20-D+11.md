---
layout: single
title:  "Day 11 학습정리"
categories: boostcamp-note-week3
sidebar:
  nav: "docs"
---

# 23/11/20 (월) 학습 내용

<h2>DL Basic</h2>

- (1강) Historical Review<br><br>
: <br><b>강의 키워드</b><br><br>
: - 딥러닝의 역사 
: <br><b>\<인트로\></b><br><br>
: : <u>좋은 딥러너의 3요소</u>
<br><br>
: : Implementation Skills (TensorFlow, PyTorch 등)
: : Math Skills (Linear Algebra, Probability)
: : Knowing (a lot of recent) Papers<br><br>
: : <u>딥러닝의 필수 4요소</u>
<br><br>
: : **Data** that the model can learn from
: : Data type depend on the type of the problem to solve
: : Classification, Semantic Segmantation(이미지의 픽셀별로 분류), Detection(이미지 안 물체에 대한 bounding box 찾기), Pose Estimation, Visual QnA
<br><br>
: : **Model** how to transform the data
: : AlexNet, GoogLeNet, ResNet, DenseNet, LSTM, Deep AutoEncoders, GAN
<br><br>
: : **Loss** function that quantifies the badness of the model
: : Regression Task - MSE<br><br>
: $ \frac{1}{N}\sum_{i=1}^{N}\sum_{d=1}^{D}(y_i^{(d)} - \hat{y}_i^{(d)})^2$<br><br>
: : Classification Task - CE 
: $ - \frac{1}{N}\sum_{i=1}^{N}\sum_{d=1}^{D} y_i^{(d)}\log\hat{y}_i^{(d)}$<br><br>
: : Probabilistic Task - MLE
: $ \frac{1}{N}\sum_{i=1}^{N}\sum_{d=1}^{D} \log 𝒩 \hspace{1mm} (y_i^{(d)};\hat{y}_i^{(d)},1)$<br><br>
<br><br>
: : **Algorithm** to adjust the parameters to minimize the loss
: : SGD, Momentum, NAG, Adagrad, Adadelta, Rmsprop
: : 정규화 - Dropout, Early stopping, k-fold validation, Weight decay, Batch normalization, MixUp, Ensemble, Bayesian Optimization
<br><br>
: <br><b>\<Historical Review\></b><br><br>
: : 2012 - AlexNet 이라는 논문 출시. CNN. 딥러닝을 이용해 처음으로 1위를 함. (Paradigm Shift)
: : 2013 - DQN. DeepMind가 RL을 이용해 Atari를 깨는 모델
: : 2014 - Encoder / Decoder, Adam
: <span style="color:gray">(Adam은 Hyperparameter를 많이 튜닝하지 않아도 웬만하면 잘 돼서, resource 부족한 사람들이 안전하게 채택하기에 좋은 모델이다.)</span>
: : 2015 - GAN, ResNet
: <span style="color:gray">(GAN - Generative Adversarial Network)</span>
: <span style="color:gray">(Resnet - Residual Networks : 이 연구 때문에 딥러닝의 deep 러닝이 가능해졌다. 이전까지는 너무 깊게 layer를 쌓으면 잘 안 된다는 인식이 있었는데, 이후에는 100단 정도까지 쌓아도 test data에서의 성능이 개선될 수 있게 되었다.)</span>
: : 2017 - Transformer
: <span style="color:gray">(Attention is all you need 논문. 이제는 Vision 등 다른 분야도 많이 대체해나가고 있다. Multi Head Attention 구조를 이해하는 것이 중요하다.)</span>
: : 2018 - Bert (Fine-tuned NLP models)
: <span style="color:gray">Bidirectional Encoder Representations from Transformers</span>
: : 2019 - Big Language Models(GPT-X)
: <span style="color:gray">(굉장히 많은 parameter로 되어 있음. GPT-3는 1750억개의 파라미터)</span>
: : 2020 - Self Supervised Learning
: <span style="color:gray">(SimCLR :  a simple framework for contrastive learning of visual representations. 주어진 학습 데이터 외에 label을 모르는 unsupervised learning을 수행하겠다는 것)</span>
<br><br>

- <b>(2강) Neural Networks & Multi-Layer Perceptron</b><br><br>
: <br><b>강의 키워드</b><br><br>
: - Multi-Layer Perceptron
: - Hidden Layers
: - Activation Functions
: <br><b>인트로</b><br><br>
: : 딥러닝은 꼭 사람의 뇌를 모방한 것일까?
: : ex) Backpropagation이 인간의 뇌에서는 일어나지 않는다
: : cf) 비행기가 꼭 새나 박쥐의 모양을 모방해서 날지는 않는다.
: : Neural networks are function approximators that stack affine transformations followed by nonlinear transformations.
: <br><b>Linear Neural Networks</b><br><br>
: Data : $D = {(x_i,y_i)}$ (i=1 ~ i=N)
: Model : $\hat{y} = wx + b$
: Loss : $loss =  \frac{1}{N} \sum_{i=1}^{N}(y_i - \hat{y}_i)^2$ (MSE)<br><br>
: $ \frac{\partial loss}{\partial w} = \frac{\partial}{\partial w} \frac{1}{N} \sum_{i=1}^{N}(y_i - \hat{y}_i)^2$
: &emsp;&emsp;&emsp;$= \frac{\partial}{\partial w} \frac{1}{N} \sum_{i=1}^{N}(y_i - wx - b)^2$
: &emsp;&emsp;&emsp;$= - \frac{1}{N} \sum_{i=1}^{N}2(y_i - wx - b)x_i$
<br><br>
: <a href="http://taewan.kim/post/cost_function_derivation/">MSE 편미분 과정 설명</a>
<br><br>
: $ \frac{\partial loss}{\partial b} = \frac{\partial}{\partial b} \frac{1}{N} \sum_{i=1}^{N}(y_i - \hat{y}_i)^2$
: &emsp;&emsp;&emsp;$= \frac{\partial}{\partial b} \frac{1}{N} \sum_{i=1}^{N}(y_i - wx - b)^2$
: &emsp;&emsp;&emsp;$= - \frac{1}{N} \sum_{i=1}^{N}2(y_i - wx - b)$
<br><br>
: $w \leftarrow w - \eta \frac{\partial loss}{\partial w}$
: $b \leftarrow b - \eta \frac{\partial loss}{\partial b}$
<br><br>
: : Gradient Descent - 손실함수를 각각 W, b에 대해 편미분을 구하고, 이 값에 learning rate를 곱한 값을 현재의 W, b에서 뺌으로써 W, b 값을 업데이트 해주는 것
: <br><b>Linear Transformation</b><br><br>
: : We can handle multi dimensional input and output
: : $\mathbb{y} = W^T + \mathbb{b}$
: : m차원에서 n차원으로 가는 선형 변환
: <br><b>Beyond Linear Neural Network</b><br><br>
: : 선형 결합만을 아무리 많이 반복해도 결국 한 개의 행렬 곱과 다를 바가 없으므로, 활성 함수를 통한 Nonlinear transformation을 거쳐야 비선형성을 지닐 수 있다.
<br><br>
: : <u>Activation functions</u>
: : ReLU (Rectified Linear Unit)
: : Sigmoid
: : Hyperbolilc Tangent
: <br><b>Multi-Layer Perceptron</b><br><br>
: : <u>Loss function?</u>
: : Regression Task - MSE
: : Classification Task - CE
: : Probabilistic Task - MLE
<br><br>

- (3강) Optimization
: <br><b>강의 키워드</b><br><br>
: - Gradient Descent
: - Learning Rate
: - Optimization Algorithms
: <br><b>인트로</b><br><br>
: : Gradient Descent - 항상 local minima로 가게 되어 있음
: <br><b>\<Important Concepts in Optimization\></b><br>
: <br><b>Generalization</b><br><br>
: : 많은 경우, 일반화 성능을 높이는 것이 우리의 목적
: : 일반적으로 Training Error와 Test Error의 차이 정도를 말한다.
: <br><b>Underfitting vs. Overfitting</b><br><br>
: : 학습 데이터는 잘 되는데, 테스트 데이터에 안 되는 것 => Overfitting
: : 모델이 너무 단순하거나 하여 학습 데이터도 잘 안됨 => Underfitting
: : 이 사이의 Sweet spot을 찾는 것
: <br><b>Cross-validation</b><br><br>
: : k-fold validation 이라고 하기도 함
: : k-fold를 통해 최적의 하이퍼파라미터를 찾고, 하이퍼파라미터를 고정시킨 후에는 모든 데이터를 사용해 train 한다.
: : Training에는 절대로 어떤 식으로든 Test Data를 활용해서는 안 된다.
: <br><b>Bias and Variance</b><br><br>
: : 원점에서 멀더라도, 항상 같은 곳에만 찍히면 좋은 것. (= Low Variance)
: : Bias and Variance Tradeoff
: : Given $$D = \{(x_i,t_i)\}_{i=1}^{N}$$, where $t = f(x) + \epsilon$ and $\epsilon \sim  \mathcal{N}(0,\sigma^2)$
: : cost는 bias, variance, noise 이렇게 세 가지로 이루어져 있으므로, bias와 variance를 동시에 줄일 수는 없다.
: <br><b>Bootstrapping</b><br><br>
: : Boostrapping is any test or metric that uses random sampling with replacement(복원 추출).
: <br><b>Bagging vs. Boosting</b><br><br>
: : <u>Bagging</u> (Bootstrapping aggregating)
: : 이를 일반적으로 Ensemble 이라고 부르기도 한다.
: : Kaggle 같은 대회를 풀 때 많이 사용하는 방법이 앙상블인데, 앙상블은 Bagging에 속하는 경우가 많다.
: : <u>Boosting</u>
: : Focuses on those specific training samples that are hard to classify
: : 잘 안되는 데이터들에 대한 모델만 따로 만들고, 이후 여러 weak learner 모델을 만들어 하나의 strong learner 모델로 합치는 것
: : Bagging은 병렬적으로 여러 모델을 만드는 느낌, Boosting은 연쇄적으로 모델을 연결시켜 하나의 모델을 만들어내는 느낌
: <br><b>Gradient Descent Methods</b><br><br>
: : 1) SGD (Stochastic gradient descent)
: : 엄밀한 의미의 SGD는 단 하나의 샘플만 활용하는 것
: : 2) **Mini-batch gradient descent**
: : Update with the gradient computed from a subset of data (대부분 GD 에서는 Mini-batch 활용)
: : 3) Batch gradient descent
: : Update with the gradient computed from the whole data
: <br><b>Batch-size Matters</b><br><br>
: : Generalize를 위해 중요함
: : Batch size를 크게 하면 sharp minimizer로 수렴하고, 작게 하면 flat minimizer 로 수렴한다.
: <br><b>Choosing Optimizer</b><br><br>
: 1) SGD
: : Gradient Descent : $W_{t+1} \leftarrow W_t - \eta g_t$
: : learning rate를 잡는 것이 어렵다. 너무 크면 학습이 안 되고, 작으면 아무리 시켜도 학습이 안 된다.
<br><br>
: 2) Momentum (관성)
: : $a_{t+1} \leftarrow \beta a_t + g_t$ ($\beta$ = momentum)
: : $W_{t+1} \leftarrow W_t - \eta a_{t+1}$
<br><br>
: 3) NAG (Nesterov Accelerated Gradient)
: : 현재 정보가 있으면 그 쪽으로 가보고, 간 곳에서 그래디언트 계산한 것을 가지고 accumulation 하는 것
: : $a_{t+1} \leftarrow \beta a_t + \triangledown \mathcal{L}(W_t-\eta\beta a_t)$
: : $W_{t+1} \leftarrow W_t - \eta a_{t+1}$
: : 봉우리에 조금 더 빨리 converge 하게 되는 효과가 있음
<br><br>
: 4) Adagrad
: : Adaptive 방법들의 시초
: : Adagrad adapts the learning rate, performing larger updates for infrequent and smaller updates for frequent parameters.
: : $G$ = Sum of gradient squares (지금까지 그래디언트가 얼마나 많이 변했는지를 제곱해서 더한 것)
: : $\epsilon$ for numerical stability(0으로 나누지 않기 위해)
: : $W_{t+1} = W_t -  \frac{\eta}{\sqrt{G_t + \epsilon}}g_t$
: : 가장 큰 문제는 $G$가 계속 커지므로, 결국 무한대로 가게 되면 분모가 무한대이므로 $W$의 업데이트가 안 되고, 뒤로 가면 갈 수록 학습이 점점 멈추는 현상이 생긴다. (이를 해결하기 위한 방법이 Adam 같은 것들)
<br><br>
5) Adadelta
: : Adadelta 의 $G$가 계속 커지는 것을 막겠다
: : 위의 문제를 막기 위해 Time window를 100개로 잡는다고 하면, 파라미터 수가 많은 모델의 경우 너무 수가 많아지게 된다.
: : 그래서 이를 막기 위해, exponential moving everage이다. (아래의 수식)
: ($G_t$ = EMA of gradient squares)
: : $G_t = \gamma G_{t+1} + (1-\gamma)g_t^2$
: : $W_{t+1} = W -  \frac{\sqrt{H_{t-1} + \epsilon}}{\sqrt{G_t + \epsilon}}g_t$
: : $H_t = \gamma H_{t-1} + (1-\gamma)(\triangle W_t)^2$
: : There is <u>no learning rate</u> in Adadelta
: : 많이 사용되지는 않았다
<br><br>
6) RMSprop
: : 많이 사용되었음
: : $G_t = \gamma G_{t-1} + (1-\gamma)g_t^2$
: : $G_t$ = EMA of gradient squares
: : $W_{t+1} = W_t -  \frac{\eta}{G_t + \epsilon}g_t$
<br><br>
7) **Adam**
: : 일반적으로 가장 잘 되고 무난하게 사용하는 기법이 Adam
: : 컨셉적으로는, Gradient Squares를 Exponential Moving Average를 가져감과 동시에, 앞에서 처음에 배웠던 momentum을 같이 활용하는 것.
: : Adaptive Moment Estimation이다.
: : $m_t$ = Momentum, $v_t$ = EMA of gradient squares
: : 하이퍼파라미터로는 $\beta_1$(모멘텀을 얼마나 유지시킬지에 대한 것), $\beta_2$(Gradient Squares에 대한 EMA 정보), $\eta$ (Learning rate), $\epsilon$
: : $m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$
: : $v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2$
: : $W_{t+1} = W_t -  \frac{\eta}{\sqrt{v_t + \epsilon}} \cdot \frac{\sqrt{1- \beta_2^t}}{1-\beta_1^t} \cdot m_t$
: : $\sqrt{1- \beta_2^t}$, $1-\beta_1^t$ 는 전체 Gradient Descent가 Unbiased Estimator가 되기 위해 수학적으로 증명한 것
: <br><b>Regularization</b><br><br>
: : 선택한 방법론이 학습 데이터 뿐만 아니라, 테스트 데이터에서도 잘 작동되도록 하는 것이 정규화의 목적이다.
<br><br>
: 1) <u>Early Stopping</u>
: : Training에 활용하지 않은 Validation data의 error가 커지기 전에 일찍 멈추는 것 (추가적인 validation data가 필요!)
<br><br>
: 2) <u>Parameter norm penalty</u>
: : NN의 파라미터가 너무 커지지 않게 하는 것
: : total cost = $\mathbb loss(\mathcal D;W) +  \frac{\alpha}{2} \lVert W \rVert_2^2 $
: : It adds smoothness to the function space
<br><br>
: 3) <u>Data augmentation</u>
: : More data are always welcomed
: : ex) 강아지 고양이 분류 문제에서 각도 회전 (Label preserving augmentation)
<br><br>
: 4) <u>Noise robustness</u>
: : Add random noises to inputs or weights
: : 그러면 테스트 단계에서 더 잘 될 수 있다
<br><br>
: 5) <u>Label smoothing</u>
: : Similar to data augmentation
: : 학습 데이터 두 개를 뽑아서, 두 개를 섞어주는 것.
: : 왜 잘 되는지 이유보다는, 활용해보면 성능이 많이 올라간다.
: : 분류 문제를 푸는데 데이터셋이 한정적이고 더 많은 데이터를 얻을 방법이 없다면 Mixup이나 CutMix 등을 활용해 보는 것이 좋다. (코드 자체가 굉장히 간단)
<br><br>
: : 6) <u>Dropout</u>
: : Neural Network의 Weight을 0으로 바꾸는 것
: : Dropout ratio=0.5 라고 하면, NN이 Inference를 할 때에는 NN의 뉴런 중 50%를 0으로 바꿔주는 것
<br><br>
: : 7) <u>Batch Normalization</u>
: : 논란이 많은 논문! (Internal Covariate Shift)
: : 얘기하자면, BN을 적용하고자 하는 Layer의 Statistics를 정규화시키는 것.
: : 가지는 효과는, NN 각각의 Layer가 1000개의 parameter로 된 Hidden layer라고 하면, 이 파라미터들에 대한 statistics가 mean=0, unit variance가 되게 만드는 것 (각각 파라미터들에 대해 평균을 빼고 표준편차를 나눠주는 것)
: : 논문에서는 Internal Covariate Shift를 줄이는 효과가 있다고 한다.
: : $\mu_B =  \frac{1}{m}  \sum_{i=1}^{m}x_i$
: : $\sigma_{B}^{2} = \frac{1}{m} \sum_{i=1}^{m}(x_i - \mu_B)^2$
: : $\hat{x}_i =  \frac{x_i - \mu_B}{\sqrt{\sigma^2_B + \epsilon}}$
: : Layer가 깊게 쌓여있게 되면, 성능이 일반적으로 많이 올라가게 된다.
: : 이것의 변형들로는 Layer Norm, Instance Norm, Group Norm 등이 있다.
: : 왜 잘 되는지보다는, 간단한 분류 문제의 경우 Batch Normalization 을 활용하면 많은 경우 성능을 올릴 수 있다!
<br><br>
: <br><b>3강 - 실습</b><br><br>
: : ADAM > Momentum > SGD 의 퍼포먼스 차이가 나오는 이유
: : ADAM은 Momentum과 Adaptive Learning Rate를 합친 것임.
: : 즉, Momentum과 Adaptive Learning Rate 둘 다 쓰는 것이 중요하고,
: : 왜 SGD는 끝까지 못 맞출까?
: : Momentum은 이전의 gradient를 활용해서 다음번에도 쓰겠다는 차이점만 있을 뿐인데(adaptive learning rate이 안 들어가 있으므로), 
: : Mini-batch training시 momentum이 좋은 이유는,
: : SGD는 너무 많은 데이터가 있어야 전체 정보가 다 Converge 할 수 있게 되는데,
: : Momentum은 이전 batch의 정보도 반영하기 때문에 좀 더 많은 데이터를 보게 되는 효과가 있는 것
: : 일단 처음에 Adam을 쓰면, 정해진 짧은 시간 내에 더 그럴싸하고 좋은 결과를 얻을 수 있을 수 있다는 것! (결론: 일단 Adam 써)
: : 모델이 잘 안 되었을 때, 모델이 잘못되었을 수도 있지만 Optimizer를 다른 것을 쓰면 더 잘 될 수도 있다는 사실을 기억하라.

<h2>Data Visualization</h2>
- (1-1강) Welcome to Visualization<br><br>
: <br><b>강의 키워드</b><br><br>
: - 데이터 시각화
: <br><b>데이터 시각화란 무엇일까?</b><br><br>
: : 데이터를 그래픽 요소로 매핑하여 시각적으로 표현하는 것
: : 목적 / 독자 / 데이터 / 스토리 / 방법 / 디자인 등 여섯 가지의 관점에서 접근
: <br><b>목표</b><br><br>
: : 1. 목적에 따라 시각화를 선택하고 사용할 수 있다.
: : 2. 시각화 결과를 효과적으로 수용할 수 있다.


- (1-2강) 시각화의 요소 상태<br><br>
: <br><b>강의 키워드</b><br><br>
: - 데이터셋, 정형 데이터, 비정형 데이터, Visual Component
: <br><b>1.1 '데이터' 시각화</b><br><br>
: : 수많은 데이터셋이 있다 (ex. 국가 Open API, Kaggle)
: <br><b>1.2 데이터셋의 종류</b><br><br>
: <u>1) 정형 데이터</u>
: : 테이블 형태로 제공되는 데이터, 일반적으로 csv, tsv 파일로 제공
: : Column은 attribute(feature)
: : 통계적 특성을 많이 시각화함
<br><br>
: <u>2) 시계열데이터 (Time series)</u>
: : 기온, 주가 등 정형데이터와 음성, 비디오 같은 비정형 데이터
<br><br>
: <u>3) 지리/지도 데이터</u>
: : 거리, 경로, 분포 등도 다양하게 사용 가능
<br><br>
: <u>4) 관계 데이터</u>
: : 객체와 객체 간의 관계를 시각화 (Graph Visualization / Network Visualization)
: : 객체는 Node로, 관계는 Link로
: : 크기, 색, 수 등으로 객체와 관계의 가중치를 표현
5) 계층적 데이터
: : 관계 중에서도 포함관계가 분명한 데이터
: : Tree, Treempa, Sunburst 등이 대표적
<br><br>
: <br><b>대표적으로는 4가지로 분류</b><br><br>
: <u>수치형(numerical)</u>
: - 연속형(continuous) : 길이, 무게, 온도 등
: - 이산형(discrete) : 주사위 눈금, 사람 수 등
: <u>범주형(categorical)</u>
: - 명목형(nominal) : 혈액형, 종교 등 (순서가 중요하지 않은 것)
: - 순서형(ordinal) : 학년, 별점, 등급 등
<br><br>
: <br><b>\<2. 시각화 이해하기\></b><br><br>
: <br><b>2.1 마크와 채널</b><br><br>
: <u>mark</u> : 점, 선, 면으로 이루어진 데이터의 시각화
: <u>channel</u> : 각 마크를 변경할 수 있는 요소들
<br><br>
: <br><b>2.2 전주의적 속성(Pre-attentive Attribute)</b><br><br>
: : Orientation(기울기), Length, Width, Size, Shape, Curvature, Added Marks, Enclosure, Contrast, Colour, Potion, Spatial Grouping
: : 동시에 사용하면 인지하기 어려움
: : Visual pop-out이 잘 일어나야 함

- (1-3강) Python과 Matplotlib<br><br>
: : 실습 코드 참조

<h2>Peer Session</h2>
- 1) gather 함수<br><br>
- 2) optimzer.zero_grad() 를 해주는 이유?<br><br>

