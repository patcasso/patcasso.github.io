---
layout: single
title:  "Day 16 학습정리"
categories: boostcamp-note-week4
sidebar:
  nav: "docs"
---

# 23/11/27 (월) 학습 내용

<h2>NLP 이론</h2>

<h3>(1강) Intro to NLP</h3>
- 강의 키워드<br><br>
: - Bag-of-Words
: - Naive Bayes Classifier
<br><br>
- 소개<br><br>
: : NLU(Understanding), NLG(Generation)의 두 가지로 구성됨
: : CV와 더불어 가장 활발하게 발전되고 있는 분야.
: : ACL, EMNLP, NAACL 의 가장 활발한 학회
<br><br>
: <br><b>Low-level parsing</b><br><br>
: : Token, Tokenization
: : 문장은 Token으로 이루어진 Sequence
: : Stemming - 다양한 어미의 변화를 가진 언어를 어근으로 추출하는 것
<br><br>
: <br><b>Word and phrase level</b><br><br>
: : Named Entity Recognition (고유명사 식별)
: : Part-of-speech(POS) tagging (품사 인식)
<br><br>
: <br><b>Sentence level</b><br><br>
: : 감정 분석, 기계 번역
<br><br>
: <br><b>Multi-sentence and paragraph level</b><br><br>
: : Entailment prediction(두 문장 간의 논리적 내포, 모순 관계 예측)
: : 챗봇과 같은 Dialog system
: : 요약(Summarization)
<br><br>
: <br><b>Text mining</b><br><br>
: : 어떤 유명인에 대한 평가가 시간에 따라 어떻게 달라지는지 등 분석
: : 키워드의 빈도수를 분석해 소비자 반응을 얻어내는 방법으로 활용
: : 키워드들을 Groupping 해야 할 필요가 생김
: : Computational Social Science와 관련이 있음 (신조어 사용으로 보는 사회 현상 분석 등)
: <br><b>Information retrieval</b><br><br>
: : 검색 기술은 어느정도 성숙한 상태에 이르렀다고 볼 수 있다.
: : 추천 시스템 (ex. 스트리밍 사이트 노래 자동 추천, 유튜브 영상 추천 등)
<br><br>

- Trends of NLP<br><br>
: : 텍스트 데이터를 단어 단위로 분리하고, 각 단어를 특정 차원으로 이루어진 벡터로 표현하는 과정을 거친다.
: : 이를 Word Embedding 이라고 부른다.
: : 현재 대부분의 NLP를 위한 모델은 Transformer 모델을 기반으로 하고 있다.
: : 이전에는 Rule 기반의 기계 번역 방식을 사용했다. (언어적 지식 활용한 알고리즘 구성)
: : 그러나, rule 기반은 너무 많은 예외 상황과 다양한 패턴을 처리하기가 불가능했고, 
: : 단지 sequence 데이터를 잘 처리할 수 있는 RNN 모델을 이용했더니 성능이 월등히 향상되었다 
: : 성능이 오를대로 오른 이 기계번역 분야에서 새로운 성능 향상을 가져오게 된 모델이 바로 Transformer!
<br><br>
: : Transformer 이전에는 task별로 특화된 다른 모델들이 따로 존재했지만,
: : 요즘에는 self-attention module을 계속 쌓아나가는 식으로 모델 크기를 키우고, 레이블링이 필요 없는 Self-supervised training을 통해 학습 후,
: : (e.g., BERT, GPT-3, ...)
: : 우리가 원하는 task에 Transfer Learning 하는 방식으로 사용하면 좋은 성능을 보이고 있다.
: : AGI에 한 단계 더 다가선 것으로 볼 수 있다.
: : 그러나 이런 모델들을 학습하기 위해서는 엄청난 양의 데이터와 컴퓨팅 자원이 필요하게 된다.
<br><br>

- Bag-of-Words<br><br>
: : Step 1. Constructing the vocabulary containing unique words
: : Step 2. Encoding unique words to one-hot vectors
: : 문장/문서도 one-hot vector로 표현될 수 있다.
<br><br>


- Naive Bayes Classifier<br><br>
: : Bayes' Rule Applied to Document and Classes
: : 예를 들어 문서 분류 작업을 한다고 할 때, 
: : 1) 각 클래스별 사전확률과 
: : 2) 각 단어가 가지는 조건부 확률값들을 곱해줌으로써 
: : 3) 각 클래스별 최종 확률을 구할 수 있게 된다(PDF에 자세한 수식 설명)
: : 클래스 수가 3개 이상일 때에도 마찬가지로 확장될 수 있다.
: : 확률이 0이 되는 단어가 나오면, 다른 단어가 아무리 확률이 높아도 곱에 의해 결국 전체 확률값이 0이 되므로 정규화 방법들을 활용하게 된다.

<h2>(2강) Word Embedding</h2>
- 강의 키워드<br><br>
: - Word Embedding
: - Word2Vec
: - Distributed Representation
<br><br>

- Word Embedding: Word2Vec, GloVe<br><br>
: : 각각의 단어를 벡터로 표현할 수 있는 기법
<br><br>
: <br><b>Word2Vec</b><br><br>
: : 같은 문장에서 나타난 인접한 단어들 간의 의미가 비슷할 것이라는 가정을 사용한다.
: : ex) The cat purrs.
: : ex) This cat hunts mice.
: : "cat"단어를 주고 주변 단어를 숨긴 채 Word2Vec의 학습이 진행되게 된다.
<br><br>
: : 먼저 유니크한 단어들을 모아 사전을 구성
: : 사전의 각 단어는, 사전 사이즈만큼의 디멘션을 가지는 one hot vector 형태로 나타나게 된다.
: : Sliding Window - 한 단어를 중심으로 앞뒤로 나타난 입출력 쌍을 구성하게 된다.
<br><br>
: : 프로그래밍을 할 때에도, One-hot vector와 첫번째 선형변환 Matrix가 곱해지는 과정을 특별히 "Embedding Layer"라고 부른다.
: : 이 과정에서는 실제로 행렬곱을 수행하지 않고, One-hot vector의 1의 자리에 해당하는 인덱스, 그리고 그 인덱스 자리에 있는 컬럼 벡터를 선형결합 행렬에서 뽑아오는 식으로 행렬곱을 계산하게 된다.
<br><br>
: : $W_2$행렬과의 연산을 살펴보면, $W_1$과의 연산에서 뽑혀 온 2차원 벡터와, $W_2$의 각각의 Row 벡터와의 내적을 통해 해당 값들이 구해지는 것을 알 수 있다.
: : $W_2$에서의 Row 벡터의 수는, Vocab에서 뽑은 단어 수만큼 존재한다. 차원 역시 $W_1$에서 뽑힌 컬럼 벡터의 차원과 같다는 것을 알 수 있다.
<br><br>
: : 이상적으로는 Softmax에 들어가는 logit값이 Ground Truth와 일치하는 자리에는 $\infty$, 다른 자리들에는 $-\infty$가 되어야 한다.
: : 이 상황에서, 여기서 이루어진 내적 연산을 벡터들간의 유사도를 나타내는 과정으로 생각한다면, : : 주어진 입력 단어의 $W_1$ 상에서의 벡터와, 주어진 출력 단어의 $W_2$ 상에서의 벡터간의 내적에 기반한 유사도가 최대한 커지도록 하고,
: : 동시에, 주어진 출력단어가 아닌 다른 단어들의 $W_2$상에서의 벡터들의 내적에 기반한 유사도는 최대한 작게 만들도록 Word Embedding을 조금씩 조정하며 학습해나가는 것이 Word2Vec 모델의 핵심이다.
<br><br>
: : Word2Vec으로 할 수 있는 task 중 하나 - Intrusion Detection (카테고리가 다른 하나의 단어를 골라내는 것)
: : 기계번역, 이미지 캡셔닝 등
<br><br>
: <br><b>GloVe: Another Word Embedding Model</b><br><br>
: : Word2Vec과의 차이는,
: : 학습데이터에서 두 단어가 한 Window 내에서 동시에 몇번 등장했는지를 사전에 미리 계산하고,
: : 입력 단어의 임베딩 벡터와 출력 단어의 임베딩 벡터간의 내적 값이, 두 단어가 한 window 내에서 총 몇번 나타났는가의 로그값을 취한 값과 최대한 가까워질 수 있도록 하는 손실 함수를 취했다는 것이다.
<br><br>
: : W2V의 경우에서는, 특정한 단어 쌍이 자주 함께 등장했을 경우 해당 단어 아이템들이 여러번에 걸쳐 학습됨으로써 해당 내적값이 커지는 학습 방식을 따랐다면,
: : GloVe에서는 미리 등장하는 (로그) 확률을 미리 계산한 후, 직접적인 두 단어간의 내적값의 Ground Truth로 사용해 학습했다는 점에서, 중복되는 계산을 줄일 수 있다는 장점이 존재한다.
: : 따라서 학습이 W2V에 비해 빠르게 진행될 수 있고, 보다 더 적은 데이터에 대해서도 잘 동작하는 특성을 보이게 된다.
: : 또한, GloVe 모델은 선형대수의 관점에서 추천시스템 등에 많이 활용되는 알고리즘인 co-occurence matrix의 low-rank matrix factorization 태스크로도 이해될 수 있다.
<br><br>
: : W2V와 GloVe 모두 주어진 학습 (텍스트)데이터에 기반해 Word Embedding을 학습하는 동일한 역할의 알고리즘이고,
: : 실제 다양한 태스크에 적용했을 때 성능도 비슷하다는 사실을 기억하자.
<br><br>

- 해당 모델들 다운로드 가능한 곳<br><br>
: : <a href="https://code.google.com/archive/p/word2vec/">Word2Vec 다운로드 가능한 곳</a>
: : <a href="https://nlp.stanford.edu/projects/glove/">GloVe 모델 다운로드 가능한 곳</a>
<br><br>
- (2강) 확인하기 퀴즈<br>
: <br><b>Q. 다음 보기 중 Word Embedding에 대한 설명으로 적절하지 않은 것을 모두 고르시오.</b><br><br>
: : Word Embedding을 통해 획득한 단어 벡터는 One-hot 벡터와 동일한 차원을 가진다. (x)
: => Word Embedding은 One-hot 벡터와 동일한 차원일 필요가 없으며, 사전에 차원의 수를 정할 수 있는 hyperparameter이다. 
<br><br>
: : 의미적으로 유사한 단어들(예시: “love” 와 “like”) 간의 코사인 유사도는 0에 가까워 진다. (x)
: => 의미적으로 유사한 단어들 간의 코사인 유사도는 1과 가까워 진다.




<h2>Peer Session</h2>
- Transformer 스터디<br><br>
: : 
: : 
- 오늘 들은 강의 내용 관련<br><br>
: <b>(1강-실습-2) Naive Bayes classifier</b><br><br>
: : 해당 실습 코드에서 로그 확률(Log probability)를 사용하는 이유는?
<a href="https://www.cs.rhodes.edu/~kirlinp/courses/ai/f18/projects/proj3/naive-bayes-log-probs.pdf">설명 글</a>
: ⇒ 요약하자면 단어의 수가 많아질수록, 확률이 0에 가깝게 수렴하기 때문에 로그 가능도를 사용해준다는 것.
