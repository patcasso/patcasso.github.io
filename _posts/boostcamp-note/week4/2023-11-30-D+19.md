---
layout: single
title:  "Day 19 학습정리"
categories: boostcamp-note-week4
sidebar:
  nav: "docs"
---

23/11/30 (목) 학습 내용

<h1>NLP 이론</h1>

<h2>(5강) Seq2Seq</h2>
- 강의 키워드<br><br>
: - Seq2Seq (with attention)
: - Encoder-decoder architecture
: - Attention mechanism

- Seq2Seq Model<br><br>
: : 앞서 배운 Many-to-many의 형태에 해당한다. (입/출력 모두 시퀀스)
: : 입력 시퀀스를 모두 읽은 후 출력 시퀀스를 예측하는 구조
: : 입력 문장을 읽어들이는 RNN 모델을 인코더라고 부르고, 출력 문장을 순차적으로 생성하는 RNN 모델을 Decoder라고 부른다.
: : 인코더 / 디코더는 서로 파라미터를 공유하지 않는 RNN 모듈이다. 
<br><br>

- Seq2Seq with attention<br><br>
: : RNN 기반의 모델 구조상, 인코더의 마지막으로 출력되는 히든스테이트에 시퀀스 전체의 모든 정보를 욱여넣어야 한다.
: : 아무리 LSTM에서 Long-term dependency 해결했다 해도, 멀리(ex. 앞쪽) 있는 단어에 대한 정보는 점점 변질되거나 소실될 수 있다.
: : 디코더의 hidden state 벡터에서, 인코더의 어떤 타임스텝의 정보를 가장 필요로 하는지를 확률 벡터 형태로 얻게 되는 것이 attention이다.
: : 어텐션 모듈에 들어가는 것은 h(d) 하나와 h(e) 세트가 입력으로 들어가고, 출력은 h(e)의 가중평균된 한개의 벡터가 출력으로 나오게 된다.
: : h(d)와 어텐션 모듈의 output vector가 concat 되어서 output layer의 입력으로 들어가서 다음에 나올 단어를 예측하게 된다.
: : Teacher forcing 방식 => Ground Truth 단어를 매 타임스텝마다 넣어주는 방식
: : 맞는 값이든, 틀린 값이든 디코더에서 다음 타임스텝의 입력으로 넣어주는 방식은 teacher forcing이 아닌 방식이다.
: : Teacher forcing이 아닌 방법이 실제 사용했을 때의 상황과 더 가깝다.
: : 처음에는 t.f.로만 진행하다가 모델이 어느정도 정확해지면 후반부에는 t.f.를 사용하지 않고 예측한 값을 다음 ts에 입력으로 주는 식으로 학습하기도 한다.

- Different Attention Mechanisms<br><br>
: 1) Dot product 기반 attention
: : 우리가 아는 기존 내적 기반 유사도
<br><br>
: 2) Generalized dot product 기반 attention
: : 내적에 기반한 유사도를 구하는 것을 선형대수에서의 벡터/행렬곱으로 생각해보면,
: : 가운데에 차원을 변경하지 않는 사이즈의 학습 가능한 paramter 행렬을 놓게 되면,
: : 모든 서로 다른 dimension간의 곱해진 값들에 각각 부여되는 가중치가 된다.
: : 보다 일반화된 dot product라는 의미에서 general 기반의 attention
<br><br>
: 3) Concat 기반 attention
: : 인코더와 디코더 벡터를 concat 해서 하나의 fully connected layer로 보내는 방식으로 하거나,
: : 레이어를 더 쌓아서 최종적으로 스칼라 값이 나오도록 하는 어텐션을 구성할수도 있다.
: : Parameter들은 어떤 방식으로 최적화가 되는가?
<br><br>

- Attention is Great!<br><br>
: : Attention이 기존의 Seq2Seq 모델에 추가가 되면서, s2s 모델이 활발히 연구되고 발전되던 NMT 분야에서 성능을 굉장히 많이 향상시키게 된다.
: : 그 원인은 decoder의 매 타임스텝마다 입력 시퀀스의 어느 부분에 집중할지에 대한 정보를 활용할 수 있게 되었기 때문이다.
: : 인코더의 마지막 타임스텝에만 몰려있던 information bottleneck problem도 해결했다.
: : Attention을 통해 backpropagation 시점에 긴 과정을 거치지 않고 빠르게, 변질 없이 전달해줄 수 있게 되었다.
: : Attention의 흥미로운 해석 가능성
: : -> 어텐션의 패턴이 어떤 방식으로 나왔는가에 따라, 디코더가 각 단어를 예측할 때, 인코더 상의 어떤 단어에 집중했는지를 알 수 있게 된다.
: : 이러한 전반적인 과정은 모두 End-to-end learning으로, alignment를 nn이 스스로 배우는 현상을 보인다.
<br><br>



<h2>Peer Session</h2>
- 1) 기본 과제 1<br><br>
: : 승백님 코드 - List Comprehension 공부해보기 (for문보다 더 실행이 빠르다고 함)
: : 
- 2) 기본 과제 2<br><br>
: : <span style="color:red">Q. temperature가 의미하는 것은?</span>
: : <span style="color:red">Q. dictionary에서 valid와 test는 빼는 것인지? 방법은 어떻게 뺐는지?</span>
: : <span style="color:red">Loss 구하는 부분에서 정확히 어떤 일이 일어나고 있는지?</span>
