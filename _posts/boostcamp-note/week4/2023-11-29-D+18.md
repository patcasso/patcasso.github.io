---
layout: single
title:  "Day 18 학습정리"
categories: boostcamp-note-week4
sidebar:
  nav: "docs"
---

23/11/29 (수) 학습 내용<br>

<h1>NLP 이론</h1>

<h2>(3강) Basics of Recurrent Neural Network</h2>
- 강의 키워드<br><br>
: - Recurrent Neural Network (RNN)
: - Language Model
: - Exploding/Vanishing Gradient

- 소개<br><br>
: : RNN의 기본적인 문제 설정과 모델 구조 학습

- Basic Structure<br><br>
: : Basic structure (rolled / unrolled diagram)
: : Inputs and outputs of RNN
: : 특정 시점에서의 hidden state vector가 다음 타임스텝의 입력으로 쓰임과 동시에, 출력값을 계산해줄 수 있어야 한다.
: : $y_t$를 매 타임스텝마다 출력해야 할 수도 있고, (ex. 단어별로 번역 결과 출력)
: : 마지막 타임스텝에만 출력해야 할 수도 있다(ex. 리뷰 긍정/부정 분류)
: : How to calculate the hidden state of RNN
: : $h_{t-1}$ 벡터의 차원 수는 hyperparameter
: : * 동일 RNN 모듈이 "재귀적(recurrent)"으로 들어가게 되는 구조
<br><br>

- Types of RNNs<br><br>
: <b>One-to-one</b><br><br>
: : 입력과 출력 데이터의 타임스텝이 단 하나인, 시퀀스 데이터가 아닌 일반적인 데이터의 모델 구조
: : ex. 사람의 키, 몸무게, 나이로 이루어진 3차원 벡터 입력 -> 2차원의 hidden state 벡터 -> 이 사람의 혈압을 3차원의 아웃풋 벡터 (고혈압/정상/저혈압) 으로 출력
: <br><b>One-to-many</b><br><br>
: : 입력은 하나의 타임스텝, 출력은 여러 개의 타임스텝
: : 대표적으로 Image Captioning 태스크
: : 입력으로는 하나의 이미지를 주고, 이미지에 대한 캡션을 순차적으로 출력하는 형태
: : 추가적으로 입력을 넣어줄 수 없는 형태이니, 타임스텝이 없는 곳에는 모두 0으로 채워진 같은 사이즈의 입력 벡터를 RNN 모듈에 넣어주게 된다.
: <br><b>Many-to-one</b><br><br>
: : 입력으로 시퀀스를 받은 후, 최종 값을 마지막 타임스텝에서 비로소 출력으로 내어주는 형태.
: : 감정 분석, Sentiment classification
: : ex. "I love movie" => Positive
: <br><b>Many-to-many</b><br><br>
: : 입력 "I go home"을 먼저 끝까지 다 읽은 후,
: : 마지막 타임스텝에서는 한글 번역 "나는 집에 간다"를 순차적으로 내뱉게 됨.
: <br><b>Many-to-many (2)</b><br><br>
: : 입력 문장을 끝까지 다 읽고 처리하는 것이 아닌, 입력이 주어질 때마다 예측을 수행하는 형태의, 딜레이가 존재하지/허용되지 않는 태스크도 존재한다.
: : ex. 단어별로 품사를 예측하는 POS(Part Of Speech) 태깅,
: : ex2. Video classification on frame level
<br><br>

- Character-level Language Model<br><br>
: : RNN의 태스크 중 하나는 주어진 문자열이나 단어들의 순서를 바탕으로, 다음 단어가 무엇인지 맞히는 Task (Language Model)
: : 본 예시에서는 더 간단하게, character level로 맞추는 Language model
: : 먼저 사전을 구성해야 한다. 
: : "hello"의 경우 중복 없이 유니크하게 모으면, [h,e,l,o]로 사전 구성
: : 각각의 캐릭터는 사전의 길이만큼의 차원을 가지는 1-hot 벡터로 나타낼 수 있다. (ex. [1,0,0,0])
: : $h_t = \mathbb{tanh}(W_{hh}h_{t-1}+W_{xh}x_t+b)$
<br><br>
: : Inference 단계에서는, 예측값으로 다음 타임스텝의 캐릭터를 얻어낸 후, 이 예측값을 다음 타임스텝의 입력으로 재사용한다.
: : 다음 캐릭터를 또 예측하고, ... and so on
: : 따라서, 무한한 길이의 캐릭터 시퀀스를 자유롭게 생성할 수 있다.
: : ex. 특정한 회사의 과거 주식 가격 데이터를 모아서, 이를 통해 다음날의 주식값을 예측하는 task를 RNN으로 학습한다고 생각해보면,
: : RNN 모델은 주어진 시퀀스로부터 바로 다음날의 주식값만을 예측하도록 학습된 모델임에도 불구하고, 첫번째 날의 주식 값을 넣고 다음 날의 주식값을 예측하고, 그 다음날은 또 그 다음날... 등 동일한 모델로 계속해서 '점화식'처럼 예측해나갈 수 있게 된다.
<br><br>
: : Character-level LM을 문단의 level에서도 학습할 수 있다.
: : 문단에서 나타나는 character들의 sequence를 학습데이터로 활용.
: : 여러 단어, 여러 문장으로 이루어진 글인데 Character-level LM을 학습하기 위해서는 공백 또한 특수문자의 하나로 vocab 상에 하나의 dimension을 차지하게 된다.
: : ,나 줄바꿈이 일어나는 경우에도 줄바꿈에 해당하는 특수문자를 vocab에 기록해주게 되면, 하나의 긴 글도 여전히 one-dimensional character sequence로 볼 수 있고, 이를 통해 LM을 학습할 수 있게 된다.
<br><br>
: : 셰익스피어 리어왕 대본 예시 - 사람 이름의 경우 All caps에 ':' 까지 잘 붙여서 생성해주는 것을 알 수 있다. (패턴이 학습 데이터에 녹아있기 때문!)
: : 또, RNN으로 논문도 작성할 수 있는데, 
: : 논문 자체를 프로그래밍 언어화 해서 LaTex로 작성하는 경우가 많은데, LaTex raw 언어 자체를 character-level language model로 RNN 학습 후 생성하게 되면, LaTex 프로그래밍 언어로 된 결과를 뱉게 되고 이를 compile 하게 되면 실제 논문과 같이 수식 등이 포함된 논문이 생성되는 것을 볼 수 있다.
: : 마찬가지의 원리로, C code 등 프로그래밍 언어도 잘 생성할 수 있게 되는 것이다! (ex. 괄호를 열었을 때 line break, 함수 내에서의 indentation 등도 모두 잘 학습하는 것을 볼 수 있다. 신기!)
<br><br>

- Backpropagation through time (BPTT)<br><br>
: : 입력 벡터 $x_t$를 변환시켜주는 $W_{xh}$, 
: : 이전 타임스텝의 히든 스테이트 벡터를 변환시켜주는 $W_{hh}$, 
: : 그리고 히든 스테이트 벡터 $h_t$를 아웃풋 벡터로 변환시켜주는 $W_{hy}$를 이루는 세 종류의 가중치 행렬들이 전체적으로 Loss 값을 최소화시키며 Backpropagation through time이 되면서 학습되는 구조를 가지고 있다.
: : 그러나 현실적으로, 한번에 계산해야 하는 Sequence가 굉장히 길어지게 되면, 한번에 처리할 수 있는 데이터 양이 한정된 GPU Memory에 담기지 못할 수 있기 때문에,
: : Truncated, 즉 잘라서 제한적인 길이씩만을 학습하는 방법을 채택하게 된다.
: : 한 번에 학습할 수 있는 타임스텝의 개수를 n개로 한다면, 해당 n개 타임스텝 내애서만 BPTT가 일어나며 Parameter를 학습하게 된다.
<br><br>

- Searching for Interpretable Cells<br><br>
: : 필요한 정보를 저장하는 공간은 매 타임스텝마다 있는 hidden state vector 라고 할 수 있다.
: : ex. 공백을 총 두 번, 네 번 생성해야 한다 라는 정보, 지금 현재까지는 space를 두 번까지 생성했다 등의 정보가 hidden state vector에 담겨져야 한다.
: : 예컨대 hidden state가 3차원 벡터라면, 하나의 벡터를 고정한 후 해당 값 변화를 추적함으로써 RNN 작동 원리를 알 수 있다.
: : 어떤 하나의 dimension에서는 따옴표가 열리고, 따옴표가 닫히는 동안 큰 값으로 유지가 되었다가, 따옴표가 닫힌 다음부터는 다시 작은 값으로 되었다가 등의 특성을 보이게 된다. 이 경우, 이 RNN 내에서 hidden state vector의 특정 dimension이 "quote detection cell"로 작동하고 있음을 알 수 있게 된다. (완전 신기.. CNN에서 자동으로 여러 convolution 필터가 학습되는 원리와 비슷하네.)
: : 예컨대, 프로그래밍 언어에서는 하나의 dimension(=cell)이 if statement cell의 기능을 담당하고 있음을 볼 수 있다.
<br><br>

- Vanihsing/Exploding Gradient Problem in RNN<br><br>
: : 앞에서 잘 학습된 예시들의 결과는 사실 Vanilla RNN이 아니라, LSTM이나 GRU를 사용했을 때의 결과들이다.
: : 오리지널 Vanilla RNN에서는 Backpropagation시 동일한 Matrix를 매 timestep마다 곱하게 되는데,
: : 이 경우 등비수열과 같은 형태가 되기 때문에, Gradient가 vanishing 하거나 exploding하는 문제가 발생하게 된다.
: : <span style="color:red">PDF에서 이를 보여주는 합성함수 미분 과정 다시 한번 자세히 보기!(p. 35)</span>
: : 위 합성함수 미분 과정에서도 3이 타임스텝 개수만큼 제곱이 되어서 증폭되는 것을 볼 수 있다. 
: : 혹은 기울기가 0.2 정도였다고 한다면 기하급수적으로 감소하게 된다.
: : LSTM의 경우 타임스텝간의 거리가 먼 상황에서, 혹은 Long-term dependency 문제를 효과적으로 개선해 줄 수 있게 된다.

<h2>(4강) LSTM, GRU</h2>
- 강의 키워드<br><br>
: - LSTM
: - GRU
: - Gating Mechanism

- 소개<br><br>
: : Vanilla RNN을 LSTM, GRU가 대체하게 됨
: : Vanishing Gradient, Long-term dependence 방면에서 훨씬 좋은 성능을 보이게 됨

- Long Short-Term Memory (LSTM)<br><br>
: : LSTM이란 무엇인가?
: : Hidden state vector를 단기 기억(short term)을 담당하는 기억 소자로 볼 수 있는데,
: : 해당 단기 기억을 더 길게(Long) 보관할 수 있게 개선한 형태
<br><br>
: : Vanilla에서는 $h_t = f_w(x_t,h_{t-1})$ 였는데,
: : LSTM에서는 Cell state vector가 새로 생기게 된다.
: : $$ \{c_t, h_t\}$$ = LSTM$(x_t, C_{t-1}, h_{t-1})$
: : 이 중 Cell state vector가 조금 더 완성된, 우리가 필요로 하는 정보를 담고 있는 벡터이고
: : Hidden state vector는 Cell state vector를 한 번 더 가공해서, 해당 타임스텝에서 노출해야할 정보만을 담은 필터링된 벡터라고 할 수 있다. (ex. 예측값을 계산하는 output layer 등 다음 layer의 입력값으로 사용하게 된다.)
<br><br>
: : 계산 과정에서 네 개의 벡터로 분할한 후, 각각의 벡터에 시그모이드 혹은 tanh를 걸어 input gate, forget gate, output gate, gate gate의 값을 얻게 된다.
: : tanh를 통과한 것이 현재 타임스텝에서 결정되는 유의미한 정보라고 볼 수 있다.
: <br><b>i, f, o, g의 자세한 역할</b><br><br>
: : $c_{t-1}$을 적절하게 변환하는데 사용된다.
<br><br>
: : <u>Forget gate</u>
: : $f_t = \sigma(W_f \cdot [h_{t-1},x_t] +b_f)$
: : 여기서 나온 벡터를 이전 타임스텝에서 온 Cell state 벡터와 성분 곱을 해주게 된다.
: : (왜냐하면 위의 벡터는 시그모이드를 거쳐서 0에서 1 사이의 값을 갖게 되는데, 예컨대 0.7 이라고 하면 이전 cell state vector에서 70%의 정보만을 유지하고 30%는 forget 한다는 의미로 해석될 수 있기 때문이다!)
<br><br>
: : <u>Gate gate</u>
<br><br>
: : $i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
: : $\tilde{C_t} = \mathbb{tanh}(W_c \cdot [h_{t-1},x_t]+b_c)$ (이게 gate gate)
<br><br>
: : $C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C_t}$ 
: : 이전 단계에서 forget gate를 통해 이전 타임스텝의 Cell state에서 원하는 부분만 남겨놓은 값을, input gate와 gate gate를 곱한 값을 덧셈의 형태로 계산해서 현재 타임스텝에서의 Cell state vector를 계산해주게 된다.
<br><br>
: : gate gate를 input gate와 곱해준 후 더해주는 이유는($\tilde{C}$와 $i_t$)
: : 한 번의 선형변환만으로 $C_{t-1}$에 더해줄 정보를 만들기 어려운 경우에는,
: : 일단 더해주고자 하는 값보다 조금 더 큰 값들로 구성된 정보를 $\tilde{C}$의 형태로 만들어준 후,
: : 각 디멘션마다 특정 비율만큼의 정보를 덜어내서, 실제로 $C_{t-1}$ 에 더해주고자 하는 정보를 두 단계에 거쳐 만들겠다는 의도로 이해하면 됨.
<br><br>
: : $h_t$를 만들 차례
: : $C_t$에 tanh를 적용해서 -1~1 사이의 값으로 만들어주고,
: : 시그모이드를 통해 만들어진 $o_t$, 0~1 사이 값을 곱해주어서
: : Cell state가 가지던 정보에서, 특정 dimension별로 적정 비율만큼 값을 작게 만들어서 최종적인 $h_t$를 구성하게 된다.
: : $C_t$는 모든 타임스텝의 정보를 담고있다고 볼 수 있지만, $h_t$는 현재 타임스텝에서 예측값을 내는 아웃풋 레이어의 입력으로 사용되는 벡터라는 점에서 해당 타임스텝의 예측값에 직접적으로 필요한 정보만을 담은, 지금 당장 필요한 정보만을 $C_t$로부터 필터링한 형태의 정보로 볼 수 있다.
: : $h_t$는 다음 RNN의 히든스테이트 벡터로 넘어감과 동시에, 위로 올라가는 path를 가지는데,
: : 이것이 바로 현재 타임스텝에서 예측을 수행할 때, 아웃풋 레이어에 입력으로 주는 $h_t$가 되는 것이다.
<br><br>
: : ex. "Hello, John" 에서, 글자 l의 타임스텝에서 '현재 따옴표가 열려있다' 라는 정보는 Cell state 벡터인 $C_t$에 저장되어 있고, 당장 다음스텝에 올 o 라는 글자를 예측하는 데에는 $C_t$를 적절히 필터링한 $h_t$에 담겨있게 되는 것이다.
<br><br>

- Gated Recurrent Unit (GRU)<br><br>
: : LSTM의 모델구조를 보다 경량화해서, 적은 메모리 요구량과 빠른 계산시간이 가능하도록 만든 모델이다.
: : GRU의 가장 큰 특징은, LSTM에서 두 가지 종류의 벡터로 존재하던 Cell State Vector와 Hidden State Vector을 일원화해서, 오직 Hidden State Vector만 존재하게 된다는 것이다.
: : 그러나 전체적인 작동 원리는 LSTM과 유사하다.
: : GRU에서는 통합된 벡터인 $h_t$는, LSTM의 Cell state vector $c_t$와 비슷한 역할을 한다고 이해할 수 있다. (전체적인 정보를 보다 잘 담고 있음)
<br><br>
: : $z_t = \sigma(W_z \cdot [h_{t-1},x_t])$
: : $r_t = \sigma(W_r \cdot [h_{t-1},x_t])$
: : $\tilde{h_t} = \mathbb{tanh}(W \cdot [r_t \cdot h_{t-1},x_t])$
: : $h_t = (1-z_t) \cdot h_{t-1} + z_t \cdot \tilde{h_t}$
<br><br>
: : c.f) $C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C_t}$ in LSTM
<br><br>
: : 이런 방식으로 GRU는 LSTM의 $c_t$와 $h_t$를 하나로 일원화했고,
: : 두 개의 독립된 게이트로 하던 계산을 하나의 게이트로만 하게 함으로써 경량화를 이루어낸 모델이다.
: : 경량화된 모델임에도, LSTM에 뒤지지 않거나 심지어 낫기도 한 성능을 보여주었다.

- Backpropagation in LSTM, GRU<br><br>
: : Uninterrupted gradient flow!
: : 기존 Vanilla처럼 $W_hh$ 를 지속적으로 곱해주는 형태가 아니라,
: : 지난 정보에 forget gate를 곱하고, 필요한 정보를 곱셈이 아닌 덧셈으로 만들어주기 때문에, gradient가 소실되지 않게 된다.
: : 따라서 멀리 있는 타임스텝에까지 gradient를 큰 변형 없이 전해줄 수 있다.(덧셈은 복제이기 때문?)
<br><br>

- Summary on RNN/LSTM/GRU<br><br>
: : RNN은 유연한 구조
: : Vanilla RNN 은 간단하지만 gradient vanishing, explosion 문제로 사용되지 않고,
: : 보다 진보된 LSTM, GRU를 실제로 많이 사용하고, 
: : 해당 방법들에서 cell state vector / hidden state vector를 각 타임스텝에서 업데이트하는 과정이 기본적으로 덧셈에 기반한 연산이므로 gradient van, exp 문제를 피하고 long-term dependence 문제를 해결할 수 있게 된 것이다!



<h2>Peer Session</h2>
- 1) 주제 1<br><br>
: : 
: : 
- 2) 주제 2<br><br>
: : 
: : 