---
layout: single
title:  "Day 20 학습정리"
categories: boostcamp-note-week4
sidebar:
  nav: "docs"
---

23/12/1 (금) 학습 내용

<h1>NLP 이론</h1>


<h2>(6강) Beam Search and BLEU score</h2>
- 강의 키워드<br><br>
: - Beam Search
: - BLEU score

- Greedy decoding<br><br>
: : Sequence로서의 전체적인 문장의 확률 값을 보는 게 아니라, 근시안적으로 현재 타임스텝에서 가장 좋아보이는 단어를 그때 그때 선택하는 방법을 greedy approach라고 부른다.
: : Greedy decoding has no way to undo decisions!

- Exhaustive search<br><br>
: : Greedy decoding과, 전체 경우의 수를 다 따지는 것 사이에 어딘가에 있는 것
: : 디코더의 매 타임스텝마다, 가장 가능성이 높은 k개의 부분적인 번역을 keep track 하는 것
: : k는 beam size라 부르고, 일반적으로 5 to 10개의 beam 사이즈를 가지도록 설정한다.
: : 최대화하고자 하는 값은 joint probability인데,
: : 로그를 붙여 곱셈이 아닌 덧셈의 형태로 로그 확률값을 더한 형태로 식이 변경된다.

- Beam search의 구체적인 예시<br><br>
: :  로그를 취한 확률값이 -값인 것은, 확률값이 0~1이기 때문에 x절편이 1이므로, 로그를 취했을 때 0에서 1 사이에서는 -값이 나오지만, 확률값이 커질수록 로그를 취한 값도 단조증가하는 형태를 보인다.
: : -값 중에서도 조금더 큰 확률을 가진 값이 가장 큰 확률이다.
: : 이렇게 함으로써 한 타임스텝이 지나도 여러 가능한 의미 branch 중에서, 가장 가능성이 높은 branch를 고를 수 있게 된다.
: : 사람이 언어를 통역하는 과정과 굉장히 유사하다!! 신기하군 (어순이 다른 언어로 통역할 때 여러 가능한 의미의 경우의 수를 놓고 생각하는 것.)

- Beam search: Stopping criterion<br><br>
: : Greedy decoding의 경우, \<END> token이 나오는 경우에 멈추게 되는데,
: : beam search decoding에서는 어느 hypothesis가 END 토큰을 어느 시점에서 생성했다고 하면, 그 경우에 대해서는 더이상 생성을 진행하지는 않고 해당 hypothesis는 완료되었다고 생각하고 저장 공간에 임시로 저장해놓게 된다.
: : n개만큼의 hypothesis가 저장되면 beam search를 중단하게 된다.

- Beam search: Stopping criterion<br><br>
: : 최종 hypothesis 중에서도, 로그 버젼의 결합확률이 가장 큰 hypothesis를 최종 결과로 예측값으로 줄 수 있게 된다.
: : 이 경우 짧은 길이를 가진 것이 더 joint prob이 높을 것이고, 긴 길이를 가진 시퀀스에 해당하는 hypothesis는 결합확률이 더 낮게 나오게 된다.
: : 이를 해결하기 위해, normalize를 해야 한다.
: : 서로 길이가 다른 hypo들중 공평하게 비교하기 위함
: :  $\frac{1}{t}$을 곱해 평균을 내줌으로써 공평한 확률로 비교할 수 있도록 한다. (t = timestep의 길이)

- Precision and Recall<br><br>
: : 앞에서 Seq2Seq w/ attention 등에서 타겟 문장 생성하는 경우에는, 모델을 학습하는 데 쓰이는 Objective function(목적식)으로, 타겟문장의 각 단어별로 걸리는 softmax loss, 즉, 정답 단어에 부여된 확률값이 최대한 커지도록 학습을 진행
<br><br>
: : 학습이 완료된 후에는 정확도를 평가할 때, 테스트 데이터에 대해 각 단어별 softmax loss값을 계산하거나, 각 단어의 분류 정확도를 계산할 수도 있다.
<br><br>
: : 그러나 이 경우, 특정 타임스텝에서는 특정 ground truth가 나와야 하는 가정 하에 평가를 진행하면, 전체적인 문장으로 볼 때, 단어를 하나 중간에 빼고 생성한다든가, 추가적 단어를 하나 더 많이 생성을 한 경우, 단어의 순서가 밀리게 되면서 어느 타임스텝에서도 단어를 맞추지 못한 결과로 해석될 수 있다.
<br><br>
: : 생성된 문장 전체를 갖고 유사도를 평가하지 않고, 고정된 위치에서 정해진 하나의 단어가 나와야 한다는 단순화된 평가 방식 때문이다.
: : 즉, 문장 전체 차원에서 평가해야할 필요가 있게 된다.
: : 이를 위한 두 가지 평가 방법<br><br>
: 1) Precision (정밀도)
: : 맞게 생성한 단어 / 예측한 문장의 길이 
<br><br>
: : 2) Recall (재현율)
: : 맞게 생성한 단어 / 레퍼런스에 존재하는 단어의 개수
<br><br>
: : Precision은 우리가 실질적으로 느끼는 정확도
: : Recall은 우리가 원하는 정보 중에서 실제로 얼마나 빠짐없이 정보를 노출해 주었는가를 측정하는 방법
: : 스타크래프트의 리콜..ㅋㅋㅋ
: : 아비터가 유닛을 하나도 빠짐없이 잘 소환해야 리콜을 잘 했듯이,
: : 실제 소환해야 하는 대상이 리콜에서의 모수가 되고, 그 중 몇 개를 잘 소환했는가가 리콜, 즉 재현율이라고 부를 수 있다.
<br><br>
: cf) 산술평균:  $\dfrac{p+r}{2}$
: cf) 기하평균: $(p \times r)^{\frac{1}{2}}$
<br><br>
: **조화평균 (F-measure)**<br><br>
:  $ = \dfrac{1}{\dfrac{\dfrac{1}{p} +  \dfrac{1}{r}}{2}}  $
<br><br>
: : 산술평균 $\geq$ 기하평균 $\geq$ 조화평균
<br><br>
: : Flaw - no penalty for reordering!

- BLEU Score<br><br>
: : 개별 단어 레벨에서 얼마나 ground truth와 겹치는 공통 단어가 나왔는지 뿐만 아니라,
: : N-gram이라는 개념을 통해 n개의 연속된 단어로 봤을 때 문구, 즉, phrase가 얼마나 gt와 겹치는지를 보고 판단하는 것이 특징.
: : Recall은 무시하고 precision만을 고려하게 된다.
: : N-gram(1~4 까지의 N)으로 precision을 구한 후, 기하평균을 내주는 것을 알 수 있다.
: : 조화평균을 쓰지 않는 이유는, 크기가 작은 값에 지나치게 큰 가중치를 주는 특성 때문이다.
: : 이후 Brevity penalty라는 것을 사용하는데, reference 길이보다 좀 더 짧은 문장을 생성한 경우에는 1보다 작은 값,
: : 만약 reference 길이보다 더 긴 문장을 생성한 경우에는 1의 값이 되게 된다.


<h2>Peer Session</h2>
- 1) 주제 1<br><br>
: : 부스트캠프 한 달 회고 기록