---
layout: single
title:  "Day 73 학습정리"
categories: boostcamp-note-week15
sidebar:
  nav: "docs"
---

24/02/14 (수) 학습 내용

<h1>MRC</h1>

<h2>(6강) Passage Retrieval-Scaling Up </h2>
- 강의 키워드<br><br>
: - MIPS
: - IVF-PQ
<br><br><br>

1. Passage Retrieval and Similarity Search<br><br>
: <br><b>복습: Retrieval with dense embedding</b><br><br>
: : 이 방식의 문제는 passage의 개수가 늘어날수록 더 큰 Embedding space가 필요하다
: : Dimension이 커지면 dot product 자체도 부담스러운 연산이 될 수 있다.
: : 이 과정을 흔히 Similarity Search 라고 부른다.
: <br><b>MIPS(Maximum Inner Product Search)</b><br><br>
: : Nearest neighbor(L2 유클리드 거리 측정) 보다, Inner product search(inner product의 최대값 찾기)가 더 많이 쓰인다
: : 검색 과정에서 brute force(exhaustive search) 아닌 더 스마트한 방법에 대해 알아보자.
: <br><b>MIPS & Challenges</b><br><br>
: : 실제 검색해야 할 데이터는 훨씬 방대함
: : 위키피디아 500만 개, 검색엔진의 경우 수십억, 조 단위까지 커질 수 있음
: : -> 더이상 모든 문서 임베딩을 일일이 보며 찾을 수 없음
: <br><b>Tradeoffs of similarity search</b><br><br>
: : 1) Search speed -> Pruning
: : 2) Memory Usage -> Compression
: : 3) Accuracy -> Exhausitve search
: : 속도(search time)와 재현율(recall)의 관계 -> 더 정확한 검색을 하려면 검색 시간이 오래 걸림
: <br><b>Increasing search space by bigger corpus</b><br><br>
: : Corpus의 크기가 커질수록 검색이 어려워진다. (탐색 공간, 메모리 공간 등. Sparse Embedding의 경우 이러한 문제가 훨씬 심함)
<br><br><br>

2. Approximating Similarity Search<br><br>
: <br><b>Compression - Scalar Quantization (SQ)</b><br><br>
: : 실제 Inner product search 할 때는 4-byte까지 필요 없고, 1-byte(8bit)로 줄여서 표현해도 잘 되는 경우가 많다.
: <br><b>Pruning - Inverted File (IVF)</b><br><br>
: : Pruning - Search space를 줄여 search 속도 개선
: : Clustering + Inverted file을 활용한 search
: : 1) Clustering: 전체 vector space를 k개의 cluster로 나눔 (ex. k-means clustering)
: : 군집화를 한 후, 가까이 있는 클러스터만 방문해서 exhaustive search로 탐색하는 방법
: : 2) Inverted file (IVF)
: : Vector의 index = inverted list structure
: : 각 클러스터에 속해있는 포인트들을 역으로 인덱스로 가지고 있기 때문이다(?)
<br><br><br>

3. Introduction to FAISS<br><br>
: <br><b>What is FAISS</b><br><br>
: : 페이스북에서 만들고 모니터링하는 fast approximation을 위한 라이브러리
: : 모든 것이 오픈소스화 되어있고, large-scale에 특화되어 있다.
: : backbone은 C++로 되어있지만, wrapping은 Python으로 되어있다.
: <br><b>Passage Retrieval with FAISS</b><br><br>
: : 1) Train index and map vectors
: : 2) Search based on FAISS index
: : nprobe - 몇 개의 가장 가까운 cluster를 방문하여 search 할 것인지
<br><br><br>

4. Scaling up with FAISS<br><br>
: <br><b>FAISS Basics</b><br><br>
: : brute-force로 모든 벡터와 쿼리를 비교하는 가장 단순한 인덱스 만들기
: <br><b>IVF with FAISS</b><br><br>
: : IVF 인덱스 만들기
: : 클러스터링을 통해서 가까운 클러스터 내 벡터들만 비교함
: : 빠른 검색 가능
: : 클러스터 내에서는 여전히 전체 벡터와 거리 비교 (Flat)
: <br><b>IVF-PQ with FAISS</b><br><br>
: : 벡터 압축 기법 (PQ) 활용하기
: : 전체 벡터를 저장하지 않고 압축된 벡터만을 저장
: : 메모리 사용량을 줄일 수 있음
: <br><b>Using GPU with FAISS</b><br><br>
: : GPU의 빠른 연산 속도 활용 가능
: : 메모리 제한이나 random access 시간이 느린 것 등이 단점
: : 여러 GPU를 활용하여 연산 속도를 한층 더 높일 수 있음
<br><br><br>


<h2>(7강) Linking MRC and Retrieval</h2>
1. Introduction to Open-domain Question Answering (ODQA)<br><br>
: : MRC - 지문이 주어진 상황에서의 응답
: : Supporting Evidence가 지문이 아닌, 위키피디아나 웹 전체임
: : 우리가 봐야 하는 문서의 크기가 위키 기준 3백만 개 이상인 것
: <br><b>History of ODQA</b><br><br>
: : TREC(Text Retrieval Conference) - QA Tracks (1999-2007)
: : 1) Question processing
: : 질문으로부터 키워드를 선택 후 answer type selection
: : 2) Passage retrieval
: : Named entity / Passage 내 question 단어의 개수 등과 같은 hand-crafted features 사용
: : 3) Answer processing
: : Hand-crafted features와 heuritic을 활용한 classifier
: : 주어진 question과 선별된 passage들 내에서 답을 선택
: : 생각보다 오래된 분야라는 것!
<br><br>
: : IBM Watson (2011)
: : The DeepQA Project
: : Jeopardy! (TV quiz show) 우승
: : Pipeline을 보면, 예전에 했던 방법론과 아주 다르지는 않다.
: <br><b>Recent ODQA Research</b><br><br>
: : 강의 자료 참조
<br><br><br>

2. Retriever-Reader Approach<br><br>
: : ODQA에서 가장 일반적인 approach!
: : 1~3강의 Reader와, 4~6강에서 다룬 Retriever를 연결하는 접근 방식
: <br><b>학습 단계</b><br><br>
: : <u>Retriever</u>
: : TF-IDF, BM25 -> 학습 없음
: : Dense -> 학습 있음 (QA 데이터셋 이용)
<br><br>
: : <u>Reader</u>
: : SQuAD와 같은 MRC 데이터셋으로 학습
: : 학습 데이터를 추가하기 위해서 Distant Supervision 활용
: <br><b>Distant Supervision</b><br><br>
: : 질문-답변만 있는 데이터셋(CuratedTREC, WebQuestions, WikiMovies 등)에서 MRC 학습 데이터 만들기. Supporting document가 필요함
: : QA만 있는 데이터셋에서 먼저 학습시킨 Retriever를 가져와서 
: : Question-Answer-Passage pair를 더 많이 만들 수 있다.
: <br><b>Inference</b><br><br>
: : Retriever가 질문과 가장 관련성 높은 5개(혹은 k개) 문서 출력
: : Reader는 5개 문서를 읽고 답변 예측
: : Reader가 예측한 답변 중 가장 score가 높은 것을 최종 답으로 사용함
<br><br><br>

3. Issues and Recent Approach<br><br>
: <br><b>Different granularities of text at indexing time</b><br><br>
: : 위키피디아에서 각 Passage의 단위를 문서, 단락, 또는 문장으로 정의할지 정해야 함
: : Retriever 단계에서 몇 개의 top-k 문서를 넘길지 정해야 함
: : ex. article -> k=5, paragraph -> k=29, sentence -> k=78 등
: <br><b>Single-passage training vs Multi-passage training</b><br><br>
: : Multi-passage - retrieved passages 전체를 하나의 passage로 취급하고, reader 모델이 그 안에서 answer span 하나를 찾도록 함
: : 단점 - 문서가 너무 길어져서, GPU에 더 많은 메모리 할당해야 함 + 연산량이 많아짐.
: <br><b>Importance of each passage</b><br><br>
: : Retriever 모델에서 추출된 top-k passage들의 retrieval score를 reader 모델에 전달
<br><br><br>